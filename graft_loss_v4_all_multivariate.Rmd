---
title: "graft_loss_v4-all multivariate"
author: "jbw"
date: "2024-02-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

Objective: 
1) use avatar to create a duplicated data set and compare the performance of original and avatar dataset in a cox model.
2) Evalaute the variability for a given avatar with bootstraping
3) Evaluate variabiltiy between different Avatar using different seed
4) Evaluate effect of data augmentation (X4)
5) Evaluate Survtvae in comaprison to Avatar
6) Evaluate Survctgan in comaprison to Avatar

In this anlaysis, the covariates selected  after bootstraping bootstepAIC each synhtetic dataset are used for the analyses of inter dataset variability

The difference with v3 is that here with fit the multivariate model for the evaluation of knn=x instead of fitting only for haplotype and maping on knn value : that means that we have ot run separately for the different knn value the script with hteir own mulitvariate models


```{r}

library(tidyverse)
library(tidymodels)
library(FNN)
library(survival)
library(survminer)
library(corrplot)
library(ggcorrplot)
library(DataExplorer)
library(patchwork)
library(tableone)
library(boot)
library(bootStepAIC)
library(conflicted)
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")
```


## Load the data

```{r}
library(readr)
original <- read_delim("td_dirc_perte_greffon.txt", 
    delim = "\t", escape_double = FALSE, 
    trim_ws = TRUE) %>% 
  mutate(rejet_aigu = as.factor(rejet_aigu)
         # age_r =  scale(age_r),
         # age_d =  scale(age_d),
         # TIF =  scale(TIF),
         ) %>% 
  mutate_if(is.character, factor) %>% 
  select(-id) %>% 
  select(haplotype:delai_event)
summary(original)
str(original)


original1 <- original %>% mutate_if(is.factor, as.numeric)# converti en factor par ordre alhpahbetique+++
write.csv(original1, file = "original1.csv")
```

# Avatar

In this code we will vary the seed for avatar with a fix number of knn (here=5) and bootsptrap the Cox model
The goal is to extract the variability of HR for a given dataset and between different seed to obtain the overall uncertainty


## run a single Avatar knn=5

We launch a single Avatar with knn=5 and a given seed and we compare the results to the original data

```{r}
data_normalized <- scale(original1)
pca <- prcomp(data_normalized, scale. = FALSE)# pour selecitonner le nombre de cp rank. = 3

```

```{r}
# Number of neighbors
k <- 5  # Adjust this based on your requirement
```

algorithm

```{r avatar basal knn5}
pca_transformed_data <- pca$x
knn_result <- get.knn(pca_transformed_data, k)

generate_avatar_weights <- function(knn_result, pca_transformed_data, k) {
  n <- nrow(pca_transformed_data)
  avatar_weights <- matrix(nrow = n, ncol = k)
  
  for (i in 1:n) {
    # Step 1: Inverse of Distances
    distances <- sqrt(rowSums((pca_transformed_data[knn_result$nn.index[i, ], ] - pca_transformed_data[i, ])^2))
    inverse_distances <- 1 / distances
    
    # Step 2: Random Weights
   
    random_weights <- rexp(k, rate = 1)
    
    # Step 3: Contribution Factors
   
    shuffled_indices <- sample(k)
    contribution_factors <- 1 / (2^shuffled_indices)
    
    # Step 4: Calculate Weights
    weights <- inverse_distances * random_weights * contribution_factors
    
    # Step 5: Normalize Weights
    normalized_weights <- weights / sum(weights)
    
    avatar_weights[i, ] <- normalized_weights
  }
  
  return(avatar_weights)
}



# Generate avatar weights
 set.seed(12)
avatar_weights <- generate_avatar_weights(knn_result, pca_transformed_data, k)
```


Generation of avatar in the latent space

```{r}
# Assuming pca_result, avatar_weights, knn_result$nn.index, and pca_transformed_data are already defined

# Function to generate avatars in PCA space based on weights
generate_avatars_pca_space <- function(pca_transformed_data, knn_indices, weights) {
  n <- nrow(pca_transformed_data)
  avatars_pca <- matrix(nrow = n, ncol = ncol(pca_transformed_data))
  
  for (i in 1:n) {
    weighted_avatars <- pca_transformed_data[knn_indices[i, ], ] * weights[i, ]
    avatars_pca[i, ] <- colSums(weighted_avatars)
  }
  
  return(avatars_pca)
}
# Generate avatars in PCA space
avatars_pca_space <- generate_avatars_pca_space(pca_transformed_data, knn_result$nn.index, avatar_weights)

```

Return to the initial scale

```{r}
# Assuming 'aids_pca' is the PCA object and 'avatars_pca_space' contains the avatars in PCA space
# Inverse PCA transformation
inverse_pca <- function(pca_object, pca_data) {
  return(pca_data %*% t(pca_object$rotation) + matrix(pca_object$center, nrow = nrow(pca_data), ncol = ncol(pca_object$rotation), byrow = TRUE))
}
avatars_original_scale <- inverse_pca(pca, avatars_pca_space)

# Assuming 'aids_data_normalized' contains the scaling attributes of the original data
# Inverse normalization (if the original data was normalized)
avatars_rescaled <- scale(avatars_original_scale, center = FALSE, scale = 1/attr(data_normalized, "scaled:scale"))
avatars_rescaled <- sweep(avatars_rescaled, 2, attr(data_normalized, "scaled:center"), "+")


```

Transform into tibble

```{r}
avatars_tibble_knn5 <- as_tibble(avatars_rescaled) %>% 
  mutate(haplotype = round(haplotype, digits=0),
         cyp3A5D = round(cyp3A5D, digits=0),
         sexe_r  = round(sexe_r , digits=0),
         sexe_d  = round(sexe_d , digits=0),
         rejet_aigu  = round(rejet_aigu , digits=0),
         event = round(event, digits=0)
         # CYP3A4_1B = round(CYP3A4_1B, digits=0),
         # MDR1_C1236T = round(MDR1_C1236T, digits=0),
         # MDR1_G2677T = round(MDR1_G2677T, digits=0),
         # MDR1_C3435T = round(MDR1_C3435T, digits=0)
  ) 

avatars_tibble_factor_knn5 <- as_tibble(avatars_rescaled) %>% 
  mutate(haplotype = round(haplotype, digits=0),
         cyp3A5D = round(cyp3A5D, digits=0),
         sexe_r  = round(sexe_r , digits=0),
         sexe_d  = round(sexe_d , digits=0),
         rejet_aigu  = round(rejet_aigu , digits=0),
         event = round(event, digits=0)
         # CYP3A4_1B = round(CYP3A4_1B, digits=0),
         # MDR1_C1236T = round(MDR1_C1236T, digits=0),
         # MDR1_G2677T = round(MDR1_G2677T, digits=0),
         # MDR1_C3435T = round(MDR1_C3435T, digits=0)
  ) %>% 
  mutate(haplotype = as.factor(haplotype),
         cyp3A5D = as.factor(cyp3A5D),
         sexe_r = as.factor(sexe_r),
         sexe_d = as.factor(sexe_d),
         # CYP3A4_1B = as.factor(CYP3A4_1B),
         # MDR1_C1236T = as.factor(MDR1_C1236T),
         # MDR1_G2677T = as.factor(MDR1_G2677T),
         # MDR1_C3435T = as.factor(MDR1_C3435T),
         rejet_aigu = as.factor(rejet_aigu))
```

Plot of the synthetic and original in the latent space

```{r}
# Combine original and synthetic data for visualization
combined_data <- rbind(
  original1 %>% mutate(DataType = 'Original'),
  avatars_tibble_knn5 %>% mutate(DataType = 'Synthetic')
)

# Perform PCA on combined data
combined_data_normalized <- scale(combined_data[, -which(names(combined_data) %in% c("DataType", "id"))])
combined_pca <- prcomp(combined_data_normalized, scale. = FALSE)

# Extract the first two principal components
combined_pca_data <- data.frame(combined_pca$x[, 1:2])
combined_pca_data$DataType <- combined_data$DataType

# Plot PCA with color differentiation
ggplot(combined_pca_data, aes(x = PC1, y = PC2, color = DataType)) +
  geom_point(alpha = 0.8) +
  theme_minimal() +
  labs(title = "PCA Plot", x = "Principal Component 1", y = "Principal Component 2", color = "Data Type")

```
Export data knn = 5 

```{r}
write_csv(avatars_tibble_knn5, file = "avatar_sfpt_knn5.csv")
```


### Comparison of the datasets

#### Summary of the 2 datasets

```{r}
## Vector of categorical variables that need transformation
catVars <- c("haplotype", "cyp3A5D",  "sexe_r",  "sexe_d", 
"rejet_aigu", "event")
## Create a variable list.
vars <- c( "haplotype", "cyp3A5D", "age_r", "sexe_r", "age_d", "sexe_d", 
"rejet_aigu", "TIF", "event", "delai_event", "DataType")
tableOne <- CreateTableOne(vars = vars, strata = "DataType",factorVars = catVars, data = combined_data)
tableOne2<-print(tableOne, nonnormal = c( "age_r", "age_d", "TIF", "delai_event"), printToggle=F, minMax=T)
```

```{r , echo=F}
kableone(tableOne2)
```

```{r}
summary(original)
summary(avatars_tibble_knn5)
```

#### individual data explorer

```{r}

# boxplots
plot_boxplot(combined_data , by ="DataType") 

# histograms

# Function to create histogram for each continuous variable
plot_histograms <- function(data, var_name, group_var) {
  ggplot(data, aes(x = !!sym(var_name), fill = !!sym(group_var))) +
    geom_histogram(alpha = 0.5,show.legend = FALSE) +
    labs(x = var_name, y = "Count") +
    theme_minimal() +
    ggtitle(paste(var_name))
}

# Using select_if to identify continuous variables and map to apply the function
plots <- combined_data %>%
  select( -sexe_r,-sexe_d) %>% 
  select_if(is.numeric) %>%
  names() %>%
  map(~plot_histograms(combined_data, ., "DataType"))

# Optionally, print or arrange plots (e.g., using gridExtra or patchwork packages)

wrap_plots(plots)
```

#### plot correlation
```{r}

##Correlation Analysis
  cor_real <- cor(original1, use = "complete.obs")
  cor_synthetic <- cor(avatars_tibble_knn5, use = "complete.obs")
  
# plots
ggcorrplot(cor_real, hc.order = TRUE, type = "lower",
           lab = TRUE,  pch.cex = 5,
  tl.cex = 6, lab_size = 2)
# plots
ggcorrplot(cor_synthetic, hc.order = TRUE, type = "lower",
           lab = TRUE,  pch.cex = 5,
  tl.cex = 6, lab_size = 2)
```

#### Modele de Cox

```{r}
# original
fit_original <- coxph(Surv(delai_event, event) ~ haplotype + cyp3A5D +  age_r + sexe_r + age_d +  sexe_d + rejet_aigu + TIF , data = original1)
summary(fit_original)
ggforest(fit_original)

# synthetique
fit_synthetique <- coxph(Surv(delai_event, event) ~ haplotype + cyp3A5D +  age_r + sexe_r + age_d +  sexe_d + rejet_aigu + TIF, data = avatars_tibble_knn5)
summary(fit_synthetique)
ggforest(fit_synthetique)

```

#### BootstepAIC based on BIC

Allow to see which vairable would have been selected

Original

```{r}

boot.stepAIC(fit_original, original1, B = 100, k=log(nrow(original1)))

```

synhtetic knn5

```{r}

boot.stepAIC(fit_synthetique, avatars_tibble_knn5, B = 100, k=log(nrow(avatars_tibble_knn5)))

```


##### Final model original

```{r}
fit_original <- coxph(Surv(delai_event, event) ~ haplotype , data = original1)
summary(fit_original)
```

##### Final model synthetic

```{r}
fit_synthetique <- coxph(Surv(delai_event, event) ~ haplotype  , data = avatars_tibble_knn5)
summary(fit_synthetique)
```

### Bootstrap of the coefficient for haplotype

Allow to define the variability range of HR for a given dataset (intra dataset variability)

```{r bootstrap knn5} 
# Define the Cox model
cox_model <- function(data, indices) {
  d <- data[indices,] # allows bootstrapping to sample the data
  fit <- coxph(Surv(delai_event, event) ~ haplotype , data=d)
  return(fit$coefficients)
}

# Set the seed for reproducibility
set.seed(12)

# Bootstrap the Cox model
boot_results <- boot(data=avatars_tibble_knn5, statistic=cox_model, R=100)

# Convert bootstrap results to a data frame for ggplot2
boot_hrs <- exp(boot_results$t) # Convert log(HR) to HR
hr_data <- data.frame(HR=boot_hrs[,1])

# Calculate summary statistics
summary_stats <- quantile(hr_data$HR, probs = c(0, 0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975, 1))
names(summary_stats) <- c("Min","2.5th", "5th", "25th", "Median", "75th", "95th","97.5th","Max")

# Create the histogram
ggplot(hr_data, aes(x=HR)) +
  geom_histogram(bins=30, fill="#007a86", color="black") +
  # geom_vline(aes(xintercept=summary_stats["Min"]), color="red", linetype="dashed") +
  geom_vline(aes(xintercept=summary_stats["25th"]), color="gray", linetype="dashed", linewidth=2) +
  geom_vline(aes(xintercept=summary_stats["Median"]), color="blue", linetype="dashed", linewidth=2) +
  geom_vline(aes(xintercept=summary_stats["75th"]), color="gray", linetype="dashed", linewidth=2) +
  # geom_vline(aes(xintercept=summary_stats["Max"]), color="purple", linetype="dashed") +
  labs(title="Bootstrap Distribution of Hazard Ratios", x="Hazard Ratio (HR)", y="Frequency") +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5))

# Print summary statistics
print(summary_stats)


```

bootstraping of the original data for comparison

```{r bootstreap original data}
# Set the seed for reproducibility
set.seed(12)

# Bootstrap the Cox model
boot_results <- boot(data=original1, statistic=cox_model, R=100)

# Convert bootstrap results to a data frame for ggplot2
boot_hrs <- exp(boot_results$t) # Convert log(HR) to HR
hr_data <- data.frame(HR=boot_hrs[,1])

# Calculate summary statistics
summary_stats <- quantile(hr_data$HR, probs = c(0, 0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975, 1))
names(summary_stats) <- c("Min","2.5th", "5th", "25th", "Median", "75th", "95th","97.5th","Max")

# Create the histogram
ggplot(hr_data, aes(x=HR)) +
  geom_histogram(bins=30, fill="#007a86", color="black") +
  # geom_vline(aes(xintercept=summary_stats["Min"]), color="red", linetype="dashed") +
  geom_vline(aes(xintercept=summary_stats["25th"]), color="gray", linetype="dashed", linewidth=2) +
  geom_vline(aes(xintercept=summary_stats["Median"]), color="blue", linetype="dashed", linewidth=2) +
  geom_vline(aes(xintercept=summary_stats["75th"]), color="gray", linetype="dashed", linewidth=2) +
  # geom_vline(aes(xintercept=summary_stats["Max"]), color="purple", linetype="dashed") +
  labs(title="Bootstrap Distribution of Hazard Ratios", x="Hazard Ratio (HR)", y="Frequency") +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5))

# Print summary statistics
print(summary_stats)

```

##### Modele final & KM

```{r KM original}
km_original <- survfit(Surv(delai_event, event) ~ haplotype, data = original)
km_original_plot <- ggsurvplot(
  km_original,
  data = original,
  size = 1,                 # change line size
  conf.int = TRUE,          # Add confidence interval
  pval = TRUE,              # Add p-value
  risk.table = TRUE,        # Add risk table
  risk.table.col = "strata",# Risk table color by groups
  risk.table.height = 0.25, # Useful to change when you have multiple groups
  ggtheme = theme_bw()      # Change ggplot2 theme
)
km_original_plot
```

```{r KM avatar KNN5}
km_synthetique <- survfit(Surv(delai_event, event) ~ haplotype, data = avatars_tibble_factor_knn5)
km_synthetique_avatar_5 <- ggsurvplot(
  km_synthetique,
  data = avatars_tibble_factor_knn5,
  size = 1,                 # change line size
  conf.int = TRUE,          # Add confidence interval
  pval = TRUE,              # Add p-value
  risk.table = TRUE,        # Add risk table
  risk.table.col = "strata",# Risk table color by groups
  risk.table.height = 0.25, # Useful to change when you have multiple groups
  ggtheme = theme_bw()      # Change ggplot2 theme
)
km_synthetique_avatar_5
```


Plots original & synthetic combined

```{r}
## combine data
combined_df <- rbind(original %>% mutate(group = "original"), avatars_tibble_factor_knn5 %>% mutate(group = "synthetic")) %>% mutate(combined_haplotype = str_c(haplotype,"_", group ))

## fit the model
km_combined <- survfit(Surv(delai_event, event) ~ combined_haplotype, data = combined_df)

# plot
ggsurvplot(fit = km_combined, 
           data = combined_df,
           
  size = 1,                 # change line size
  conf.int = FALSE,          # Add confidence interval
  pval = TRUE,              # Add p-value
  risk.table = TRUE,        # Add risk table
  risk.table.col = "strata",# Risk table color by groups
  risk.table.height = 0.35, # Useful to change when you have multiple groups
  ggtheme = theme_bw()      # Change ggplot2 theme
)

```

#### Graphical exploraiotn of distribution

```{r}
library(GGally)

pm_knn5 <- combined_df %>% select(haplotype:delai_event, group) %>% 
  rename(acute_rejection = rejet_aigu, sex_r = sexe_r, sex_d = sexe_d) %>% 
  ggpairs(
  ggplot2::aes(colour = group,alpha = 0.5),
  upper = list(continuous = wrap("cor", size = 1.5)),
  lower=list(combo=wrap("facethist", binwidth=0.5))) + 
  theme(strip.text.x = element_text(size = 5),
           strip.text.y = element_text(size = 5),axis.text = element_text(size = 5))
pm_knn5
ggsave("Figure1_graft_loss.pdf")
# ggsave("comparaison_distribution_knn5.pdf")

```

### function to aggregating the HR and CI95 results of Cox models after changing 100 times the seed with  knn = 5
Allow to define the variability range of HR for different Avatar generated with different seed but the same knn (inter dataset variability)

```{r change seed knn5}

# Assuming all your existing functions and necessary libraries are loaded

run_model_with_seed <- function(seed_value) {
 
  
  # Number of neighbors
  k <- 5  # Adjust this based on your requirement
  
  pca_transformed_data <- pca$x
  knn_result <- get.knn(pca_transformed_data, k)
  
  generate_avatar_weights <- function(knn_result, pca_transformed_data, k) {
    n <- nrow(pca_transformed_data)
    avatar_weights <- matrix(nrow = n, ncol = k)
    
    for (i in 1:n) {
      # Step 1: Inverse of Distances
      distances <- sqrt(rowSums((pca_transformed_data[knn_result$nn.index[i, ], ] - pca_transformed_data[i, ])^2))
      inverse_distances <- 1 / distances
      
      # Step 2: Random Weights

      random_weights <- rexp(k, rate = 1)
      
      # Step 3: Contribution Factors
  
      shuffled_indices <- sample(k)
      contribution_factors <- 1 / (2^shuffled_indices)
      
      # Step 4: Calculate Weights
      weights <- inverse_distances * random_weights * contribution_factors
      
      # Step 5: Normalize Weights
      normalized_weights <- weights / sum(weights)
      
      avatar_weights[i, ] <- normalized_weights
    }
    
    return(avatar_weights)
  }
  
  
  
  # Generate avatar weights
   set.seed(seed_value)
  avatar_weights <- generate_avatar_weights(knn_result, pca_transformed_data, k)
  
  
  # Function to generate avatars in PCA space based on weights
  generate_avatars_pca_space <- function(pca_transformed_data, knn_indices, weights) {
    n <- nrow(pca_transformed_data)
    avatars_pca <- matrix(nrow = n, ncol = ncol(pca_transformed_data))
    
    for (i in 1:n) {
      weighted_avatars <- pca_transformed_data[knn_indices[i, ], ] * weights[i, ]
      avatars_pca[i, ] <- colSums(weighted_avatars)
    }
    
    return(avatars_pca)
  }
  # Generate avatars in PCA space
  avatars_pca_space <- generate_avatars_pca_space(pca_transformed_data, knn_result$nn.index, avatar_weights)
  

  # Inverse PCA transformation
  inverse_pca <- function(pca_object, pca_data) {
    return(pca_data %*% t(pca_object$rotation) + matrix(pca_object$center, nrow = nrow(pca_data), ncol = ncol(pca_object$rotation), byrow = TRUE))
  }
  avatars_original_scale <- inverse_pca(pca, avatars_pca_space)
  
 
  # Inverse normalization (if the original data was normalized)
  avatars_rescaled <- scale(avatars_original_scale, center = FALSE, scale = 1/attr(data_normalized, "scaled:scale"))
  avatars_rescaled <- sweep(avatars_rescaled, 2, attr(data_normalized, "scaled:center"), "+")
  
  avatars_tibble_knn5 <- as_tibble(avatars_rescaled) %>% 
    mutate(haplotype = round(haplotype, digits=0),
           cyp3A5D = round(cyp3A5D, digits=0),
           sexe_r  = round(sexe_r , digits=0),
           sexe_d  = round(sexe_d , digits=0),
           rejet_aigu  = round(rejet_aigu , digits=0),
           event = round(event, digits=0)
           # CYP3A4_1B = round(CYP3A4_1B, digits=0),
           # MDR1_C1236T = round(MDR1_C1236T, digits=0),
           # MDR1_G2677T = round(MDR1_G2677T, digits=0),
           # MDR1_C3435T = round(MDR1_C3435T, digits=0)
    ) 
  
  avatars_tibble_factor_knn5 <- as_tibble(avatars_rescaled) %>% 
    mutate(haplotype = round(haplotype, digits=0),
           cyp3A5D = round(cyp3A5D, digits=0),
           sexe_r  = round(sexe_r , digits=0),
           sexe_d  = round(sexe_d , digits=0),
           rejet_aigu  = round(rejet_aigu , digits=0),
           event = round(event, digits=0)
           # CYP3A4_1B = round(CYP3A4_1B, digits=0),
           # MDR1_C1236T = round(MDR1_C1236T, digits=0),
           # MDR1_G2677T = round(MDR1_G2677T, digits=0),
           # MDR1_C3435T = round(MDR1_C3435T, digits=0)
    ) %>% 
    mutate(haplotype = as.factor(haplotype),
           cyp3A5D = as.factor(cyp3A5D),
           sexe_r = as.factor(sexe_r),
           sexe_d = as.factor(sexe_d),
           # CYP3A4_1B = as.factor(CYP3A4_1B),
           # MDR1_C1236T = as.factor(MDR1_C1236T),
           # MDR1_G2677T = as.factor(MDR1_G2677T),
           # MDR1_C3435T = as.factor(MDR1_C3435T),
           rejet_aigu = as.factor(rejet_aigu))

  
  # Finally, fit the Cox model
  fit <- coxph(Surv(delai_event, event) ~ haplotype , 
               data = avatars_tibble_knn5)

  # Calculate confidence intervals
  ci <- confint(fit)
  
  return(list(fit = fit, ci = ci))
}


extract_hrs_and_cis <- function(model_output) {
  coefs <- model_output$fit$coefficients
  ci <- model_output$ci

  hr <- exp(coefs)
  ci_lower <- exp(ci[,"2.5 %"])
  ci_upper <- exp(ci[,"97.5 %"])

  return(data.frame(variable = names(hr), hr = hr, ci_lower = ci_lower, ci_upper = ci_upper))
}

# Generate a list of seed values
seed_values <- sample(x=100) # Modify this if you need different seed values

# Apply the algorithm with different seed values
model_results <- map(seed_values, run_model_with_seed)

# Extract HR and CI from model results
extracted_results <- map(model_results, extract_hrs_and_cis)

# Combine results into a single data frame
combined_results <- bind_rows(extracted_results)

# Calculate median HR and CI for each variable
# aggregate_metrics <- combined_results %>%
#   group_by(variable) %>%
#   summarize(
#     median_hr = median(hr),
#     median_ci_lower = median(ci_lower),
#     median_ci_upper = median(ci_upper)
#   )
# 
# aggregate_metrics

# Calculate the specified percentiles for HRs for each variable
percentile_metrics <- combined_results %>%
  group_by(variable) %>%
  summarize(
    percentile_0 = quantile(hr, probs = 0),
    percentile_5 = quantile(hr, probs = 0.05),
    percentile_25 = quantile(hr, probs = 0.25),
    percentile_50 = quantile(hr, probs = 0.5),
    percentile_75 = quantile(hr, probs = 0.75),
    percentile_95 = quantile(hr, probs = 0.95),
    percentile_100 = quantile(hr, probs = 1)
  ) %>%
  pivot_longer(-variable, names_to = "Percentile_HR", values_to = "Value_HR") %>% 
  mutate(Value_HR = round(Value_HR, 2))
# percentile_metrics
# datatable(percentile_metrics)
knitr::kable(percentile_metrics %>%  mutate(Value_HR = round(Value_HR, 2)), "simple")

```

### function to aggregating the HR and CI95 results of Cox models after changing 100 times the seed with different knn 

Allow to define the variability range of HR for different Avatar generated with different seed and different values of knn


```{r change seed and knn}
# # Assuming all necessary functions (PCA transformation, KNN, avatar generation, etc.) are defined
# 
# run_analysis_for_k_and_seed <- function(k, seed_value) {
#  
# 
#  pca_transformed_data <- pca$x
#   knn_result <- get.knn(pca_transformed_data, k)
#   
#   generate_avatar_weights <- function(knn_result, pca_transformed_data, k) {
#     n <- nrow(pca_transformed_data)
#     avatar_weights <- matrix(nrow = n, ncol = k)
#     
#     for (i in 1:n) {
#       # Step 1: Inverse of Distances
#       distances <- sqrt(rowSums((pca_transformed_data[knn_result$nn.index[i, ], ] - pca_transformed_data[i, ])^2))
#       inverse_distances <- 1 / distances
#       
#       # Step 2: Random Weights
# 
#       random_weights <- rexp(k, rate = 1)
#       
#       # Step 3: Contribution Factors
#   
#       shuffled_indices <- sample(k)
#       contribution_factors <- 1 / (2^shuffled_indices)
#       
#       # Step 4: Calculate Weights
#       weights <- inverse_distances * random_weights * contribution_factors
#       
#       # Step 5: Normalize Weights
#       normalized_weights <- weights / sum(weights)
#       
#       avatar_weights[i, ] <- normalized_weights
#     }
#     
#     return(avatar_weights)
#   }
#   
#   
#   
#   # Generate avatar weights
#    set.seed(seed_value)
#   avatar_weights <- generate_avatar_weights(knn_result, pca_transformed_data, k)
#   
#   
#   # Function to generate avatars in PCA space based on weights
#   generate_avatars_pca_space <- function(pca_transformed_data, knn_indices, weights) {
#     n <- nrow(pca_transformed_data)
#     avatars_pca <- matrix(nrow = n, ncol = ncol(pca_transformed_data))
#     
#     for (i in 1:n) {
#       weighted_avatars <- pca_transformed_data[knn_indices[i, ], ] * weights[i, ]
#       avatars_pca[i, ] <- colSums(weighted_avatars)
#     }
#     
#     return(avatars_pca)
#   }
#   # Generate avatars in PCA space
#   avatars_pca_space <- generate_avatars_pca_space(pca_transformed_data, knn_result$nn.index, avatar_weights)
#   
#     # Assuming 'aids_pca' is the PCA object and 'avatars_pca_space' contains the avatars in PCA space
#   # Inverse PCA transformation
#   inverse_pca <- function(pca_object, pca_data) {
#     return(pca_data %*% t(pca_object$rotation) + matrix(pca_object$center, nrow = nrow(pca_data), ncol = ncol(pca_object$rotation), byrow = TRUE))
#   }
#   avatars_original_scale <- inverse_pca(pca, avatars_pca_space)
#   
# 
#   # Inverse normalization (if the original data was normalized)
#   avatars_rescaled <- scale(avatars_original_scale, center = FALSE, scale = 1/attr(data_normalized, "scaled:scale"))
#   avatars_rescaled <- sweep(avatars_rescaled, 2, attr(data_normalized, "scaled:center"), "+")
#   
#   avatars_tibble_knn <- as_tibble(avatars_rescaled) %>% 
#     mutate(haplotype = round(haplotype, digits=0),
#            cyp3A5D = round(cyp3A5D, digits=0),
#            sexe_r  = round(sexe_r , digits=0),
#            sexe_d  = round(sexe_d , digits=0),
#            rejet_aigu  = round(rejet_aigu , digits=0),
#            event = round(event, digits=0)
#            # CYP3A4_1B = round(CYP3A4_1B, digits=0),
#            # MDR1_C1236T = round(MDR1_C1236T, digits=0),
#            # MDR1_G2677T = round(MDR1_G2677T, digits=0),
#            # MDR1_C3435T = round(MDR1_C3435T, digits=0)
#     ) 
#   
#   avatars_tibble_factor_knn <- as_tibble(avatars_rescaled) %>% 
#     mutate(haplotype = round(haplotype, digits=0),
#            cyp3A5D = round(cyp3A5D, digits=0),
#            sexe_r  = round(sexe_r , digits=0),
#            sexe_d  = round(sexe_d , digits=0),
#            rejet_aigu  = round(rejet_aigu , digits=0),
#            event = round(event, digits=0)
#            # CYP3A4_1B = round(CYP3A4_1B, digits=0),
#            # MDR1_C1236T = round(MDR1_C1236T, digits=0),
#            # MDR1_G2677T = round(MDR1_G2677T, digits=0),
#            # MDR1_C3435T = round(MDR1_C3435T, digits=0)
#     ) %>% 
#     mutate(haplotype = as.factor(haplotype),
#            cyp3A5D = as.factor(cyp3A5D),
#            sexe_r = as.factor(sexe_r),
#            sexe_d = as.factor(sexe_d),
#            # CYP3A4_1B = as.factor(CYP3A4_1B),
#            # MDR1_C1236T = as.factor(MDR1_C1236T),
#            # MDR1_G2677T = as.factor(MDR1_G2677T),
#            # MDR1_C3435T = as.factor(MDR1_C3435T),
#            rejet_aigu = as.factor(rejet_aigu))
# 
#   
#   # Finally, fit the Cox model
#   fit <- coxph(Surv(delai_event, event) ~ haplotype , data = avatars_tibble_knn)
# 
# 
#   # Calculate confidence intervals
#   ci <- confint(fit)
# 
#   return(list(fit = fit, ci = ci))
# }
# 
# extract_hrs_and_cis <- function(model_output) {
#   coefs <- model_output$fit$coefficients
#   ci <- model_output$ci
# 
#   hr <- exp(coefs)
#   ci_lower <- exp(ci[,"2.5 %"])
#   ci_upper <- exp(ci[,"97.5 %"])
# 
#   return(data.frame(variable = names(hr), hr = hr, ci_lower = ci_lower, ci_upper = ci_upper))
# }
# 
# run_for_k_values <- function(k) {
#   seed_values <- sample(x = 100)
#   model_results <- map(seed_values, ~run_analysis_for_k_and_seed(k, .x))
#   extracted_results <- map(model_results, extract_hrs_and_cis)
#   combined_results <- bind_rows(extracted_results)
#   
#   aggregate_metrics <- combined_results %>%
#     group_by(variable) %>%
#     summarize(
#       percentile_0 = quantile(hr, probs = 0, na.rm = TRUE),
#       percentile_5 = quantile(hr, probs = 0.05, na.rm = TRUE),
#       percentile_25 = quantile(hr, probs = 0.25, na.rm = TRUE),
#       percentile_50 = quantile(hr, probs = 0.5, na.rm = TRUE),
#       percentile_75 = quantile(hr, probs = 0.75, na.rm = TRUE),
#       percentile_95 = quantile(hr, probs = 0.95, na.rm = TRUE),
#       percentile_100 = quantile(hr, probs = 1, na.rm = TRUE)
#     )
#   
#  
#   
#   return(aggregate_metrics)
# }
# 
# # Define different k values
# k_values <- c(3, 5, 10, 15, 20, 50)
# 
# # Apply the analysis for each k value
# results_for_k_values <- map(k_values, run_for_k_values)
# names(results_for_k_values) <- paste("K =", k_values)
# 
# results_for_k_values
# 
# # datatable(percentile_metrics)
# knitr::kable(results_for_k_values, "simple")
```


# data augmentation avatar

## With 5 knn

We investigate the effect of data augmentaiotn with a defined seed and knn=5

```{r data augment knn5}
data_augment_avatar <- function(x) {
data_normalized <- scale(original1)
pca <- prcomp(data_normalized, scale. = FALSE)# pour selecitonner le nombre de cp rank. = 3
# Number of neighbors
k <- 5  # Adjust this based on your requirement
pca_transformed_data <- pca$x
knn_result <- get.knn(pca_transformed_data, k)

generate_avatar_weights <- function(knn_result, pca_transformed_data, k) {
  n <- nrow(pca_transformed_data)
  avatar_weights <- matrix(nrow = n, ncol = k)
  
  for (i in 1:n) {
    # Step 1: Inverse of Distances
    distances <- sqrt(rowSums((pca_transformed_data[knn_result$nn.index[i, ], ] - pca_transformed_data[i, ])^2))
    inverse_distances <- 1 / distances
    
    # Step 2: Random Weights

    random_weights <- rexp(k, rate = 1)
    
    # Step 3: Contribution Factors
 
    shuffled_indices <- sample(k)
    contribution_factors <- 1 / (2^shuffled_indices)
    
    # Step 4: Calculate Weights
    weights <- inverse_distances * random_weights * contribution_factors
    
    # Step 5: Normalize Weights
    normalized_weights <- weights / sum(weights)
    
    avatar_weights[i, ] <- normalized_weights
  }
  
  return(avatar_weights)
}



# Generate avatar weights
   set.seed( str_c(1,x))
avatar_weights <- generate_avatar_weights(knn_result, pca_transformed_data, k)

# Assuming pca_result, avatar_weights, knn_result$nn.index, and pca_transformed_data are already defined

# Function to generate avatars in PCA space based on weights
generate_avatars_pca_space <- function(pca_transformed_data, knn_indices, weights) {
  n <- nrow(pca_transformed_data)
  avatars_pca <- matrix(nrow = n, ncol = ncol(pca_transformed_data))
  
  for (i in 1:n) {
    weighted_avatars <- pca_transformed_data[knn_indices[i, ], ] * weights[i, ]
    avatars_pca[i, ] <- colSums(weighted_avatars)
  }
  
  return(avatars_pca)
}
# Generate avatars in PCA space
avatars_pca_space <- generate_avatars_pca_space(pca_transformed_data, knn_result$nn.index, avatar_weights)
# Assuming 'aids_pca' is the PCA object and 'avatars_pca_space' contains the avatars in PCA space
# Inverse PCA transformation
inverse_pca <- function(pca_object, pca_data) {
  return(pca_data %*% t(pca_object$rotation) + matrix(pca_object$center, nrow = nrow(pca_data), ncol = ncol(pca_object$rotation), byrow = TRUE))
}
avatars_original_scale <- inverse_pca(pca, avatars_pca_space)

# Assuming 'aids_data_normalized' contains the scaling attributes of the original data
# Inverse normalization (if the original data was normalized)
avatars_rescaled <- scale(avatars_original_scale, center = FALSE, scale = 1/attr(data_normalized, "scaled:scale"))
avatars_rescaled <- sweep(avatars_rescaled, 2, attr(data_normalized, "scaled:center"), "+")

avatars_tibble <- as_tibble(avatars_rescaled) %>% 
  mutate(haplotype = round(haplotype, digits=0),
         cyp3A5D = round(cyp3A5D, digits=0),
         sexe_r  = round(sexe_r , digits=0),
         sexe_d  = round(sexe_d , digits=0),
         rejet_aigu  = round(rejet_aigu , digits=0),
         event = round(event, digits=0)
         # CYP3A4_1B = round(CYP3A4_1B, digits=0),
         # MDR1_C1236T = round(MDR1_C1236T, digits=0),
         # MDR1_G2677T = round(MDR1_G2677T, digits=0),
         # MDR1_C3435T = round(MDR1_C3435T, digits=0)
  ) 
}

iteration <- c(1:4)

augmented_data_5 <- map_dfr(iteration, data_augment_avatar, .id = "iter_")

augmented_data_5 <- augmented_data_5 %>% 
  select(-iter_)

augmented_data_5_factor_knn5 <- augmented_data_5 %>% 
 mutate(haplotype = as.factor(haplotype),
         cyp3A5D = as.factor(cyp3A5D),
         sexe_r = as.factor(sexe_r),
         sexe_d = as.factor(sexe_d),
         # CYP3A4_1B = as.factor(CYP3A4_1B),
         # MDR1_C1236T = as.factor(MDR1_C1236T),
         # MDR1_G2677T = as.factor(MDR1_G2677T),
         # MDR1_C3435T = as.factor(MDR1_C3435T),
         rejet_aigu = as.factor(rejet_aigu))

```

Plot of the synthetic and original in the latent space

```{r}
# Combine original and synthetic data for visualization
combined_data <- rbind(
  original1 %>% mutate(DataType = 'Original'),
  augmented_data_5 %>% mutate(DataType = 'Synthetic')
)

# Perform PCA on combined data
combined_data_normalized <- scale(combined_data[, -which(names(combined_data) %in% c("DataType", "id"))])
combined_pca <- prcomp(combined_data_normalized, scale. = FALSE)

# Extract the first two principal components
combined_pca_data <- data.frame(combined_pca$x[, 1:2])
combined_pca_data$DataType <- combined_data$DataType

# Plot PCA with color differentiation
ggplot(combined_pca_data, aes(x = PC1, y = PC2, color = DataType)) +
  geom_point(alpha = 0.5) +
  theme_minimal() +
  labs(title = "PCA Plot", x = "Principal Component 1", y = "Principal Component 2", color = "Data Type")

```

Export data augmented knn = 5 

```{r}
write_csv(augmented_data_5, file = "avatar_sfpt_knn5_data_augmented.csv")
```

# Comparison of the datasets augmented and original


## Summary of the 2 datasets
```{r}
## Vector of categorical variables that need transformation
catVars <- c("haplotype", "cyp3A5D",  "sexe_r",  "sexe_d", 
"rejet_aigu", "event")
## Create a variable list.
vars <- c( "haplotype", "cyp3A5D", "age_r", "sexe_r", "age_d", "sexe_d", 
"rejet_aigu", "TIF", "event", "delai_event", "DataType")
tableOne <- CreateTableOne(vars = vars, strata = "DataType",factorVars = catVars, data = combined_data)
tableOne2<-print(tableOne, nonnormal = c( "age_r", "age_d", "TIF", "delai_event"), printToggle=F, minMax=T)
```

```{r , echo=F}
kableone(tableOne2)
```

```{r}
summary(original)
summary(augmented_data_5)
```

### individual data explorer

```{r}

# boxplots
plot_boxplot(combined_data , by ="DataType") 

# histograms

# Function to create histogram for each continuous variable
plot_histograms <- function(data, var_name, group_var) {
  ggplot(data, aes(x = !!sym(var_name), fill = !!sym(group_var))) +
    geom_histogram(alpha = 0.5,show.legend = FALSE) +
    labs(x = var_name, y = "Count") +
    theme_minimal() +
    ggtitle(paste(var_name))
}

# Using select_if to identify continuous variables and map to apply the function
plots <- combined_data %>%
  select( -sexe_r,-sexe_d) %>% 
  select_if(is.numeric) %>%
  names() %>%
  map(~plot_histograms(combined_data, ., "DataType"))

# Optionally, print or arrange plots (e.g., using gridExtra or patchwork packages)

wrap_plots(plots)
```

### plot correlation
```{r}

##Correlation Analysis
  cor_real <- cor(original1, use = "complete.obs")
  cor_synthetic <- cor(augmented_data_5, use = "complete.obs")
  
# plots
ggcorrplot(cor_real, hc.order = TRUE, type = "lower",
           lab = TRUE,  pch.cex = 5,
  tl.cex = 6, lab_size = 2)
# plots
ggcorrplot(cor_synthetic, hc.order = TRUE, type = "lower",
           lab = TRUE,  pch.cex = 5,
  tl.cex = 6, lab_size = 2)
```

## Modele de Cox

```{r}
# original
fit_original <- coxph(Surv(delai_event, event) ~ haplotype + cyp3A5D +  age_r + sexe_r + age_d +  sexe_d + rejet_aigu + TIF , data = original1)
summary(fit_original)
ggforest(fit_original)

# synthetique
fit_synthetique <- coxph(Surv(delai_event, event) ~ haplotype + cyp3A5D +  age_r + sexe_r + age_d +  sexe_d + rejet_aigu + TIF , data = augmented_data_5)
summary(fit_synthetique)
ggforest(fit_synthetique)
```

BootstepAIC augmented synthetic knn5

```{r}

boot.stepAIC(fit_synthetique, augmented_data_5, B = 100, k=log(nrow(augmented_data_5)))

```


Final model original

```{r}
fit_original <- coxph(Surv(delai_event, event) ~ haplotype + rejet_aigu , data = original1)
summary(fit_original)
```

Final model synthetic

```{r}
fit_synthetique <- coxph(Surv(delai_event, event) ~ haplotype + age_r + sexe_d + rejet_aigu , data = augmented_data_5)
summary(fit_synthetique)
```

# Modele final & KM

```{r}
km_original <- survfit(Surv(delai_event, event) ~ haplotype, data = original)
ggsurvplot(
  km_original,
  data = original,
  size = 1,                 # change line size
  conf.int = TRUE,          # Add confidence interval
  pval = TRUE,              # Add p-value
  risk.table = TRUE,        # Add risk table
  risk.table.col = "strata",# Risk table color by groups
  risk.table.height = 0.25, # Useful to change when you have multiple groups
  ggtheme = theme_bw()      # Change ggplot2 theme
)
```

```{r KM augmented avatar knn5}
km_synthetique <- survfit(Surv(delai_event, event) ~ haplotype, data = augmented_data_5_factor_knn5)
km_synthetique_avatar_5_augmented <- ggsurvplot(
  km_synthetique,
  data = augmented_data_5_factor_knn5,
  size = 1,                 # change line size
  conf.int = TRUE,          # Add confidence interval
  pval = TRUE,              # Add p-value
  risk.table = TRUE,        # Add risk table
  risk.table.col = "strata",# Risk table color by groups
  risk.table.height = 0.25, # Useful to change when you have multiple groups
  ggtheme = theme_bw()      # Change ggplot2 theme
)
km_synthetique_avatar_5_augmented
```


Plots original & synthetic combined

```{r}
## combine data
combined_df <- rbind(original %>% mutate(group = "original"), augmented_data_5_factor_knn5 %>% mutate(group = "synthetic")) %>% mutate(combined_haplotype = str_c(haplotype,"_", group ))

## fit the model
km_combined <- survfit(Surv(delai_event, event) ~ combined_haplotype, data = combined_df)

# plot
ggsurvplot(fit = km_combined, 
           data = combined_df,
           
  size = 1,                 # change line size
  conf.int = FALSE,          # Add confidence interval
  pval = TRUE,              # Add p-value
  risk.table = TRUE,        # Add risk table
  risk.table.col = "strata",# Risk table color by groups
  risk.table.height = 0.35, # Useful to change when you have multiple groups
  ggtheme = theme_bw()      # Change ggplot2 theme
)

```

Graphical exploraiotn of distribution

```{r}
library(GGally)

pm_knn5_augmented <- combined_df %>% select(haplotype:delai_event, group) %>% ggpairs(
  ggplot2::aes(colour = group,alpha = 0.5),
  upper = list(continuous = wrap("cor", size = 1.5)),
  lower=list(combo=wrap("facethist", binwidth=0.5))) + 
  theme(strip.text.x = element_text(size = 5),
           strip.text.y = element_text(size = 5),axis.text = element_text(size = 5))
pm_knn5_augmented
# ggsave("comparaison_distribution_augmented_knn5.pdf")

```

### Bootstrap of the coefficient for haplotype

Allow to define the variability range of HR for a given dataset (intra dataset variability)

```{r bootstrap augmented data knn5}
# Define the Cox model
cox_model <- function(data, indices) {
  d <- data[indices,] # allows bootstrapping to sample the data
  fit <- coxph(Surv(delai_event, event) ~ haplotype + age_r + sexe_d + rejet_aigu , data=d)
  return(fit$coefficients)
}

# Set the seed for reproducibility
set.seed(12)

# Bootstrap the Cox model
boot_results <- boot(data=augmented_data_5, statistic=cox_model, R=100)

# Convert bootstrap results to a data frame for ggplot2
boot_hrs <- exp(boot_results$t) # Convert log(HR) to HR
hr_data_haplo <- data.frame(HR=boot_hrs[,1])
hr_data_age_r <- data.frame(HR=boot_hrs[,2])
hr_data_sexe_d <- data.frame(HR=boot_hrs[,3])
hr_data_rejet_aigu <- data.frame(HR=boot_hrs[,4])
# Calculate summary statistics
summary_stats <- quantile(hr_data_haplo$HR, probs = c(0, 0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975, 1)) %>%  
  bind_rows(quantile(hr_data_age_r$HR, probs = c(0, 0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975, 1)) ) %>% 
  bind_rows(quantile(hr_data_sexe_d$HR, probs = c(0, 0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975, 1)) ) %>% 
  bind_rows(quantile(hr_data_rejet_aigu$HR, probs = c(0, 0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975, 1)) )  
names(summary_stats) <- c("Min","2.5th", "5th", "25th", "Median", "75th", "95th","97.5th","Max")

# Create the histogram
ggplot(hr_data_haplo, aes(x=HR)) +
  geom_histogram(bins=30, fill="#007a86", color="black") +
  # # geom_vline(aes(xintercept=summary_stats["Min"]), color="red", linetype="dashed") +
  # geom_vline(aes(xintercept=summary_stats["25th"][[1]][[1]]), color="gray", linetype="dashed", linewidth=2) +
  # geom_vline(aes(xintercept=summary_stats["Median"][[1]][[1]]), color="blue", linetype="dashed", linewidth=2) +
  # geom_vline(aes(xintercept=summary_stats["75th"])[[1]][[1]], color="gray", linetype="dashed", linewidth=2) +
  # geom_vline(aes(xintercept=summary_stats["Max"]), color="purple", linetype="dashed") +
  labs(title="Bootstrap Distribution of Hazard Ratios", x="Hazard Ratio (HR)", y="Frequency") +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5))

# Print summary statistics
knitr::kable(summary_stats, "simple")

```

Allow to define the inter variability range of HR for augmented knn=5 (inter dataset variability) by using 100 bootraps

```{r intervariability 100 dataset augmented knn5}
# Assuming all your existing functions and necessary libraries are loaded

run_model_with_seed <- function(seed_value) {
  
  
  # augmentaiotn of data
  
  data_augment_avatar <- function(x) {
    data_normalized <- scale(original1)
    pca <- prcomp(data_normalized, scale. = FALSE)# pour selecitonner le nombre de cp rank. = 3
    # Number of neighbors
    k <- 5 # Adjust this based on your requirement
    pca_transformed_data <- pca$x
    knn_result <- get.knn(pca_transformed_data, k)
    
    generate_avatar_weights <- function(knn_result, pca_transformed_data, k) {
      n <- nrow(pca_transformed_data)
      avatar_weights <- matrix(nrow = n, ncol = k)
      
      for (i in 1:n) {
        # Step 1: Inverse of Distances
        distances <- sqrt(rowSums((pca_transformed_data[knn_result$nn.index[i, ], ] - pca_transformed_data[i, ])^2))
        inverse_distances <- 1 / distances
        
        # Step 2: Random Weights
        
        random_weights <- rexp(k, rate = 1)
        
        # Step 3: Contribution Factors
        
        shuffled_indices <- sample(k)
        contribution_factors <- 1 / (2^shuffled_indices)
        
        # Step 4: Calculate Weights
        weights <- inverse_distances * random_weights * contribution_factors
        
        # Step 5: Normalize Weights
        normalized_weights <- weights / sum(weights)
        
        avatar_weights[i, ] <- normalized_weights
      }
      
      return(avatar_weights)
    }
    
    
    
    # Generate avatar weights
 
    avatar_weights <- generate_avatar_weights(knn_result, pca_transformed_data, k)
    
    # Assuming pca_result, avatar_weights, knn_result$nn.index, and pca_transformed_data are already defined
    
    # Function to generate avatars in PCA space based on weights
    generate_avatars_pca_space <- function(pca_transformed_data, knn_indices, weights) {
      n <- nrow(pca_transformed_data)
      avatars_pca <- matrix(nrow = n, ncol = ncol(pca_transformed_data))
      
      for (i in 1:n) {
        weighted_avatars <- pca_transformed_data[knn_indices[i, ], ] * weights[i, ]
        avatars_pca[i, ] <- colSums(weighted_avatars)
      }
      
      return(avatars_pca)
    }
    # Generate avatars in PCA space
    avatars_pca_space <- generate_avatars_pca_space(pca_transformed_data, knn_result$nn.index, avatar_weights)
    # Assuming 'aids_pca' is the PCA object and 'avatars_pca_space' contains the avatars in PCA space
    # Inverse PCA transformation
    inverse_pca <- function(pca_object, pca_data) {
      return(pca_data %*% t(pca_object$rotation) + matrix(pca_object$center, nrow = nrow(pca_data), ncol = ncol(pca_object$rotation), byrow = TRUE))
    }
    avatars_original_scale <- inverse_pca(pca, avatars_pca_space)
    
    # Assuming 'aids_data_normalized' contains the scaling attributes of the original data
    # Inverse normalization (if the original data was normalized)
    avatars_rescaled <- scale(avatars_original_scale, center = FALSE, scale = 1/attr(data_normalized, "scaled:scale"))
    avatars_rescaled <- sweep(avatars_rescaled, 2, attr(data_normalized, "scaled:center"), "+")
    
    avatars_tibble <- as_tibble(avatars_rescaled) %>% 
      mutate(haplotype = round(haplotype, digits=0),
             cyp3A5D = round(cyp3A5D, digits=0),
             sexe_r  = round(sexe_r , digits=0),
             sexe_d  = round(sexe_d , digits=0),
             rejet_aigu  = round(rejet_aigu , digits=0),
             event = round(event, digits=0)
             # CYP3A4_1B = round(CYP3A4_1B, digits=0),
             # MDR1_C1236T = round(MDR1_C1236T, digits=0),
             # MDR1_G2677T = round(MDR1_G2677T, digits=0),
             # MDR1_C3435T = round(MDR1_C3435T, digits=0)
      ) 
  }
  
  iteration <- c(1:4)
  set.seed(seed_value)
  augmented_data_x <- map_dfr(iteration, data_augment_avatar, .id = "iter_")
  
  augmented_data_x <- augmented_data_x %>% 
    select(-iter_)
  
  
  ###############
  # Finally, fit the Cox model
  fit <- coxph(Surv(delai_event, event) ~ haplotype +age_r + sexe_d + rejet_aigu, 
               data = augmented_data_x)
  coefs <- fit$coefficients
  hr <- exp(coefs)
  return(data.frame(variable = names(hr), hr = hr))
  # Calculate confidence intervals
#  ci <- confint(fit)
  
 # return(list(fit = fit, ci = ci))
}




# Generate a list of seed values
seed_value <- sample(x=100) # Modify this if you need different seed values

# Apply the algorithm with different seed values
model_results <- map(seed_value, run_model_with_seed)

# Extract HR and CI from model results
#extracted_results <- map(model_results, extract_hrs_and_cis)

# Combine results into a single data frame
combined_results <- bind_rows(model_results)

# Calculate median HR and CI for each variable
# aggregate_metrics <- combined_results %>%
#   group_by(variable) %>%
#   summarize(
#     median_hr = median(hr),
#     median_ci_lower = median(ci_lower),
#     median_ci_upper = median(ci_upper)
#   )
# 
# aggregate_metrics

# Calculate the specified percentiles for HRs for each variable
percentile_metrics <- combined_results %>%
  group_by(variable) %>%
  summarize(
    percentile_0 = quantile(hr, probs = 0),
    percentile_5 = quantile(hr, probs = 0.05),
    percentile_25 = quantile(hr, probs = 0.25),
    percentile_50 = quantile(hr, probs = 0.5),
    percentile_75 = quantile(hr, probs = 0.75),
    percentile_95 = quantile(hr, probs = 0.95),
    percentile_100 = quantile(hr, probs = 1)
  ) %>%
  pivot_longer(-variable, names_to = "Percentile_HR", values_to = "Value_HR") %>% 
  mutate(Value_HR = round(Value_HR, 2))
# percentile_metrics
# datatable(percentile_metrics)
knitr::kable(percentile_metrics %>%  mutate(Value_HR = round(Value_HR, 2)), "simple")
```


# KNN=20

```{r}
# Number of neighbors
k <- 20  # Adjust this based on your requirement
```

algorithm

```{r avatar basal knn20}
pca_transformed_data <- pca$x
knn_result <- get.knn(pca_transformed_data, k)

generate_avatar_weights <- function(knn_result, pca_transformed_data, k) {
  n <- nrow(pca_transformed_data)
  avatar_weights <- matrix(nrow = n, ncol = k)
  
  for (i in 1:n) {
    # Step 1: Inverse of Distances
    distances <- sqrt(rowSums((pca_transformed_data[knn_result$nn.index[i, ], ] - pca_transformed_data[i, ])^2))
    inverse_distances <- 1 / distances
    
    # Step 2: Random Weights
   
    random_weights <- rexp(k, rate = 1)
    
    # Step 3: Contribution Factors
   
    shuffled_indices <- sample(k)
    contribution_factors <- 1 / (2^shuffled_indices)
    
    # Step 4: Calculate Weights
    weights <- inverse_distances * random_weights * contribution_factors
    
    # Step 5: Normalize Weights
    normalized_weights <- weights / sum(weights)
    
    avatar_weights[i, ] <- normalized_weights
  }
  
  return(avatar_weights)
}



# Generate avatar weights
 set.seed(12)
avatar_weights <- generate_avatar_weights(knn_result, pca_transformed_data, k)
```


Generation of avatar in the latent space

```{r}
# Assuming pca_result, avatar_weights, knn_result$nn.index, and pca_transformed_data are already defined

# Function to generate avatars in PCA space based on weights
generate_avatars_pca_space <- function(pca_transformed_data, knn_indices, weights) {
  n <- nrow(pca_transformed_data)
  avatars_pca <- matrix(nrow = n, ncol = ncol(pca_transformed_data))
  
  for (i in 1:n) {
    weighted_avatars <- pca_transformed_data[knn_indices[i, ], ] * weights[i, ]
    avatars_pca[i, ] <- colSums(weighted_avatars)
  }
  
  return(avatars_pca)
}
# Generate avatars in PCA space
avatars_pca_space <- generate_avatars_pca_space(pca_transformed_data, knn_result$nn.index, avatar_weights)

```

Return to the initial scale

```{r}
# Assuming 'aids_pca' is the PCA object and 'avatars_pca_space' contains the avatars in PCA space
# Inverse PCA transformation
inverse_pca <- function(pca_object, pca_data) {
  return(pca_data %*% t(pca_object$rotation) + matrix(pca_object$center, nrow = nrow(pca_data), ncol = ncol(pca_object$rotation), byrow = TRUE))
}
avatars_original_scale <- inverse_pca(pca, avatars_pca_space)

# Assuming 'aids_data_normalized' contains the scaling attributes of the original data
# Inverse normalization (if the original data was normalized)
avatars_rescaled <- scale(avatars_original_scale, center = FALSE, scale = 1/attr(data_normalized, "scaled:scale"))
avatars_rescaled <- sweep(avatars_rescaled, 2, attr(data_normalized, "scaled:center"), "+")


```

Transform into tibble

```{r}
avatars_tibble_knn20 <- as_tibble(avatars_rescaled) %>% 
  mutate(haplotype = round(haplotype, digits=0),
         cyp3A5D = round(cyp3A5D, digits=0),
         sexe_r  = round(sexe_r , digits=0),
         sexe_d  = round(sexe_d , digits=0),
         rejet_aigu  = round(rejet_aigu , digits=0),
         event = round(event, digits=0)
         # CYP3A4_1B = round(CYP3A4_1B, digits=0),
         # MDR1_C1236T = round(MDR1_C1236T, digits=0),
         # MDR1_G2677T = round(MDR1_G2677T, digits=0),
         # MDR1_C3435T = round(MDR1_C3435T, digits=0)
  ) 

avatars_tibble_factor_knn20 <- as_tibble(avatars_rescaled) %>% 
  mutate(haplotype = round(haplotype, digits=0),
         cyp3A5D = round(cyp3A5D, digits=0),
         sexe_r  = round(sexe_r , digits=0),
         sexe_d  = round(sexe_d , digits=0),
         rejet_aigu  = round(rejet_aigu , digits=0),
         event = round(event, digits=0)
         # CYP3A4_1B = round(CYP3A4_1B, digits=0),
         # MDR1_C1236T = round(MDR1_C1236T, digits=0),
         # MDR1_G2677T = round(MDR1_G2677T, digits=0),
         # MDR1_C3435T = round(MDR1_C3435T, digits=0)
  ) %>% 
  mutate(haplotype = as.factor(haplotype),
         cyp3A5D = as.factor(cyp3A5D),
         sexe_r = as.factor(sexe_r),
         sexe_d = as.factor(sexe_d),
         # CYP3A4_1B = as.factor(CYP3A4_1B),
         # MDR1_C1236T = as.factor(MDR1_C1236T),
         # MDR1_G2677T = as.factor(MDR1_G2677T),
         # MDR1_C3435T = as.factor(MDR1_C3435T),
         rejet_aigu = as.factor(rejet_aigu))
```

Plot of the synthetic and original in the latent space

```{r}
# Combine original and synthetic data for visualization
combined_data <- rbind(
  original1 %>% mutate(DataType = 'Original'),
  avatars_tibble_knn20 %>% mutate(DataType = 'Synthetic')
)

# Perform PCA on combined data
combined_data_normalized <- scale(combined_data[, -which(names(combined_data) %in% c("DataType", "id"))])
combined_pca <- prcomp(combined_data_normalized, scale. = FALSE)

# Extract the first two principal components
combined_pca_data <- data.frame(combined_pca$x[, 1:2])
combined_pca_data$DataType <- combined_data$DataType

# Plot PCA with color differentiation
ggplot(combined_pca_data, aes(x = PC1, y = PC2, color = DataType)) +
  geom_point(alpha = 0.8) +
  theme_minimal() +
  labs(title = "PCA Plot", x = "Principal Component 1", y = "Principal Component 2", color = "Data Type")

```
Export data knn = 20

```{r}
write_csv(avatars_tibble_knn20, file = "avatar_sfpt_knn20.csv")
```


### Comparison of the datasets

#### Summary of the 2 datasets

```{r}
## Vector of categorical variables that need transformation
catVars <- c("haplotype", "cyp3A5D",  "sexe_r",  "sexe_d", 
"rejet_aigu", "event")
## Create a variable list.
vars <- c( "haplotype", "cyp3A5D", "age_r", "sexe_r", "age_d", "sexe_d", 
"rejet_aigu", "TIF", "event", "delai_event", "DataType")
tableOne <- CreateTableOne(vars = vars, strata = "DataType",factorVars = catVars, data = combined_data)
tableOne2<-print(tableOne, nonnormal = c( "age_r", "age_d", "TIF", "delai_event"), printToggle=F, minMax=T)
```

```{r , echo=F}
kableone(tableOne2)
```

```{r}
summary(original)
summary(avatars_tibble_knn20)
```

#### individual data explorer

```{r}

# boxplots
plot_boxplot(combined_data , by ="DataType") 

# histograms

# Function to create histogram for each continuous variable
plot_histograms <- function(data, var_name, group_var) {
  ggplot(data, aes(x = !!sym(var_name), fill = !!sym(group_var))) +
    geom_histogram(alpha = 0.5,show.legend = FALSE) +
    labs(x = var_name, y = "Count") +
    theme_minimal() +
    ggtitle(paste(var_name))
}

# Using select_if to identify continuous variables and map to apply the function
plots <- combined_data %>%
  select( -sexe_r,-sexe_d) %>% 
  select_if(is.numeric) %>%
  names() %>%
  map(~plot_histograms(combined_data, ., "DataType"))

# Optionally, print or arrange plots (e.g., using gridExtra or patchwork packages)

wrap_plots(plots)
```

#### plot correlation
```{r}

##Correlation Analysis
  cor_real <- cor(original1, use = "complete.obs")
  cor_synthetic <- cor(avatars_tibble_knn20, use = "complete.obs")
  
# plots
ggcorrplot(cor_real, hc.order = TRUE, type = "lower",
           lab = TRUE,  pch.cex = 5,
  tl.cex = 6, lab_size = 2)
# plots
ggcorrplot(cor_synthetic, hc.order = TRUE, type = "lower",
           lab = TRUE,  pch.cex = 5,
  tl.cex = 6, lab_size = 2)
```

#### Modele de Cox

```{r}
# original
fit_original <- coxph(Surv(delai_event, event) ~ haplotype + cyp3A5D +  age_r + sexe_r + age_d +  sexe_d + rejet_aigu + TIF , data = original1)
summary(fit_original)
ggforest(fit_original)

# synthetique
fit_synthetique <- coxph(Surv(delai_event, event) ~ haplotype + cyp3A5D +  age_r + sexe_r + age_d +  sexe_d + rejet_aigu + TIF, data = avatars_tibble_knn20)
summary(fit_synthetique)
ggforest(fit_synthetique)

```

#### BootstepAIC based on BIC

Allow to see which vairable would have been selected

Original

```{r}

boot.stepAIC(fit_original, original1, B = 100, k=log(nrow(original1)))

```

synhtetic knn20

```{r}

boot.stepAIC(fit_synthetique, avatars_tibble_knn20, B = 100, k=log(nrow(avatars_tibble_knn20)))

```


##### Final model original

```{r}
fit_original <- coxph(Surv(delai_event, event) ~ haplotype , data = original1)
summary(fit_original)
```

##### Final model synthetic

```{r}
fit_synthetique <- coxph(Surv(delai_event, event) ~ haplotype  , data = avatars_tibble_knn20)
summary(fit_synthetique)
```

### Bootstrap of the coefficient for haplotype

Allow to define the variability range of HR for a given dataset (intra dataset variability)

```{r bootstrap knn20} 
# Define the Cox model
cox_model <- function(data, indices) {
  d <- data[indices,] # allows bootstrapping to sample the data
  fit <- coxph(Surv(delai_event, event) ~ haplotype , data=d)
  return(fit$coefficients)
}

# Set the seed for reproducibility
set.seed(12)

# Bootstrap the Cox model
boot_results <- boot(data=avatars_tibble_knn20, statistic=cox_model, R=100)

# Convert bootstrap results to a data frame for ggplot2
boot_hrs <- exp(boot_results$t) # Convert log(HR) to HR
hr_data <- data.frame(HR=boot_hrs[,1])

# Calculate summary statistics
summary_stats <- quantile(hr_data$HR, probs = c(0, 0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975, 1))
names(summary_stats) <- c("Min","2.5th", "5th", "25th", "Median", "75th", "95th","97.5th","Max")

# Create the histogram
ggplot(hr_data, aes(x=HR)) +
  geom_histogram(bins=30, fill="#007a86", color="black") +
  # geom_vline(aes(xintercept=summary_stats["Min"]), color="red", linetype="dashed") +
  geom_vline(aes(xintercept=summary_stats["25th"]), color="gray", linetype="dashed", linewidth=2) +
  geom_vline(aes(xintercept=summary_stats["Median"]), color="blue", linetype="dashed", linewidth=2) +
  geom_vline(aes(xintercept=summary_stats["75th"]), color="gray", linetype="dashed", linewidth=2) +
  # geom_vline(aes(xintercept=summary_stats["Max"]), color="purple", linetype="dashed") +
  labs(title="Bootstrap Distribution of Hazard Ratios", x="Hazard Ratio (HR)", y="Frequency") +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5))

# Print summary statistics
print(summary_stats)


```


##### Modele final & KM

```{r KM original knn20}
km_original <- survfit(Surv(delai_event, event) ~ haplotype, data = original)
km_original_plot <- ggsurvplot(
  km_original,
  data = original,
  size = 1,                 # change line size
  conf.int = TRUE,          # Add confidence interval
  pval = TRUE,              # Add p-value
  risk.table = TRUE,        # Add risk table
  risk.table.col = "strata",# Risk table color by groups
  risk.table.height = 0.25, # Useful to change when you have multiple groups
  ggtheme = theme_bw()      # Change ggplot2 theme
)
km_original_plot
```

```{r KM avatar KNN20}
km_synthetique <- survfit(Surv(delai_event, event) ~ haplotype, data = avatars_tibble_knn20)
km_synthetique_avatar_20 <- ggsurvplot(
  km_synthetique,
  data = avatars_tibble_knn20,
  size = 1,                 # change line size
  conf.int = TRUE,          # Add confidence interval
  pval = TRUE,              # Add p-value
  risk.table = TRUE,        # Add risk table
  risk.table.col = "strata",# Risk table color by groups
  risk.table.height = 0.25, # Useful to change when you have multiple groups
  ggtheme = theme_bw()      # Change ggplot2 theme
)
km_synthetique_avatar_20
```


Plots original & synthetic combined

```{r}
## combine data
combined_df <- rbind(original %>% mutate(group = "original"), avatars_tibble_factor_knn20 %>% mutate(group = "synthetic")) %>% mutate(combined_haplotype = str_c(haplotype,"_", group ))

## fit the model
km_combined <- survfit(Surv(delai_event, event) ~ combined_haplotype, data = combined_df)

# plot
ggsurvplot(fit = km_combined, 
           data = combined_df,
           
  size = 1,                 # change line size
  conf.int = FALSE,          # Add confidence interval
  pval = TRUE,              # Add p-value
  risk.table = TRUE,        # Add risk table
  risk.table.col = "strata",# Risk table color by groups
  risk.table.height = 0.35, # Useful to change when you have multiple groups
  ggtheme = theme_bw()      # Change ggplot2 theme
)

```

#### Graphical exploraiotn of distribution

```{r}
library(GGally)

pm_knn20 <- combined_df %>% select(haplotype:delai_event, group) %>% ggpairs(
  ggplot2::aes(colour = group,alpha = 0.5),
  upper = list(continuous = wrap("cor", size = 1.5)),
  lower=list(combo=wrap("facethist", binwidth=0.5))) + 
  theme(strip.text.x = element_text(size = 5),
           strip.text.y = element_text(size = 5),axis.text = element_text(size = 5))
pm_knn20
# ggsave("comparaison_distribution_knn20.pdf")

```

### function to aggregating the HR and CI95 results of Cox models after changing 100 times the seed with  knn = 20
Allow to define the variability range of HR for different Avatar generated with different seed but the same knn (inter dataset variability)

```{r change seed knn20}

# Assuming all your existing functions and necessary libraries are loaded

run_model_with_seed <- function(seed_value) {
 
  
  # Number of neighbors
  k <- 20  # Adjust this based on your requirement
  
  pca_transformed_data <- pca$x
  knn_result <- get.knn(pca_transformed_data, k)
  
  generate_avatar_weights <- function(knn_result, pca_transformed_data, k) {
    n <- nrow(pca_transformed_data)
    avatar_weights <- matrix(nrow = n, ncol = k)
    
    for (i in 1:n) {
      # Step 1: Inverse of Distances
      distances <- sqrt(rowSums((pca_transformed_data[knn_result$nn.index[i, ], ] - pca_transformed_data[i, ])^2))
      inverse_distances <- 1 / distances
      
      # Step 2: Random Weights

      random_weights <- rexp(k, rate = 1)
      
      # Step 3: Contribution Factors
  
      shuffled_indices <- sample(k)
      contribution_factors <- 1 / (2^shuffled_indices)
      
      # Step 4: Calculate Weights
      weights <- inverse_distances * random_weights * contribution_factors
      
      # Step 5: Normalize Weights
      normalized_weights <- weights / sum(weights)
      
      avatar_weights[i, ] <- normalized_weights
    }
    
    return(avatar_weights)
  }
  
  
  
  # Generate avatar weights
   set.seed(seed_value)
  avatar_weights <- generate_avatar_weights(knn_result, pca_transformed_data, k)
  
  
  # Function to generate avatars in PCA space based on weights
  generate_avatars_pca_space <- function(pca_transformed_data, knn_indices, weights) {
    n <- nrow(pca_transformed_data)
    avatars_pca <- matrix(nrow = n, ncol = ncol(pca_transformed_data))
    
    for (i in 1:n) {
      weighted_avatars <- pca_transformed_data[knn_indices[i, ], ] * weights[i, ]
      avatars_pca[i, ] <- colSums(weighted_avatars)
    }
    
    return(avatars_pca)
  }
  # Generate avatars in PCA space
  avatars_pca_space <- generate_avatars_pca_space(pca_transformed_data, knn_result$nn.index, avatar_weights)
  

  # Inverse PCA transformation
  inverse_pca <- function(pca_object, pca_data) {
    return(pca_data %*% t(pca_object$rotation) + matrix(pca_object$center, nrow = nrow(pca_data), ncol = ncol(pca_object$rotation), byrow = TRUE))
  }
  avatars_original_scale <- inverse_pca(pca, avatars_pca_space)
  
 
  # Inverse normalization (if the original data was normalized)
  avatars_rescaled <- scale(avatars_original_scale, center = FALSE, scale = 1/attr(data_normalized, "scaled:scale"))
  avatars_rescaled <- sweep(avatars_rescaled, 2, attr(data_normalized, "scaled:center"), "+")
  
avatars_tibble_knn20   <- as_tibble(avatars_rescaled) %>% 
    mutate(haplotype = round(haplotype, digits=0),
           cyp3A5D = round(cyp3A5D, digits=0),
           sexe_r  = round(sexe_r , digits=0),
           sexe_d  = round(sexe_d , digits=0),
           rejet_aigu  = round(rejet_aigu , digits=0),
           event = round(event, digits=0)
           # CYP3A4_1B = round(CYP3A4_1B, digits=0),
           # MDR1_C1236T = round(MDR1_C1236T, digits=0),
           # MDR1_G2677T = round(MDR1_G2677T, digits=0),
           # MDR1_C3435T = round(MDR1_C3435T, digits=0)
    ) 
  
avatars_tibble_factor_knn20   <- as_tibble(avatars_rescaled) %>% 
    mutate(haplotype = round(haplotype, digits=0),
           cyp3A5D = round(cyp3A5D, digits=0),
           sexe_r  = round(sexe_r , digits=0),
           sexe_d  = round(sexe_d , digits=0),
           rejet_aigu  = round(rejet_aigu , digits=0),
           event = round(event, digits=0)
           # CYP3A4_1B = round(CYP3A4_1B, digits=0),
           # MDR1_C1236T = round(MDR1_C1236T, digits=0),
           # MDR1_G2677T = round(MDR1_G2677T, digits=0),
           # MDR1_C3435T = round(MDR1_C3435T, digits=0)
    ) %>% 
    mutate(haplotype = as.factor(haplotype),
           cyp3A5D = as.factor(cyp3A5D),
           sexe_r = as.factor(sexe_r),
           sexe_d = as.factor(sexe_d),
           # CYP3A4_1B = as.factor(CYP3A4_1B),
           # MDR1_C1236T = as.factor(MDR1_C1236T),
           # MDR1_G2677T = as.factor(MDR1_G2677T),
           # MDR1_C3435T = as.factor(MDR1_C3435T),
           rejet_aigu = as.factor(rejet_aigu))

  
  # Finally, fit the Cox model
  fit <- coxph(Surv(delai_event, event) ~ haplotype , 
               data = avatars_tibble_knn20)

  # Calculate confidence intervals
  ci <- confint(fit)
  
  return(list(fit = fit, ci = ci))
}


extract_hrs_and_cis <- function(model_output) {
  coefs <- model_output$fit$coefficients
  ci <- model_output$ci

  hr <- exp(coefs)
  ci_lower <- exp(ci[,"2.5 %"])
  ci_upper <- exp(ci[,"97.5 %"])

  return(data.frame(variable = names(hr), hr = hr, ci_lower = ci_lower, ci_upper = ci_upper))
}

# Generate a list of seed values
seed_values <- sample(x=100) # Modify this if you need different seed values

# Apply the algorithm with different seed values
model_results <- map(seed_values, run_model_with_seed)

# Extract HR and CI from model results
extracted_results <- map(model_results, extract_hrs_and_cis)

# Combine results into a single data frame
combined_results <- bind_rows(extracted_results)

# Calculate median HR and CI for each variable
# aggregate_metrics <- combined_results %>%
#   group_by(variable) %>%
#   summarize(
#     median_hr = median(hr),
#     median_ci_lower = median(ci_lower),
#     median_ci_upper = median(ci_upper)
#   )
# 
# aggregate_metrics

# Calculate the specified percentiles for HRs for each variable
percentile_metrics <- combined_results %>%
  group_by(variable) %>%
  summarize(
    percentile_0 = quantile(hr, probs = 0),
    percentile_5 = quantile(hr, probs = 0.05),
    percentile_25 = quantile(hr, probs = 0.25),
    percentile_50 = quantile(hr, probs = 0.5),
    percentile_75 = quantile(hr, probs = 0.75),
    percentile_95 = quantile(hr, probs = 0.95),
    percentile_100 = quantile(hr, probs = 1)
  ) %>%
  pivot_longer(-variable, names_to = "Percentile_HR", values_to = "Value_HR") %>% 
  mutate(Value_HR = round(Value_HR, 2))
# percentile_metrics
# datatable(percentile_metrics)
knitr::kable(percentile_metrics %>%  mutate(Value_HR = round(Value_HR, 2)), "simple")

```


# data augmentation avatar

## With 20 knn

We investigate the effect of data augmentaiotn with a defined seed and knn=20

```{r data augment knn20}
data_augment_avatar <- function(x) {
data_normalized <- scale(original1)
pca <- prcomp(data_normalized, scale. = FALSE)# pour selecitonner le nombre de cp rank. = 3
# Number of neighbors
k <- 20  # Adjust this based on your requirement
pca_transformed_data <- pca$x
knn_result <- get.knn(pca_transformed_data, k)

generate_avatar_weights <- function(knn_result, pca_transformed_data, k) {
  n <- nrow(pca_transformed_data)
  avatar_weights <- matrix(nrow = n, ncol = k)
  
  for (i in 1:n) {
    # Step 1: Inverse of Distances
    distances <- sqrt(rowSums((pca_transformed_data[knn_result$nn.index[i, ], ] - pca_transformed_data[i, ])^2))
    inverse_distances <- 1 / distances
    
    # Step 2: Random Weights

    random_weights <- rexp(k, rate = 1)
    
    # Step 3: Contribution Factors
 
    shuffled_indices <- sample(k)
    contribution_factors <- 1 / (2^shuffled_indices)
    
    # Step 4: Calculate Weights
    weights <- inverse_distances * random_weights * contribution_factors
    
    # Step 5: Normalize Weights
    normalized_weights <- weights / sum(weights)
    
    avatar_weights[i, ] <- normalized_weights
  }
  
  return(avatar_weights)
}



# Generate avatar weights
   set.seed( str_c(1,x))
avatar_weights <- generate_avatar_weights(knn_result, pca_transformed_data, k)

# Assuming pca_result, avatar_weights, knn_result$nn.index, and pca_transformed_data are already defined

# Function to generate avatars in PCA space based on weights
generate_avatars_pca_space <- function(pca_transformed_data, knn_indices, weights) {
  n <- nrow(pca_transformed_data)
  avatars_pca <- matrix(nrow = n, ncol = ncol(pca_transformed_data))
  
  for (i in 1:n) {
    weighted_avatars <- pca_transformed_data[knn_indices[i, ], ] * weights[i, ]
    avatars_pca[i, ] <- colSums(weighted_avatars)
  }
  
  return(avatars_pca)
}
# Generate avatars in PCA space
avatars_pca_space <- generate_avatars_pca_space(pca_transformed_data, knn_result$nn.index, avatar_weights)
# Assuming 'aids_pca' is the PCA object and 'avatars_pca_space' contains the avatars in PCA space
# Inverse PCA transformation
inverse_pca <- function(pca_object, pca_data) {
  return(pca_data %*% t(pca_object$rotation) + matrix(pca_object$center, nrow = nrow(pca_data), ncol = ncol(pca_object$rotation), byrow = TRUE))
}
avatars_original_scale <- inverse_pca(pca, avatars_pca_space)

# Assuming 'aids_data_normalized' contains the scaling attributes of the original data
# Inverse normalization (if the original data was normalized)
avatars_rescaled <- scale(avatars_original_scale, center = FALSE, scale = 1/attr(data_normalized, "scaled:scale"))
avatars_rescaled <- sweep(avatars_rescaled, 2, attr(data_normalized, "scaled:center"), "+")

avatars_tibble <- as_tibble(avatars_rescaled) %>% 
  mutate(haplotype = round(haplotype, digits=0),
         cyp3A5D = round(cyp3A5D, digits=0),
         sexe_r  = round(sexe_r , digits=0),
         sexe_d  = round(sexe_d , digits=0),
         rejet_aigu  = round(rejet_aigu , digits=0),
         event = round(event, digits=0)
         # CYP3A4_1B = round(CYP3A4_1B, digits=0),
         # MDR1_C1236T = round(MDR1_C1236T, digits=0),
         # MDR1_G2677T = round(MDR1_G2677T, digits=0),
         # MDR1_C3435T = round(MDR1_C3435T, digits=0)
  ) 
}

iteration <- c(1:4)

augmented_data_20 <- map_dfr(iteration, data_augment_avatar, .id = "iter_")

augmented_data_20 <- augmented_data_20 %>% 
  select(-iter_)

augmented_data_20_factor_knn20 <- augmented_data_20 %>% 
 mutate(haplotype = as.factor(haplotype),
         cyp3A5D = as.factor(cyp3A5D),
         sexe_r = as.factor(sexe_r),
         sexe_d = as.factor(sexe_d),
         # CYP3A4_1B = as.factor(CYP3A4_1B),
         # MDR1_C1236T = as.factor(MDR1_C1236T),
         # MDR1_G2677T = as.factor(MDR1_G2677T),
         # MDR1_C3435T = as.factor(MDR1_C3435T),
         rejet_aigu = as.factor(rejet_aigu))

```

Plot of the synthetic and original in the latent space

```{r}
# Combine original and synthetic data for visualization
combined_data <- rbind(
  original1 %>% mutate(DataType = 'Original'),
  augmented_data_20 %>% mutate(DataType = 'Synthetic')
)

# Perform PCA on combined data
combined_data_normalized <- scale(combined_data[, -which(names(combined_data) %in% c("DataType", "id"))])
combined_pca <- prcomp(combined_data_normalized, scale. = FALSE)

# Extract the first two principal components
combined_pca_data <- data.frame(combined_pca$x[, 1:2])
combined_pca_data$DataType <- combined_data$DataType

# Plot PCA with color differentiation
ggplot(combined_pca_data, aes(x = PC1, y = PC2, color = DataType)) +
  geom_point(alpha = 0.5) +
  theme_minimal() +
  labs(title = "PCA Plot", x = "Principal Component 1", y = "Principal Component 2", color = "Data Type")

```

Export data augmented knn = 20 

```{r}
write_csv(augmented_data_20, file = "avatar_sfpt_knn20_data_augmented.csv")
```

# Comparison of the datasets augmented and original


## Summary of the 2 datasets
```{r}
## Vector of categorical variables that need transformation
catVars <- c("haplotype", "cyp3A5D",  "sexe_r",  "sexe_d", 
"rejet_aigu", "event")
## Create a variable list.
vars <- c( "haplotype", "cyp3A5D", "age_r", "sexe_r", "age_d", "sexe_d", 
"rejet_aigu", "TIF", "event", "delai_event", "DataType")
tableOne <- CreateTableOne(vars = vars, strata = "DataType",factorVars = catVars, data = combined_data)
tableOne2<-print(tableOne, nonnormal = c( "age_r", "age_d", "TIF", "delai_event"), printToggle=F, minMax=T)
```

```{r , echo=F}
kableone(tableOne2)
```

```{r}
summary(original)
summary(augmented_data_20)
```

### individual data explorer

```{r}

# boxplots
plot_boxplot(combined_data , by ="DataType") 

# histograms

# Function to create histogram for each continuous variable
plot_histograms <- function(data, var_name, group_var) {
  ggplot(data, aes(x = !!sym(var_name), fill = !!sym(group_var))) +
    geom_histogram(alpha = 0.5,show.legend = FALSE) +
    labs(x = var_name, y = "Count") +
    theme_minimal() +
    ggtitle(paste(var_name))
}

# Using select_if to identify continuous variables and map to apply the function
plots <- combined_data %>%
  select( -sexe_r,-sexe_d) %>% 
  select_if(is.numeric) %>%
  names() %>%
  map(~plot_histograms(combined_data, ., "DataType"))

# Optionally, print or arrange plots (e.g., using gridExtra or patchwork packages)

wrap_plots(plots)
```

### plot correlation
```{r}

##Correlation Analysis
  cor_real <- cor(original1, use = "complete.obs")
  cor_synthetic <- cor(augmented_data_20, use = "complete.obs")
  
# plots
ggcorrplot(cor_real, hc.order = TRUE, type = "lower",
           lab = TRUE,  pch.cex = 5,
  tl.cex = 6, lab_size = 2)
# plots
ggcorrplot(cor_synthetic, hc.order = TRUE, type = "lower",
           lab = TRUE,  pch.cex = 5,
  tl.cex = 6, lab_size = 2)
```

## Modele de Cox

```{r}
# original
fit_original <- coxph(Surv(delai_event, event) ~ haplotype + cyp3A5D +  age_r + sexe_r + age_d +  sexe_d + rejet_aigu + TIF , data = original1)
summary(fit_original)
ggforest(fit_original)

# synthetique
fit_synthetique <- coxph(Surv(delai_event, event) ~ haplotype + cyp3A5D +  age_r + sexe_r + age_d +  sexe_d + rejet_aigu + TIF , data = augmented_data_20)
summary(fit_synthetique)
ggforest(fit_synthetique)
```

BootstepAIC augmented synthetic knn20

```{r}

boot.stepAIC(fit_synthetique, augmented_data_20, B = 100, k=log(nrow(augmented_data_20)))

```


Final model original

```{r}
fit_original <- coxph(Surv(delai_event, event) ~ haplotype + rejet_aigu , data = original1)
summary(fit_original)
```

Final model synthetic

```{r}
fit_synthetique <- coxph(Surv(delai_event, event) ~ haplotype + cyp3A5D + age_d + rejet_aigu, data = augmented_data_20)
summary(fit_synthetique)
```

# Modele final & KM

```{r}
km_original <- survfit(Surv(delai_event, event) ~ haplotype, data = original)
ggsurvplot(
  km_original,
  data = original,
  size = 1,                 # change line size
  conf.int = TRUE,          # Add confidence interval
  pval = TRUE,              # Add p-value
  risk.table = TRUE,        # Add risk table
  risk.table.col = "strata",# Risk table color by groups
  risk.table.height = 0.25, # Useful to change when you have multiple groups
  ggtheme = theme_bw()      # Change ggplot2 theme
)
```

```{r KM augmented avatar knn20}
km_synthetique <- survfit(Surv(delai_event, event) ~ haplotype, data = augmented_data_20_factor_knn20)
km_synthetique_avatar_20_augmented <- ggsurvplot(
  km_synthetique,
  data = augmented_data_20_factor_knn20,
  size = 1,                 # change line size
  conf.int = TRUE,          # Add confidence interval
  pval = TRUE,              # Add p-value
  risk.table = TRUE,        # Add risk table
  risk.table.col = "strata",# Risk table color by groups
  risk.table.height = 0.25, # Useful to change when you have multiple groups
  ggtheme = theme_bw()      # Change ggplot2 theme
)
km_synthetique_avatar_20_augmented
```


Plots original & synthetic combined

```{r}
## combine data
combined_df <- rbind(original %>% mutate(group = "original"), augmented_data_20_factor_knn20 %>% mutate(group = "synthetic")) %>% mutate(combined_haplotype = str_c(haplotype,"_", group ))

## fit the model
km_combined <- survfit(Surv(delai_event, event) ~ combined_haplotype, data = combined_df)

# plot
ggsurvplot(fit = km_combined, 
           data = combined_df,
           
  size = 1,                 # change line size
  conf.int = FALSE,          # Add confidence interval
  pval = TRUE,              # Add p-value
  risk.table = TRUE,        # Add risk table
  risk.table.col = "strata",# Risk table color by groups
  risk.table.height = 0.35, # Useful to change when you have multiple groups
  ggtheme = theme_bw()      # Change ggplot2 theme
)

```

Graphical exploraiotn of distribution

```{r}
library(GGally)

pm_knn20_augmented <- combined_df %>% select(haplotype:delai_event, group) %>% ggpairs(
  ggplot2::aes(colour = group,alpha = 0.5),
  upper = list(continuous = wrap("cor", size = 1.5)),
  lower=list(combo=wrap("facethist", binwidth=0.5))) + 
  theme(strip.text.x = element_text(size = 5),
           strip.text.y = element_text(size = 5),axis.text = element_text(size = 5))
pm_knn20_augmented
# ggsave("comparaison_distribution_augmented_knn20.pdf")

```

### Bootstrap of the coefficient for haplotype

Allow to define the variability range of HR for a given dataset (intra dataset variability)

```{r bootstrap augmented data knn20}
# Define the Cox model
cox_model <- function(data, indices) {
  d <- data[indices,] # allows bootstrapping to sample the data
  fit <- coxph(Surv(delai_event, event) ~ haplotype + age_d + cyp3A5D + rejet_aigu , data=d)
  return(fit$coefficients)
}

# Set the seed for reproducibility
set.seed(12)

# Bootstrap the Cox model
boot_results <- boot(data=augmented_data_20, statistic=cox_model, R=100)

# Convert bootstrap results to a data frame for ggplot2
boot_hrs <- exp(boot_results$t) # Convert log(HR) to HR
hr_data_haplo <- data.frame(HR=boot_hrs[,1])
hr_data_age_d <- data.frame(HR=boot_hrs[,2])
hr_data_cyp3A5D <- data.frame(HR=boot_hrs[,3])
hr_data_rejet_aigu <- data.frame(HR=boot_hrs[,4])
# Calculate summary statistics
summary_stats <- quantile(hr_data_haplo$HR, probs = c(0, 0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975, 1)) %>%  
  bind_rows(quantile(hr_data_age_d$HR, probs = c(0, 0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975, 1)) ) %>% 
  bind_rows(quantile(hr_data_cyp3A5D$HR, probs = c(0, 0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975, 1)) ) %>% 
  bind_rows(quantile(hr_data_rejet_aigu$HR, probs = c(0, 0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975, 1)) )  
names(summary_stats) <- c("Min","2.5th", "5th", "25th", "Median", "75th", "95th","97.5th","Max")

# Create the histogram
ggplot(hr_data_haplo, aes(x=HR)) +
  geom_histogram(bins=30, fill="#007a86", color="black") +
  # # geom_vline(aes(xintercept=summary_stats["Min"]), color="red", linetype="dashed") +
  # geom_vline(aes(xintercept=summary_stats["25th"][[1]][[1]]), color="gray", linetype="dashed", linewidth=2) +
  # geom_vline(aes(xintercept=summary_stats["Median"][[1]][[1]]), color="blue", linetype="dashed", linewidth=2) +
  # geom_vline(aes(xintercept=summary_stats["75th"])[[1]][[1]], color="gray", linetype="dashed", linewidth=2) +
  # geom_vline(aes(xintercept=summary_stats["Max"]), color="purple", linetype="dashed") +
  labs(title="Bootstrap Distribution of Hazard Ratios", x="Hazard Ratio (HR)", y="Frequency") +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5))

# Print summary statistics
knitr::kable(summary_stats, "simple")

```

## inter seed vairbaility augmented knn20
Allow to define the inter variability range of HR for augmented knn=20 (inter dataset variability) by using 100 bootraps

```{r intervariability 100 dataset augmented knn20}
# Assuming all your existing functions and necessary libraries are loaded

run_model_with_seed <- function(seed_value) {
  
  
  # augmentaiotn of data
  
  data_augment_avatar <- function(x) {
    data_normalized <- scale(original1)
    pca <- prcomp(data_normalized, scale. = FALSE)# pour selecitonner le nombre de cp rank. = 3
    # Number of neighbors
    k <- 20 # Adjust this based on your requirement
    pca_transformed_data <- pca$x
    knn_result <- get.knn(pca_transformed_data, k)
    
    generate_avatar_weights <- function(knn_result, pca_transformed_data, k) {
      n <- nrow(pca_transformed_data)
      avatar_weights <- matrix(nrow = n, ncol = k)
      
      for (i in 1:n) {
        # Step 1: Inverse of Distances
        distances <- sqrt(rowSums((pca_transformed_data[knn_result$nn.index[i, ], ] - pca_transformed_data[i, ])^2))
        inverse_distances <- 1 / distances
        
        # Step 2: Random Weights
        
        random_weights <- rexp(k, rate = 1)
        
        # Step 3: Contribution Factors
        
        shuffled_indices <- sample(k)
        contribution_factors <- 1 / (2^shuffled_indices)
        
        # Step 4: Calculate Weights
        weights <- inverse_distances * random_weights * contribution_factors
        
        # Step 5: Normalize Weights
        normalized_weights <- weights / sum(weights)
        
        avatar_weights[i, ] <- normalized_weights
      }
      
      return(avatar_weights)
    }
    
    
    
    # Generate avatar weights
 
    avatar_weights <- generate_avatar_weights(knn_result, pca_transformed_data, k)
    
    # Assuming pca_result, avatar_weights, knn_result$nn.index, and pca_transformed_data are already defined
    
    # Function to generate avatars in PCA space based on weights
    generate_avatars_pca_space <- function(pca_transformed_data, knn_indices, weights) {
      n <- nrow(pca_transformed_data)
      avatars_pca <- matrix(nrow = n, ncol = ncol(pca_transformed_data))
      
      for (i in 1:n) {
        weighted_avatars <- pca_transformed_data[knn_indices[i, ], ] * weights[i, ]
        avatars_pca[i, ] <- colSums(weighted_avatars)
      }
      
      return(avatars_pca)
    }
    # Generate avatars in PCA space
    avatars_pca_space <- generate_avatars_pca_space(pca_transformed_data, knn_result$nn.index, avatar_weights)
    # Assuming 'aids_pca' is the PCA object and 'avatars_pca_space' contains the avatars in PCA space
    # Inverse PCA transformation
    inverse_pca <- function(pca_object, pca_data) {
      return(pca_data %*% t(pca_object$rotation) + matrix(pca_object$center, nrow = nrow(pca_data), ncol = ncol(pca_object$rotation), byrow = TRUE))
    }
    avatars_original_scale <- inverse_pca(pca, avatars_pca_space)
    
    # Assuming 'aids_data_normalized' contains the scaling attributes of the original data
    # Inverse normalization (if the original data was normalized)
    avatars_rescaled <- scale(avatars_original_scale, center = FALSE, scale = 1/attr(data_normalized, "scaled:scale"))
    avatars_rescaled <- sweep(avatars_rescaled, 2, attr(data_normalized, "scaled:center"), "+")
    
    avatars_tibble <- as_tibble(avatars_rescaled) %>% 
      mutate(haplotype = round(haplotype, digits=0),
             cyp3A5D = round(cyp3A5D, digits=0),
             sexe_r  = round(sexe_r , digits=0),
             sexe_d  = round(sexe_d , digits=0),
             rejet_aigu  = round(rejet_aigu , digits=0),
             event = round(event, digits=0)
             # CYP3A4_1B = round(CYP3A4_1B, digits=0),
             # MDR1_C1236T = round(MDR1_C1236T, digits=0),
             # MDR1_G2677T = round(MDR1_G2677T, digits=0),
             # MDR1_C3435T = round(MDR1_C3435T, digits=0)
      ) 
  }
  
  iteration <- c(1:4)
  set.seed(seed_value)
  augmented_data_x <- map_dfr(iteration, data_augment_avatar, .id = "iter_")
  
  augmented_data_x <- augmented_data_x %>% 
    select(-iter_)
  
  
  ###############
  # Finally, fit the Cox model
  fit <- coxph(Surv(delai_event, event) ~ haplotype + age_d + cyp3A5D + rejet_aigu, 
               data = augmented_data_x)
  coefs <- fit$coefficients
  hr <- exp(coefs)
  return(data.frame(variable = names(hr), hr = hr))
  # Calculate confidence intervals
#  ci <- confint(fit)
  
 # return(list(fit = fit, ci = ci))
}




# Generate a list of seed values
seed_value <- sample(x=100) # Modify this if you need different seed values

# Apply the algorithm with different seed values
model_results <- map(seed_value, run_model_with_seed)

# Extract HR and CI from model results
#extracted_results <- map(model_results, extract_hrs_and_cis)

# Combine results into a single data frame
combined_results <- bind_rows(model_results)

# Calculate median HR and CI for each variable
# aggregate_metrics <- combined_results %>%
#   group_by(variable) %>%
#   summarize(
#     median_hr = median(hr),
#     median_ci_lower = median(ci_lower),
#     median_ci_upper = median(ci_upper)
#   )
# 
# aggregate_metrics

# Calculate the specified percentiles for HRs for each variable
percentile_metrics <- combined_results %>%
  group_by(variable) %>%
  summarize(
    percentile_0 = quantile(hr, probs = 0),
    percentile_5 = quantile(hr, probs = 0.05),
    percentile_25 = quantile(hr, probs = 0.25),
    percentile_50 = quantile(hr, probs = 0.5),
    percentile_75 = quantile(hr, probs = 0.75),
    percentile_95 = quantile(hr, probs = 0.95),
    percentile_100 = quantile(hr, probs = 1)
  ) %>%
  pivot_longer(-variable, names_to = "Percentile_HR", values_to = "Value_HR") %>% 
  mutate(Value_HR = round(Value_HR, 2))
# percentile_metrics
# datatable(percentile_metrics)
knitr::kable(percentile_metrics %>%  mutate(Value_HR = round(Value_HR, 2)), "simple")
```


## augmented data: function to aggregating the HR and CI95 results of Cox models after changing 100 times the seed with different knn = 3, 5, 10, 15, 20, 50

Allow to define the variability range of HR for different Avatar generated with different seed and different values of knn

```{r bootstrap augmented data different knn different seed avatar}
# library(tidyverse)
# 
# # Assuming all your existing functions and necessary libraries are loaded
# run_for_k_values <- function(k) {
# run_model_with_seed <- function(seed_value) {
#   
#   
#   # augmentaiotn of data
#   
#   data_augment_avatar <- function(x) {
#     data_normalized <- scale(original1)
#     pca <- prcomp(data_normalized, scale. = FALSE)# pour selecitonner le nombre de cp rank. = 3
#     # Number of neighbors
#     #k <- 20 # Adjust this based on your requirement
#     pca_transformed_data <- pca$x
#     knn_result <- get.knn(pca_transformed_data, k)
#     
#     generate_avatar_weights <- function(knn_result, pca_transformed_data, k) {
#       n <- nrow(pca_transformed_data)
#       avatar_weights <- matrix(nrow = n, ncol = k)
#       
#       for (i in 1:n) {
#         # Step 1: Inverse of Distances
#         distances <- sqrt(rowSums((pca_transformed_data[knn_result$nn.index[i, ], ] - pca_transformed_data[i, ])^2))
#         inverse_distances <- 1 / distances
#         
#         # Step 2: Random Weights
#         
#         random_weights <- rexp(k, rate = 1)
#         
#         # Step 3: Contribution Factors
#         
#         shuffled_indices <- sample(k)
#         contribution_factors <- 1 / (2^shuffled_indices)
#         
#         # Step 4: Calculate Weights
#         weights <- inverse_distances * random_weights * contribution_factors
#         
#         # Step 5: Normalize Weights
#         normalized_weights <- weights / sum(weights)
#         
#         avatar_weights[i, ] <- normalized_weights
#       }
#       
#       return(avatar_weights)
#     }
#     
#     
#     
#     # Generate avatar weights
#    
#     avatar_weights <- generate_avatar_weights(knn_result, pca_transformed_data, k)
#     
#     # Assuming pca_result, avatar_weights, knn_result$nn.index, and pca_transformed_data are already defined
#     
#     # Function to generate avatars in PCA space based on weights
#     generate_avatars_pca_space <- function(pca_transformed_data, knn_indices, weights) {
#       n <- nrow(pca_transformed_data)
#       avatars_pca <- matrix(nrow = n, ncol = ncol(pca_transformed_data))
#       
#       for (i in 1:n) {
#         weighted_avatars <- pca_transformed_data[knn_indices[i, ], ] * weights[i, ]
#         avatars_pca[i, ] <- colSums(weighted_avatars)
#       }
#       
#       return(avatars_pca)
#     }
#     # Generate avatars in PCA space
#     avatars_pca_space <- generate_avatars_pca_space(pca_transformed_data, knn_result$nn.index, avatar_weights)
#     # Assuming 'aids_pca' is the PCA object and 'avatars_pca_space' contains the avatars in PCA space
#     # Inverse PCA transformation
#     inverse_pca <- function(pca_object, pca_data) {
#       return(pca_data %*% t(pca_object$rotation) + matrix(pca_object$center, nrow = nrow(pca_data), ncol = ncol(pca_object$rotation), byrow = TRUE))
#     }
#     avatars_original_scale <- inverse_pca(pca, avatars_pca_space)
#     
#     # Assuming 'aids_data_normalized' contains the scaling attributes of the original data
#     # Inverse normalization (if the original data was normalized)
#     avatars_rescaled <- scale(avatars_original_scale, center = FALSE, scale = 1/attr(data_normalized, "scaled:scale"))
#     avatars_rescaled <- sweep(avatars_rescaled, 2, attr(data_normalized, "scaled:center"), "+")
#     
#     avatars_tibble <- as_tibble(avatars_rescaled) %>% 
#       mutate(haplotype = round(haplotype, digits=0),
#              cyp3A5D = round(cyp3A5D, digits=0),
#              sexe_r  = round(sexe_r , digits=0),
#              sexe_d  = round(sexe_d , digits=0),
#              rejet_aigu  = round(rejet_aigu , digits=0),
#              event = round(event, digits=0)
#              # CYP3A4_1B = round(CYP3A4_1B, digits=0),
#              # MDR1_C1236T = round(MDR1_C1236T, digits=0),
#              # MDR1_G2677T = round(MDR1_G2677T, digits=0),
#              # MDR1_C3435T = round(MDR1_C3435T, digits=0)
#       ) 
#   }
#   
#   iteration <- c(1:4)
#   # set.seed(seed_value)
#    set.seed(seed_value)
#   augmented_data_x <- map_dfr(iteration, data_augment_avatar, .id = "iter_")
#   
#   augmented_data_x <- augmented_data_x %>% 
#     select(-iter_)
#   
#   
#   ###############
#   # Finally, fit the Cox model
#   fit <- coxph(Surv(delai_event, event) ~ haplotype , 
#                data = augmented_data_x)
#   coefs <- fit$coefficients
#   hr <- exp(coefs)
#   return(data.frame(variable = names(hr), hr = hr))
#   # Calculate confidence intervals
#   #  ci <- confint(fit)
#   
#   # return(list(fit = fit, ci = ci))
# }
# 
# 
# 
# 
# # Generate a list of seed values
# seed_value <- sample(x=100) # Modify this if you need different seed values
# 
# # Apply the algorithm with different seed values
# model_results <- map(seed_value, run_model_with_seed)
# 
# # Extract HR and CI from model results
# #extracted_results <- map(model_results, extract_hrs_and_cis)
# 
# # Combine results into a single data frame
# combined_results <- bind_rows(model_results)
# 
# # Calculate median HR and CI for each variable
# # aggregate_metrics <- combined_results %>%
# #   group_by(variable) %>%
# #   summarize(
# #     median_hr = median(hr),
# #     median_ci_lower = median(ci_lower),
# #     median_ci_upper = median(ci_upper)
# #   )
# # 
# # aggregate_metrics
# 
# # Calculate the specified percentiles for HRs for each variable
# percentile_metrics <- combined_results %>%
#   group_by(variable) %>%
#   summarize(
#     percentile_0 = quantile(hr, probs = 0),
#     percentile_5 = quantile(hr, probs = 0.05),
#     percentile_25 = quantile(hr, probs = 0.25),
#     percentile_50 = quantile(hr, probs = 0.5),
#     percentile_75 = quantile(hr, probs = 0.75),
#     percentile_95 = quantile(hr, probs = 0.95),
#     percentile_100 = quantile(hr, probs = 1)
#   ) %>%
#   pivot_longer(-variable, names_to = "Percentile_HR", values_to = "Value_HR") %>% 
#   mutate(Value_HR = round(Value_HR, 2))
# # percentile_metrics
# 
# return(percentile_metrics)
# }
# # Define different k values
# k_values <- c(3, 5, 10, 15, 20, 50)
# 
# results_list <- map(k_values, run_for_k_values)
# names(results_list) <- paste("K =", k_values)
# 
# 
# # datatable(percentile_metrics)
# knitr::kable(results_list, "simple")

```

# KNN=10

```{r}
# Number of neighbors
k <- 10  # Adjust this based on your requirement
```

algorithm

```{r avatar basal knn10}
pca_transformed_data <- pca$x
knn_result <- get.knn(pca_transformed_data, k)

generate_avatar_weights <- function(knn_result, pca_transformed_data, k) {
  n <- nrow(pca_transformed_data)
  avatar_weights <- matrix(nrow = n, ncol = k)
  
  for (i in 1:n) {
    # Step 1: Inverse of Distances
    distances <- sqrt(rowSums((pca_transformed_data[knn_result$nn.index[i, ], ] - pca_transformed_data[i, ])^2))
    inverse_distances <- 1 / distances
    
    # Step 2: Random Weights
   
    random_weights <- rexp(k, rate = 1)
    
    # Step 3: Contribution Factors
   
    shuffled_indices <- sample(k)
    contribution_factors <- 1 / (2^shuffled_indices)
    
    # Step 4: Calculate Weights
    weights <- inverse_distances * random_weights * contribution_factors
    
    # Step 5: Normalize Weights
    normalized_weights <- weights / sum(weights)
    
    avatar_weights[i, ] <- normalized_weights
  }
  
  return(avatar_weights)
}



# Generate avatar weights
 set.seed(12)
avatar_weights <- generate_avatar_weights(knn_result, pca_transformed_data, k)
```


Generation of avatar in the latent space

```{r}
# Assuming pca_result, avatar_weights, knn_result$nn.index, and pca_transformed_data are already defined

# Function to generate avatars in PCA space based on weights
generate_avatars_pca_space <- function(pca_transformed_data, knn_indices, weights) {
  n <- nrow(pca_transformed_data)
  avatars_pca <- matrix(nrow = n, ncol = ncol(pca_transformed_data))
  
  for (i in 1:n) {
    weighted_avatars <- pca_transformed_data[knn_indices[i, ], ] * weights[i, ]
    avatars_pca[i, ] <- colSums(weighted_avatars)
  }
  
  return(avatars_pca)
}
# Generate avatars in PCA space
avatars_pca_space <- generate_avatars_pca_space(pca_transformed_data, knn_result$nn.index, avatar_weights)

```

Return to the initial scale

```{r}
# Assuming 'aids_pca' is the PCA object and 'avatars_pca_space' contains the avatars in PCA space
# Inverse PCA transformation
inverse_pca <- function(pca_object, pca_data) {
  return(pca_data %*% t(pca_object$rotation) + matrix(pca_object$center, nrow = nrow(pca_data), ncol = ncol(pca_object$rotation), byrow = TRUE))
}
avatars_original_scale <- inverse_pca(pca, avatars_pca_space)

# Assuming 'aids_data_normalized' contains the scaling attributes of the original data
# Inverse normalization (if the original data was normalized)
avatars_rescaled <- scale(avatars_original_scale, center = FALSE, scale = 1/attr(data_normalized, "scaled:scale"))
avatars_rescaled <- sweep(avatars_rescaled, 2, attr(data_normalized, "scaled:center"), "+")


```

Transform into tibble

```{r}
avatars_tibble_knn10 <- as_tibble(avatars_rescaled) %>% 
  mutate(haplotype = round(haplotype, digits=0),
         cyp3A5D = round(cyp3A5D, digits=0),
         sexe_r  = round(sexe_r , digits=0),
         sexe_d  = round(sexe_d , digits=0),
         rejet_aigu  = round(rejet_aigu , digits=0),
         event = round(event, digits=0)
         # CYP3A4_1B = round(CYP3A4_1B, digits=0),
         # MDR1_C1236T = round(MDR1_C1236T, digits=0),
         # MDR1_G2677T = round(MDR1_G2677T, digits=0),
         # MDR1_C3435T = round(MDR1_C3435T, digits=0)
  ) 

avatars_tibble_factor_knn10 <- as_tibble(avatars_rescaled) %>% 
  mutate(haplotype = round(haplotype, digits=0),
         cyp3A5D = round(cyp3A5D, digits=0),
         sexe_r  = round(sexe_r , digits=0),
         sexe_d  = round(sexe_d , digits=0),
         rejet_aigu  = round(rejet_aigu , digits=0),
         event = round(event, digits=0)
         # CYP3A4_1B = round(CYP3A4_1B, digits=0),
         # MDR1_C1236T = round(MDR1_C1236T, digits=0),
         # MDR1_G2677T = round(MDR1_G2677T, digits=0),
         # MDR1_C3435T = round(MDR1_C3435T, digits=0)
  ) %>% 
  mutate(haplotype = as.factor(haplotype),
         cyp3A5D = as.factor(cyp3A5D),
         sexe_r = as.factor(sexe_r),
         sexe_d = as.factor(sexe_d),
         # CYP3A4_1B = as.factor(CYP3A4_1B),
         # MDR1_C1236T = as.factor(MDR1_C1236T),
         # MDR1_G2677T = as.factor(MDR1_G2677T),
         # MDR1_C3435T = as.factor(MDR1_C3435T),
         rejet_aigu = as.factor(rejet_aigu))
```

Plot of the synthetic and original in the latent space

```{r}
# Combine original and synthetic data for visualization
combined_data <- rbind(
  original1 %>% mutate(DataType = 'Original'),
  avatars_tibble_knn10 %>% mutate(DataType = 'Synthetic')
)

# Perform PCA on combined data
combined_data_normalized <- scale(combined_data[, -which(names(combined_data) %in% c("DataType", "id"))])
combined_pca <- prcomp(combined_data_normalized, scale. = FALSE)

# Extract the first two principal components
combined_pca_data <- data.frame(combined_pca$x[, 1:2])
combined_pca_data$DataType <- combined_data$DataType

# Plot PCA with color differentiation
ggplot(combined_pca_data, aes(x = PC1, y = PC2, color = DataType)) +
  geom_point(alpha = 0.8) +
  theme_minimal() +
  labs(title = "PCA Plot", x = "Principal Component 1", y = "Principal Component 2", color = "Data Type")

```

Export data knn = 10 

```{r}
write_csv(avatars_tibble_knn10, file = "avatar_sfpt_knn10.csv")
```


### Comparison of the datasets

#### Summary of the 2 datasets

```{r}
## Vector of categorical variables that need transformation
catVars <- c("haplotype", "cyp3A5D",  "sexe_r",  "sexe_d", 
"rejet_aigu", "event")
## Create a variable list.
vars <- c( "haplotype", "cyp3A5D", "age_r", "sexe_r", "age_d", "sexe_d", 
"rejet_aigu", "TIF", "event", "delai_event", "DataType")
tableOne <- CreateTableOne(vars = vars, strata = "DataType",factorVars = catVars, data = combined_data)
tableOne2<-print(tableOne, nonnormal = c( "age_r", "age_d", "TIF", "delai_event"), printToggle=F, minMax=T)
```

```{r , echo=F}
kableone(tableOne2)
```

```{r}
summary(original)
summary(avatars_tibble_knn10)
```

#### individual data explorer

```{r}

# boxplots
plot_boxplot(combined_data , by ="DataType") 

# histograms

# Function to create histogram for each continuous variable
plot_histograms <- function(data, var_name, group_var) {
  ggplot(data, aes(x = !!sym(var_name), fill = !!sym(group_var))) +
    geom_histogram(alpha = 0.5,show.legend = FALSE) +
    labs(x = var_name, y = "Count") +
    theme_minimal() +
    ggtitle(paste(var_name))
}

# Using select_if to identify continuous variables and map to apply the function
plots <- combined_data %>%
  select( -sexe_r,-sexe_d) %>% 
  select_if(is.numeric) %>%
  names() %>%
  map(~plot_histograms(combined_data, ., "DataType"))

# Optionally, print or arrange plots (e.g., using gridExtra or patchwork packages)

wrap_plots(plots)
```

#### plot correlation
```{r}

##Correlation Analysis
  cor_real <- cor(original1, use = "complete.obs")
  cor_synthetic <- cor(avatars_tibble_knn10, use = "complete.obs")
  
# plots
ggcorrplot(cor_real, hc.order = TRUE, type = "lower",
           lab = TRUE,  pch.cex = 5,
  tl.cex = 6, lab_size = 2)
# plots
ggcorrplot(cor_synthetic, hc.order = TRUE, type = "lower",
           lab = TRUE,  pch.cex = 5,
  tl.cex = 6, lab_size = 2)
```

#### Modele de Cox

```{r}
# original
fit_original <- coxph(Surv(delai_event, event) ~ haplotype + cyp3A5D +  age_r + sexe_r + age_d +  sexe_d + rejet_aigu + TIF , data = original1)
summary(fit_original)
ggforest(fit_original)

# synthetique
fit_synthetique <- coxph(Surv(delai_event, event) ~ haplotype + cyp3A5D +  age_r + sexe_r + age_d +  sexe_d + rejet_aigu + TIF, data = avatars_tibble_knn10)
summary(fit_synthetique)
ggforest(fit_synthetique)

```

#### BootstepAIC based on BIC

Allow to see which vairable would have been selected

Original

```{r}

boot.stepAIC(fit_original, original1, B = 100, k=log(nrow(original1)))

```

synhtetic knn10

```{r}

boot.stepAIC(fit_synthetique, avatars_tibble_knn10, B = 100, k=log(nrow(avatars_tibble_knn10)))

```


##### Final model original

```{r}
fit_original <- coxph(Surv(delai_event, event) ~ haplotype , data = original1)
summary(fit_original)
```

##### Final model synthetic

```{r}
fit_synthetique <- coxph(Surv(delai_event, event) ~ haplotype  , data = avatars_tibble_knn10)
summary(fit_synthetique)
```

### Bootstrap of the coefficient for haplotype

Allow to define the variability range of HR for a given dataset (intra dataset variability)

```{r bootstrap knn10} 
# Define the Cox model
cox_model <- function(data, indices) {
  d <- data[indices,] # allows bootstrapping to sample the data
  fit <- coxph(Surv(delai_event, event) ~ haplotype , data=d)
  return(fit$coefficients)
}

# Set the seed for reproducibility
set.seed(12)

# Bootstrap the Cox model
boot_results <- boot(data=avatars_tibble_knn10, statistic=cox_model, R=100)

# Convert bootstrap results to a data frame for ggplot2
boot_hrs <- exp(boot_results$t) # Convert log(HR) to HR
hr_data <- data.frame(HR=boot_hrs[,1])

# Calculate summary statistics
summary_stats <- quantile(hr_data$HR, probs = c(0, 0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975, 1))
names(summary_stats) <- c("Min","2.5th", "5th", "25th", "Median", "75th", "95th","97.5th","Max")

# Create the histogram
ggplot(hr_data, aes(x=HR)) +
  geom_histogram(bins=30, fill="#007a86", color="black") +
  # geom_vline(aes(xintercept=summary_stats["Min"]), color="red", linetype="dashed") +
  geom_vline(aes(xintercept=summary_stats["25th"]), color="gray", linetype="dashed", linewidth=2) +
  geom_vline(aes(xintercept=summary_stats["Median"]), color="blue", linetype="dashed", linewidth=2) +
  geom_vline(aes(xintercept=summary_stats["75th"]), color="gray", linetype="dashed", linewidth=2) +
  # geom_vline(aes(xintercept=summary_stats["Max"]), color="purple", linetype="dashed") +
  labs(title="Bootstrap Distribution of Hazard Ratios", x="Hazard Ratio (HR)", y="Frequency") +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5))

# Print summary statistics
print(summary_stats)


```


##### Modele final & KM

```{r KM original knn10}
km_original <- survfit(Surv(delai_event, event) ~ haplotype, data = original)
km_original_plot <- ggsurvplot(
  km_original,
  data = original,
  size = 1,                 # change line size
  conf.int = TRUE,          # Add confidence interval
  pval = TRUE,              # Add p-value
  risk.table = TRUE,        # Add risk table
  risk.table.col = "strata",# Risk table color by groups
  risk.table.height = 0.25, # Useful to change when you have multiple groups
  ggtheme = theme_bw()      # Change ggplot2 theme
)
km_original_plot
```

```{r KM avatar KNN10}
km_synthetique <- survfit(Surv(delai_event, event) ~ haplotype, data = avatars_tibble_knn10)
km_synthetique_avatar_10 <- ggsurvplot(
  km_synthetique,
  data = avatars_tibble_knn10,
  size = 1,                 # change line size
  conf.int = TRUE,          # Add confidence interval
  pval = TRUE,              # Add p-value
  risk.table = TRUE,        # Add risk table
  risk.table.col = "strata",# Risk table color by groups
  risk.table.height = 0.25, # Useful to change when you have multiple groups
  ggtheme = theme_bw()      # Change ggplot2 theme
)
km_synthetique_avatar_10
```


Plots original & synthetic combined

```{r}
## combine data
combined_df <- rbind(original %>% mutate(group = "original"), avatars_tibble_factor_knn10 %>% mutate(group = "synthetic")) %>% mutate(combined_haplotype = str_c(haplotype,"_", group ))

## fit the model
km_combined <- survfit(Surv(delai_event, event) ~ combined_haplotype, data = combined_df)

# plot
ggsurvplot(fit = km_combined, 
           data = combined_df,
           
  size = 1,                 # change line size
  conf.int = FALSE,          # Add confidence interval
  pval = TRUE,              # Add p-value
  risk.table = TRUE,        # Add risk table
  risk.table.col = "strata",# Risk table color by groups
  risk.table.height = 0.35, # Useful to change when you have multiple groups
  ggtheme = theme_bw()      # Change ggplot2 theme
)

```

#### Graphical exploraiotn of distribution

```{r}
library(GGally)

pm_knn10 <- combined_df %>% select(haplotype:delai_event, group) %>% ggpairs(
  ggplot2::aes(colour = group,alpha = 0.5),
  upper = list(continuous = wrap("cor", size = 1.5)),
  lower=list(combo=wrap("facethist", binwidth=0.5))) + 
  theme(strip.text.x = element_text(size = 5),
           strip.text.y = element_text(size = 5),axis.text = element_text(size = 5))
pm_knn10
# ggsave("comparaison_distribution_knn10.pdf")

```

### function to aggregating the HR and CI95 results of Cox models after changing 100 times the seed with  knn = 10
Allow to define the variability range of HR for different Avatar generated with different seed but the same knn (inter dataset variability)

```{r change seed knn10}

# Assuming all your existing functions and necessary libraries are loaded

run_model_with_seed <- function(seed_value) {
 
  
  # Number of neighbors
  k <- 10  # Adjust this based on your requirement
  
  pca_transformed_data <- pca$x
  knn_result <- get.knn(pca_transformed_data, k)
  
  generate_avatar_weights <- function(knn_result, pca_transformed_data, k) {
    n <- nrow(pca_transformed_data)
    avatar_weights <- matrix(nrow = n, ncol = k)
    
    for (i in 1:n) {
      # Step 1: Inverse of Distances
      distances <- sqrt(rowSums((pca_transformed_data[knn_result$nn.index[i, ], ] - pca_transformed_data[i, ])^2))
      inverse_distances <- 1 / distances
      
      # Step 2: Random Weights

      random_weights <- rexp(k, rate = 1)
      
      # Step 3: Contribution Factors
  
      shuffled_indices <- sample(k)
      contribution_factors <- 1 / (2^shuffled_indices)
      
      # Step 4: Calculate Weights
      weights <- inverse_distances * random_weights * contribution_factors
      
      # Step 5: Normalize Weights
      normalized_weights <- weights / sum(weights)
      
      avatar_weights[i, ] <- normalized_weights
    }
    
    return(avatar_weights)
  }
  
  
  
  # Generate avatar weights
   set.seed(seed_value)
  avatar_weights <- generate_avatar_weights(knn_result, pca_transformed_data, k)
  
  
  # Function to generate avatars in PCA space based on weights
  generate_avatars_pca_space <- function(pca_transformed_data, knn_indices, weights) {
    n <- nrow(pca_transformed_data)
    avatars_pca <- matrix(nrow = n, ncol = ncol(pca_transformed_data))
    
    for (i in 1:n) {
      weighted_avatars <- pca_transformed_data[knn_indices[i, ], ] * weights[i, ]
      avatars_pca[i, ] <- colSums(weighted_avatars)
    }
    
    return(avatars_pca)
  }
  # Generate avatars in PCA space
  avatars_pca_space <- generate_avatars_pca_space(pca_transformed_data, knn_result$nn.index, avatar_weights)
  

  # Inverse PCA transformation
  inverse_pca <- function(pca_object, pca_data) {
    return(pca_data %*% t(pca_object$rotation) + matrix(pca_object$center, nrow = nrow(pca_data), ncol = ncol(pca_object$rotation), byrow = TRUE))
  }
  avatars_original_scale <- inverse_pca(pca, avatars_pca_space)
  
 
  # Inverse normalization (if the original data was normalized)
  avatars_rescaled <- scale(avatars_original_scale, center = FALSE, scale = 1/attr(data_normalized, "scaled:scale"))
  avatars_rescaled <- sweep(avatars_rescaled, 2, attr(data_normalized, "scaled:center"), "+")
  
avatars_tibble_knn10   <- as_tibble(avatars_rescaled) %>% 
    mutate(haplotype = round(haplotype, digits=0),
           cyp3A5D = round(cyp3A5D, digits=0),
           sexe_r  = round(sexe_r , digits=0),
           sexe_d  = round(sexe_d , digits=0),
           rejet_aigu  = round(rejet_aigu , digits=0),
           event = round(event, digits=0)
           # CYP3A4_1B = round(CYP3A4_1B, digits=0),
           # MDR1_C1236T = round(MDR1_C1236T, digits=0),
           # MDR1_G2677T = round(MDR1_G2677T, digits=0),
           # MDR1_C3435T = round(MDR1_C3435T, digits=0)
    ) 
  
avatars_tibble_factor_knn10   <- as_tibble(avatars_rescaled) %>% 
    mutate(haplotype = round(haplotype, digits=0),
           cyp3A5D = round(cyp3A5D, digits=0),
           sexe_r  = round(sexe_r , digits=0),
           sexe_d  = round(sexe_d , digits=0),
           rejet_aigu  = round(rejet_aigu , digits=0),
           event = round(event, digits=0)
           # CYP3A4_1B = round(CYP3A4_1B, digits=0),
           # MDR1_C1236T = round(MDR1_C1236T, digits=0),
           # MDR1_G2677T = round(MDR1_G2677T, digits=0),
           # MDR1_C3435T = round(MDR1_C3435T, digits=0)
    ) %>% 
    mutate(haplotype = as.factor(haplotype),
           cyp3A5D = as.factor(cyp3A5D),
           sexe_r = as.factor(sexe_r),
           sexe_d = as.factor(sexe_d),
           # CYP3A4_1B = as.factor(CYP3A4_1B),
           # MDR1_C1236T = as.factor(MDR1_C1236T),
           # MDR1_G2677T = as.factor(MDR1_G2677T),
           # MDR1_C3435T = as.factor(MDR1_C3435T),
           rejet_aigu = as.factor(rejet_aigu))

  
  # Finally, fit the Cox model
  fit <- coxph(Surv(delai_event, event) ~ haplotype , 
               data = avatars_tibble_knn10)

  # Calculate confidence intervals
  ci <- confint(fit)
  
  return(list(fit = fit, ci = ci))
}


extract_hrs_and_cis <- function(model_output) {
  coefs <- model_output$fit$coefficients
  ci <- model_output$ci

  hr <- exp(coefs)
  ci_lower <- exp(ci[,"2.5 %"])
  ci_upper <- exp(ci[,"97.5 %"])

  return(data.frame(variable = names(hr), hr = hr, ci_lower = ci_lower, ci_upper = ci_upper))
}

# Generate a list of seed values
seed_values <- sample(x=100) # Modify this if you need different seed values

# Apply the algorithm with different seed values
model_results <- map(seed_values, run_model_with_seed)

# Extract HR and CI from model results
extracted_results <- map(model_results, extract_hrs_and_cis)

# Combine results into a single data frame
combined_results <- bind_rows(extracted_results)

# Calculate median HR and CI for each variable
# aggregate_metrics <- combined_results %>%
#   group_by(variable) %>%
#   summarize(
#     median_hr = median(hr),
#     median_ci_lower = median(ci_lower),
#     median_ci_upper = median(ci_upper)
#   )
# 
# aggregate_metrics

# Calculate the specified percentiles for HRs for each variable
percentile_metrics <- combined_results %>%
  group_by(variable) %>%
  summarize(
    percentile_0 = quantile(hr, probs = 0),
    percentile_5 = quantile(hr, probs = 0.05),
    percentile_25 = quantile(hr, probs = 0.25),
    percentile_50 = quantile(hr, probs = 0.5),
    percentile_75 = quantile(hr, probs = 0.75),
    percentile_95 = quantile(hr, probs = 0.95),
    percentile_100 = quantile(hr, probs = 1)
  ) %>%
  pivot_longer(-variable, names_to = "Percentile_HR", values_to = "Value_HR") %>% 
  mutate(Value_HR = round(Value_HR, 2))
# percentile_metrics
# datatable(percentile_metrics)
knitr::kable(percentile_metrics %>%  mutate(Value_HR = round(Value_HR, 2)), "simple")

```


# data augmentation avatar

## With 10 knn

We investigate the effect of data augmentaiotn with a defined seed and knn=10

```{r data augment knn10}
data_augment_avatar <- function(x) {
data_normalized <- scale(original1)
pca <- prcomp(data_normalized, scale. = FALSE)# pour selecitonner le nombre de cp rank. = 3
# Number of neighbors
k <- 10  # Adjust this based on your requirement
pca_transformed_data <- pca$x
knn_result <- get.knn(pca_transformed_data, k)

generate_avatar_weights <- function(knn_result, pca_transformed_data, k) {
  n <- nrow(pca_transformed_data)
  avatar_weights <- matrix(nrow = n, ncol = k)
  
  for (i in 1:n) {
    # Step 1: Inverse of Distances
    distances <- sqrt(rowSums((pca_transformed_data[knn_result$nn.index[i, ], ] - pca_transformed_data[i, ])^2))
    inverse_distances <- 1 / distances
    
    # Step 2: Random Weights

    random_weights <- rexp(k, rate = 1)
    
    # Step 3: Contribution Factors
 
    shuffled_indices <- sample(k)
    contribution_factors <- 1 / (2^shuffled_indices)
    
    # Step 4: Calculate Weights
    weights <- inverse_distances * random_weights * contribution_factors
    
    # Step 5: Normalize Weights
    normalized_weights <- weights / sum(weights)
    
    avatar_weights[i, ] <- normalized_weights
  }
  
  return(avatar_weights)
}



# Generate avatar weights
   set.seed( str_c(1,x))
avatar_weights <- generate_avatar_weights(knn_result, pca_transformed_data, k)

# Assuming pca_result, avatar_weights, knn_result$nn.index, and pca_transformed_data are already defined

# Function to generate avatars in PCA space based on weights
generate_avatars_pca_space <- function(pca_transformed_data, knn_indices, weights) {
  n <- nrow(pca_transformed_data)
  avatars_pca <- matrix(nrow = n, ncol = ncol(pca_transformed_data))
  
  for (i in 1:n) {
    weighted_avatars <- pca_transformed_data[knn_indices[i, ], ] * weights[i, ]
    avatars_pca[i, ] <- colSums(weighted_avatars)
  }
  
  return(avatars_pca)
}
# Generate avatars in PCA space
avatars_pca_space <- generate_avatars_pca_space(pca_transformed_data, knn_result$nn.index, avatar_weights)
# Assuming 'aids_pca' is the PCA object and 'avatars_pca_space' contains the avatars in PCA space
# Inverse PCA transformation
inverse_pca <- function(pca_object, pca_data) {
  return(pca_data %*% t(pca_object$rotation) + matrix(pca_object$center, nrow = nrow(pca_data), ncol = ncol(pca_object$rotation), byrow = TRUE))
}
avatars_original_scale <- inverse_pca(pca, avatars_pca_space)

# Assuming 'aids_data_normalized' contains the scaling attributes of the original data
# Inverse normalization (if the original data was normalized)
avatars_rescaled <- scale(avatars_original_scale, center = FALSE, scale = 1/attr(data_normalized, "scaled:scale"))
avatars_rescaled <- sweep(avatars_rescaled, 2, attr(data_normalized, "scaled:center"), "+")

avatars_tibble <- as_tibble(avatars_rescaled) %>% 
  mutate(haplotype = round(haplotype, digits=0),
         cyp3A5D = round(cyp3A5D, digits=0),
         sexe_r  = round(sexe_r , digits=0),
         sexe_d  = round(sexe_d , digits=0),
         rejet_aigu  = round(rejet_aigu , digits=0),
         event = round(event, digits=0)
         # CYP3A4_1B = round(CYP3A4_1B, digits=0),
         # MDR1_C1236T = round(MDR1_C1236T, digits=0),
         # MDR1_G2677T = round(MDR1_G2677T, digits=0),
         # MDR1_C3435T = round(MDR1_C3435T, digits=0)
  ) 
}

iteration <- c(1:4)

augmented_data_10 <- map_dfr(iteration, data_augment_avatar, .id = "iter_")

augmented_data_10 <- augmented_data_10 %>% 
  select(-iter_)

augmented_data_10_factor_knn10 <- augmented_data_10 %>% 
 mutate(haplotype = as.factor(haplotype),
         cyp3A5D = as.factor(cyp3A5D),
         sexe_r = as.factor(sexe_r),
         sexe_d = as.factor(sexe_d),
         # CYP3A4_1B = as.factor(CYP3A4_1B),
         # MDR1_C1236T = as.factor(MDR1_C1236T),
         # MDR1_G2677T = as.factor(MDR1_G2677T),
         # MDR1_C3435T = as.factor(MDR1_C3435T),
         rejet_aigu = as.factor(rejet_aigu))

```

Plot of the synthetic and original in the latent space

```{r}
# Combine original and synthetic data for visualization
combined_data <- rbind(
  original1 %>% mutate(DataType = 'Original'),
  augmented_data_10 %>% mutate(DataType = 'Synthetic')
)

# Perform PCA on combined data
combined_data_normalized <- scale(combined_data[, -which(names(combined_data) %in% c("DataType", "id"))])
combined_pca <- prcomp(combined_data_normalized, scale. = FALSE)

# Extract the first two principal components
combined_pca_data <- data.frame(combined_pca$x[, 1:2])
combined_pca_data$DataType <- combined_data$DataType

# Plot PCA with color differentiation
ggplot(combined_pca_data, aes(x = PC1, y = PC2, color = DataType)) +
  geom_point(alpha = 0.5) +
  theme_minimal() +
  labs(title = "PCA Plot", x = "Principal Component 1", y = "Principal Component 2", color = "Data Type")

```

Export data augmented knn = 10 

```{r}
write_csv(augmented_data_10, file = "avatar_sfpt_knn10_data_augmented.csv")
```

# Comparison of the datasets augmented and original


## Summary of the 2 datasets
```{r}
## Vector of categorical variables that need transformation
catVars <- c("haplotype", "cyp3A5D",  "sexe_r",  "sexe_d", 
"rejet_aigu", "event")
## Create a variable list.
vars <- c( "haplotype", "cyp3A5D", "age_r", "sexe_r", "age_d", "sexe_d", 
"rejet_aigu", "TIF", "event", "delai_event", "DataType")
tableOne <- CreateTableOne(vars = vars, strata = "DataType",factorVars = catVars, data = combined_data)
tableOne2<-print(tableOne, nonnormal = c( "age_r", "age_d", "TIF", "delai_event"), printToggle=F, minMax=T)
```

```{r , echo=F}
kableone(tableOne2)
```

```{r}
summary(original)
summary(augmented_data_10)
```

### individual data explorer

```{r}

# boxplots
plot_boxplot(combined_data , by ="DataType") 

# histograms

# Function to create histogram for each continuous variable
plot_histograms <- function(data, var_name, group_var) {
  ggplot(data, aes(x = !!sym(var_name), fill = !!sym(group_var))) +
    geom_histogram(alpha = 0.5,show.legend = FALSE) +
    labs(x = var_name, y = "Count") +
    theme_minimal() +
    ggtitle(paste(var_name))
}

# Using select_if to identify continuous variables and map to apply the function
plots <- combined_data %>%
  select( -sexe_r,-sexe_d) %>% 
  select_if(is.numeric) %>%
  names() %>%
  map(~plot_histograms(combined_data, ., "DataType"))

# Optionally, print or arrange plots (e.g., using gridExtra or patchwork packages)

wrap_plots(plots)
```

### plot correlation
```{r}

##Correlation Analysis
  cor_real <- cor(original1, use = "complete.obs")
  cor_synthetic <- cor(augmented_data_10, use = "complete.obs")
  
# plots
ggcorrplot(cor_real, hc.order = TRUE, type = "lower",
           lab = TRUE,  pch.cex = 5,
  tl.cex = 6, lab_size = 2)
# plots
ggcorrplot(cor_synthetic, hc.order = TRUE, type = "lower",
           lab = TRUE,  pch.cex = 5,
  tl.cex = 6, lab_size = 2)
```

## Modele de Cox

```{r}
# original
fit_original <- coxph(Surv(delai_event, event) ~ haplotype + cyp3A5D +  age_r + sexe_r + age_d +  sexe_d + rejet_aigu + TIF , data = original1)
summary(fit_original)
ggforest(fit_original)

# synthetique
fit_synthetique <- coxph(Surv(delai_event, event) ~ haplotype + cyp3A5D +  age_r + sexe_r + age_d +  sexe_d + rejet_aigu + TIF , data = augmented_data_10)
summary(fit_synthetique)
ggforest(fit_synthetique)
```

BootstepAIC augmented synthetic knn10

```{r}

boot.stepAIC(fit_synthetique, augmented_data_10, B = 100, k=log(nrow(augmented_data_10)))

```


Final model original

```{r}
fit_original <- coxph(Surv(delai_event, event) ~ haplotype + rejet_aigu , data = original1)
summary(fit_original)
```

Final model synthetic

```{r}
fit_synthetique <- coxph(Surv(delai_event, event) ~ haplotype + cyp3A5D + age_d + rejet_aigu, data = augmented_data_10)
summary(fit_synthetique)
```

# Modele final & KM

```{r}
km_original <- survfit(Surv(delai_event, event) ~ haplotype, data = original)
ggsurvplot(
  km_original,
  data = original,
  size = 1,                 # change line size
  conf.int = TRUE,          # Add confidence interval
  pval = TRUE,              # Add p-value
  risk.table = TRUE,        # Add risk table
  risk.table.col = "strata",# Risk table color by groups
  risk.table.height = 0.25, # Useful to change when you have multiple groups
  ggtheme = theme_bw()      # Change ggplot2 theme
)
```

```{r KM augmented avatar knn10}
km_synthetique <- survfit(Surv(delai_event, event) ~ haplotype, data = augmented_data_10_factor_knn10)
km_synthetique_avatar_10_augmented <- ggsurvplot(
  km_synthetique,
  data = augmented_data_10_factor_knn10,
  size = 1,                 # change line size
  conf.int = TRUE,          # Add confidence interval
  pval = TRUE,              # Add p-value
  risk.table = TRUE,        # Add risk table
  risk.table.col = "strata",# Risk table color by groups
  risk.table.height = 0.25, # Useful to change when you have multiple groups
  ggtheme = theme_bw()      # Change ggplot2 theme
)
km_synthetique_avatar_10_augmented
```


Plots original & synthetic combined

```{r}
## combine data
combined_df <- rbind(original %>% mutate(group = "original"), augmented_data_10_factor_knn10 %>% mutate(group = "synthetic")) %>% mutate(combined_haplotype = str_c(haplotype,"_", group ))

## fit the model
km_combined <- survfit(Surv(delai_event, event) ~ combined_haplotype, data = combined_df)

# plot
ggsurvplot(fit = km_combined, 
           data = combined_df,
           
  size = 1,                 # change line size
  conf.int = FALSE,          # Add confidence interval
  pval = TRUE,              # Add p-value
  risk.table = TRUE,        # Add risk table
  risk.table.col = "strata",# Risk table color by groups
  risk.table.height = 0.35, # Useful to change when you have multiple groups
  ggtheme = theme_bw()      # Change ggplot2 theme
)

```

Graphical exploraiotn of distribution

```{r}
library(GGally)

pm_knn10_augmented <- combined_df %>% select(haplotype:delai_event, group) %>% ggpairs(
  ggplot2::aes(colour = group,alpha = 0.5),
  upper = list(continuous = wrap("cor", size = 1.5)),
  lower=list(combo=wrap("facethist", binwidth=0.5))) + 
  theme(strip.text.x = element_text(size = 5),
           strip.text.y = element_text(size = 5),axis.text = element_text(size = 5))
pm_knn10_augmented
# ggsave("comparaison_distribution_augmented_knn10.pdf")

```

### Bootstrap of the coefficient for haplotype

Allow to define the variability range of HR for a given dataset (intra dataset variability)

```{r bootstrap augmented data knn10}
# Define the Cox model
cox_model <- function(data, indices) {
  d <- data[indices,] # allows bootstrapping to sample the data
  fit <- coxph(Surv(delai_event, event) ~ haplotype + age_d + sexe_d + rejet_aigu , data=d)
  return(fit$coefficients)
}

# Set the seed for reproducibility
set.seed(12)

# Bootstrap the Cox model
boot_results <- boot(data=augmented_data_10, statistic=cox_model, R=100)

# Convert bootstrap results to a data frame for ggplot2
boot_hrs <- exp(boot_results$t) # Convert log(HR) to HR
hr_data_haplo <- data.frame(HR=boot_hrs[,1])
hr_data_age_d <- data.frame(HR=boot_hrs[,2])
hr_data_cyp3A5D <- data.frame(HR=boot_hrs[,3])
hr_data_rejet_aigu <- data.frame(HR=boot_hrs[,4])
# Calculate summary statistics
summary_stats <- quantile(hr_data_haplo$HR, probs = c(0, 0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975, 1)) %>%  
  bind_rows(quantile(hr_data_age_d$HR, probs = c(0, 0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975, 1)) ) %>% 
  bind_rows(quantile(hr_data_cyp3A5D$HR, probs = c(0, 0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975, 1)) ) %>% 
  bind_rows(quantile(hr_data_rejet_aigu$HR, probs = c(0, 0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975, 1)) )  
names(summary_stats) <- c("Min","2.5th", "5th", "25th", "Median", "75th", "95th","97.5th","Max")

# Create the histogram
ggplot(hr_data_haplo, aes(x=HR)) +
  geom_histogram(bins=30, fill="#007a86", color="black") +
  # # geom_vline(aes(xintercept=summary_stats["Min"]), color="red", linetype="dashed") +
  # geom_vline(aes(xintercept=summary_stats["25th"][[1]][[1]]), color="gray", linetype="dashed", linewidth=2) +
  # geom_vline(aes(xintercept=summary_stats["Median"][[1]][[1]]), color="blue", linetype="dashed", linewidth=2) +
  # geom_vline(aes(xintercept=summary_stats["75th"])[[1]][[1]], color="gray", linetype="dashed", linewidth=2) +
  # geom_vline(aes(xintercept=summary_stats["Max"]), color="purple", linetype="dashed") +
  labs(title="Bootstrap Distribution of Hazard Ratios", x="Hazard Ratio (HR)", y="Frequency") +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5))

# Print summary statistics
knitr::kable(summary_stats, "simple")

```

## inter seed vairbaility augmented knn10
Allow to define the inter variability range of HR for augmented knn=10 (inter dataset variability) by using 100 bootraps

```{r intervariability 100 dataset augmented knn10}
# Assuming all your existing functions and necessary libraries are loaded

run_model_with_seed <- function(seed_value) {
  
  
  # augmentaiotn of data
  
  data_augment_avatar <- function(x) {
    data_normalized <- scale(original1)
    pca <- prcomp(data_normalized, scale. = FALSE)# pour selecitonner le nombre de cp rank. = 3
    # Number of neighbors
    k <- 10 # Adjust this based on your requirement
    pca_transformed_data <- pca$x
    knn_result <- get.knn(pca_transformed_data, k)
    
    generate_avatar_weights <- function(knn_result, pca_transformed_data, k) {
      n <- nrow(pca_transformed_data)
      avatar_weights <- matrix(nrow = n, ncol = k)
      
      for (i in 1:n) {
        # Step 1: Inverse of Distances
        distances <- sqrt(rowSums((pca_transformed_data[knn_result$nn.index[i, ], ] - pca_transformed_data[i, ])^2))
        inverse_distances <- 1 / distances
        
        # Step 2: Random Weights
        
        random_weights <- rexp(k, rate = 1)
        
        # Step 3: Contribution Factors
        
        shuffled_indices <- sample(k)
        contribution_factors <- 1 / (2^shuffled_indices)
        
        # Step 4: Calculate Weights
        weights <- inverse_distances * random_weights * contribution_factors
        
        # Step 5: Normalize Weights
        normalized_weights <- weights / sum(weights)
        
        avatar_weights[i, ] <- normalized_weights
      }
      
      return(avatar_weights)
    }
    
    
    
    # Generate avatar weights
 
    avatar_weights <- generate_avatar_weights(knn_result, pca_transformed_data, k)
    
    # Assuming pca_result, avatar_weights, knn_result$nn.index, and pca_transformed_data are already defined
    
    # Function to generate avatars in PCA space based on weights
    generate_avatars_pca_space <- function(pca_transformed_data, knn_indices, weights) {
      n <- nrow(pca_transformed_data)
      avatars_pca <- matrix(nrow = n, ncol = ncol(pca_transformed_data))
      
      for (i in 1:n) {
        weighted_avatars <- pca_transformed_data[knn_indices[i, ], ] * weights[i, ]
        avatars_pca[i, ] <- colSums(weighted_avatars)
      }
      
      return(avatars_pca)
    }
    # Generate avatars in PCA space
    avatars_pca_space <- generate_avatars_pca_space(pca_transformed_data, knn_result$nn.index, avatar_weights)
    # Assuming 'aids_pca' is the PCA object and 'avatars_pca_space' contains the avatars in PCA space
    # Inverse PCA transformation
    inverse_pca <- function(pca_object, pca_data) {
      return(pca_data %*% t(pca_object$rotation) + matrix(pca_object$center, nrow = nrow(pca_data), ncol = ncol(pca_object$rotation), byrow = TRUE))
    }
    avatars_original_scale <- inverse_pca(pca, avatars_pca_space)
    
    # Assuming 'aids_data_normalized' contains the scaling attributes of the original data
    # Inverse normalization (if the original data was normalized)
    avatars_rescaled <- scale(avatars_original_scale, center = FALSE, scale = 1/attr(data_normalized, "scaled:scale"))
    avatars_rescaled <- sweep(avatars_rescaled, 2, attr(data_normalized, "scaled:center"), "+")
    
    avatars_tibble <- as_tibble(avatars_rescaled) %>% 
      mutate(haplotype = round(haplotype, digits=0),
             cyp3A5D = round(cyp3A5D, digits=0),
             sexe_r  = round(sexe_r , digits=0),
             sexe_d  = round(sexe_d , digits=0),
             rejet_aigu  = round(rejet_aigu , digits=0),
             event = round(event, digits=0)
             # CYP3A4_1B = round(CYP3A4_1B, digits=0),
             # MDR1_C1236T = round(MDR1_C1236T, digits=0),
             # MDR1_G2677T = round(MDR1_G2677T, digits=0),
             # MDR1_C3435T = round(MDR1_C3435T, digits=0)
      ) 
  }
  
  iteration <- c(1:4)
  set.seed(seed_value)
  augmented_data_x <- map_dfr(iteration, data_augment_avatar, .id = "iter_")
  
  augmented_data_x <- augmented_data_x %>% 
    select(-iter_)
  
  
  ###############
  # Finally, fit the Cox model
  fit <- coxph(Surv(delai_event, event) ~ haplotype + age_d + sexe_d + rejet_aigu, 
               data = augmented_data_x)
  coefs <- fit$coefficients
  hr <- exp(coefs)
  return(data.frame(variable = names(hr), hr = hr))
  # Calculate confidence intervals
#  ci <- confint(fit)
  
 # return(list(fit = fit, ci = ci))
}




# Generate a list of seed values
seed_value <- sample(x=100) # Modify this if you need different seed values

# Apply the algorithm with different seed values
model_results <- map(seed_value, run_model_with_seed)

# Extract HR and CI from model results
#extracted_results <- map(model_results, extract_hrs_and_cis)

# Combine results into a single data frame
combined_results <- bind_rows(model_results)

# Calculate median HR and CI for each variable
# aggregate_metrics <- combined_results %>%
#   group_by(variable) %>%
#   summarize(
#     median_hr = median(hr),
#     median_ci_lower = median(ci_lower),
#     median_ci_upper = median(ci_upper)
#   )
# 
# aggregate_metrics

# Calculate the specified percentiles for HRs for each variable
percentile_metrics <- combined_results %>%
  group_by(variable) %>%
  summarize(
    percentile_0 = quantile(hr, probs = 0),
    percentile_5 = quantile(hr, probs = 0.05),
    percentile_25 = quantile(hr, probs = 0.25),
    percentile_50 = quantile(hr, probs = 0.5),
    percentile_75 = quantile(hr, probs = 0.75),
    percentile_95 = quantile(hr, probs = 0.95),
    percentile_100 = quantile(hr, probs = 1)
  ) %>%
  pivot_longer(-variable, names_to = "Percentile_HR", values_to = "Value_HR") %>% 
  mutate(Value_HR = round(Value_HR, 2))
# percentile_metrics
# datatable(percentile_metrics)
knitr::kable(percentile_metrics %>%  mutate(Value_HR = round(Value_HR, 2)), "simple")
```



# Survival Variational Auto-Encoder (surVAE)

These data for ct-GAN and TVAE have been generated by Clement Benoist using the Synthcity python libnrary from the Van der Schaar lab using the following code

```{python, eval=FALSE}


from synthcity.plugins.core.dataloader import SurvivalAnalysisDataLoader
from synthcity.plugins import Plugins
import pandas as pd


donnees=pd.read_csv("../../Donnees/original_sfpt.csv")
n_generate=donnees.shape[0]

data = SurvivalAnalysisDataLoader(
  donnees,
  target_column="event",
  time_to_event_column="delai_event",
  )

# ct gan and augmented *4
syn_model = Plugins().get("survival_ctgan")
syn_model.fit(data)
syn_data=syn_model.generate(count=n_generate*4)
syn_data.data.to_csv(path_or_buf="sfpt24_survctgan_data_large.dat",index=False)


syn_model = Plugins().get("survival_ctgan")
syn_model.fit(data)
syn_data=syn_model.generate(count=n_generate)
syn_data.data.to_csv(path_or_buf="sfpt24_survctgan_data.dat",index=False)

# tvae and augmented *4
syn_model2 = Plugins().get("survae")
syn_model2.fit(data)
syn_data2=syn_model2.generate(count=n_generate)
syn_data2.data.to_csv(path_or_buf="sfpt24_survae_data.dat",index=False)


syn_model2 = Plugins().get("survae")
syn_model2.fit(data)
syn_data2=syn_model2.generate(count=n_generate*4)
syn_data2.data.to_csv(path_or_buf="sfpt24_survae_data_large.dat",index=False)


```




## Non augmented data

### load the data

```{r}
survae <- read_csv("sfpt24_survae_data_v240111.dat") %>% 
   select(haplotype:delai_event)
survae_factor <- survae %>% 
  mutate(haplotype = as.factor(haplotype),
         cyp3A5D = as.factor(cyp3A5D),
         sexe_r = as.factor(sexe_r),
         sexe_d = as.factor(sexe_d),
         # CYP3A4_1B = as.factor(CYP3A4_1B),
         # MDR1_C1236T = as.factor(MDR1_C1236T),
         # MDR1_G2677T = as.factor(MDR1_G2677T),
         # MDR1_C3435T = as.factor(MDR1_C3435T),
         rejet_aigu = as.factor(rejet_aigu))
```

# Comparison of the datasets

## Summary of the 2 datasets

```{r}
summary(original)
summary(survae)

# Combine original and synthetic data for visualization
combined_data <- rbind(
  original1 %>% mutate(DataType = 'Original'),
  survae_factor %>% mutate(DataType = 'Synthetic')
) %>% mutate_if(is.character, factor)
```

```{r}
## Vector of categorical variables that need transformation
catVars <- c("haplotype", "cyp3A5D",  "sexe_r",  "sexe_d", 
"rejet_aigu", "event")
## Create a variable list.
vars <- c( "haplotype", "cyp3A5D", "age_r", "sexe_r", "age_d", "sexe_d", 
"rejet_aigu", "TIF", "event", "delai_event", "DataType")
tableOne <- CreateTableOne(vars = vars, strata = "DataType",factorVars = catVars, data = combined_data)
tableOne2<-print(tableOne, nonnormal = c( "age_r", "age_d", "TIF", "delai_event"), printToggle=F, minMax=T)
```

```{r , echo=F}
kableone(tableOne2)
```
### individual data explorer

```{r}

# boxplots
plot_boxplot(combined_data , by ="DataType") 

# histograms

# Function to create histogram for each continuous variable
plot_histograms <- function(data, var_name, group_var) {
  ggplot(data, aes(x = !!sym(var_name), fill = !!sym(group_var))) +
    geom_histogram(alpha = 0.5,show.legend = FALSE) +
    labs(x = var_name, y = "Count") +
    theme_minimal() +
    ggtitle(paste(var_name))
}

# Using select_if to identify continuous variables and map to apply the function
plots <- combined_data %>%
  select( -sexe_r,-sexe_d) %>% 
  select_if(is.numeric) %>%
  names() %>%
  map(~plot_histograms(combined_data, ., "DataType"))

# Optionally, print or arrange plots (e.g., using gridExtra or patchwork packages)

wrap_plots(plots)
```

### plot correlation
```{r}

##Correlation Analysis
  cor_real <- cor(original1, use = "complete.obs")
  cor_synthetic <- cor(survae, use = "complete.obs")
  
# plots
ggcorrplot(cor_real, hc.order = TRUE, type = "lower",
           lab = TRUE,  pch.cex = 5,
  tl.cex = 6, lab_size = 2)
# plots
ggcorrplot(cor_synthetic, hc.order = TRUE, type = "lower",
           lab = TRUE,  pch.cex = 5,
  tl.cex = 6, lab_size = 2)
```

## Modele de Cox

```{r}
# original
fit_original <- coxph(Surv(delai_event, event) ~ haplotype + cyp3A5D +  age_r + sexe_r + age_d +  sexe_d + rejet_aigu + TIF , data = original1)
summary(fit_original)
ggforest(fit_original, data = original)

# synthetique
fit_synthetique <- coxph(Surv(delai_event, event) ~ haplotype + cyp3A5D +  age_r + sexe_r + age_d +  sexe_d + rejet_aigu + TIF , data = survae)
summary(fit_synthetique)
ggforest(fit_synthetique)
```


BootstepAIC synhtetic survae

```{r}

boot.stepAIC(fit_synthetique, survae, B = 100, k=log(nrow(survae)))

```


Final model original

```{r}
fit_original <- coxph(Surv(delai_event, event) ~ haplotype +    rejet_aigu , data = original1)
summary(fit_original)
```

Final model synthetic

```{r}
fit_synthetique <- coxph(Surv(delai_event, event) ~ haplotype + rejet_aigu , data = survae)
summary(fit_synthetique)
```


### Bootstrap of the coefficient for haplotype

Allow to define the variability range of HR for a given dataset (intra dataset variability)

```{r bootstrap of the vae}

# Set the seed for reproducibility
set.seed(12)

# Bootstrap the Cox model
boot_results <- boot(data=survae, statistic=cox_model, R=100)

# Convert bootstrap results to a data frame for ggplot2
boot_hrs <- exp(boot_results$t) # Convert log(HR) to HR
hr_data <- data.frame(HR=boot_hrs[,1])

# Calculate summary statistics
summary_stats <- quantile(hr_data$HR, probs = c(0, 0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975, 1))
names(summary_stats) <- c("Min","2.5th", "5th", "25th", "Median", "75th", "95th","97.5th","Max")

# Create the histogram
# Create the histogram
ggplot(hr_data, aes(x=HR)) +
  geom_histogram(bins=30, fill="#007a86", color="black") +
  # # geom_vline(aes(xintercept=summary_stats["Min"]), color="red", linetype="dashed") +
  # geom_vline(aes(xintercept=summary_stats["25th"][[1]][[1]]), color="gray", linetype="dashed", linewidth=2) +
  # geom_vline(aes(xintercept=summary_stats["Median"][[1]][[1]]), color="blue", linetype="dashed", linewidth=2) +
  # geom_vline(aes(xintercept=summary_stats["75th"])[[1]][[1]], color="gray", linetype="dashed", linewidth=2) +
  # geom_vline(aes(xintercept=summary_stats["Max"]), color="purple", linetype="dashed") +
  labs(title="Bootstrap Distribution of Hazard Ratios", x="Hazard Ratio (HR)", y="Frequency") +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5))

# Print summary statistics
knitr::kable(summary_stats, "simple")

```

# Modele final & KM

```{r}
km_original <- survfit(Surv(delai_event, event) ~ haplotype, data = original)
ggsurvplot(
  km_original,
  data = original,
  size = 1,                 # change line size
  conf.int = TRUE,          # Add confidence interval
  pval = TRUE,              # Add p-value
  risk.table = TRUE,        # Add risk table
  risk.table.col = "strata",# Risk table color by groups
  risk.table.height = 0.25, # Useful to change when you have multiple groups
  ggtheme = theme_bw()      # Change ggplot2 theme
)
```

```{r KM survae}
km_synthetique <- survfit(Surv(delai_event, event) ~ haplotype, data = survae_factor)
km_synthetique_survae <- ggsurvplot(
  km_synthetique,
  data = survae_factor,
  size = 1,                 # change line size
  conf.int = TRUE,          # Add confidence interval
  pval = TRUE,              # Add p-value
  risk.table = TRUE,        # Add risk table
  risk.table.col = "strata",# Risk table color by groups
  risk.table.height = 0.25, # Useful to change when you have multiple groups
  ggtheme = theme_bw()      # Change ggplot2 theme
)
km_synthetique_survae
```


Plots original & synthetic combined

```{r}
## combine data
combined_df <- rbind(original %>% mutate(group = "original"), survae_factor %>% mutate(group = "synthetic")) %>% mutate(combined_haplotype = str_c(haplotype,"_", group ))

## fit the model
km_combined <- survfit(Surv(delai_event, event) ~ combined_haplotype, data = combined_df)

# plot
ggsurvplot(fit = km_combined, 
           data = combined_df,
           
  size = 1,                 # change line size
  conf.int = FALSE,          # Add confidence interval
  pval = TRUE,              # Add p-value
  risk.table = TRUE,        # Add risk table
  risk.table.col = "strata",# Risk table color by groups
  risk.table.height = 0.35, # Useful to change when you have multiple groups
  ggtheme = theme_bw()      # Change ggplot2 theme
)

```

Graphical exploration of distribution

```{r}
library(GGally)

pm_survae <- combined_df %>% select(haplotype:delai_event, group) %>% ggpairs(
  ggplot2::aes(colour = group,alpha = 0.5),
  upper = list(continuous = wrap("cor", size = 1.5)),
  lower=list(combo=wrap("facethist", binwidth=0.5))) + 
  theme(strip.text.x = element_text(size = 5),
           strip.text.y = element_text(size = 5),axis.text = element_text(size = 5))
pm_survae
# ggsave("comparaison_distribution_survae.pdf")

```


## Evaluaiton of variability interdataset tvae

```{r variability inter seed tva }

# Définir le répertoire où se trouvent les fichiers
repertoire <- "~/Documents/avatar/tvae_ctgan_variability/Gen_data_synth_for_bootstrap/Generate_graft_loss3_multi/Graft_loss_survae"

# Lire tous les fichiers CSV dans le répertoire
liste_donnees <- list.files(repertoire, pattern = "*.dat", full.names = TRUE) %>%
  map(read_csv)

# Appliquer le modèle de Cox à chaque jeu de données
resultats <- map(liste_donnees, ~ coxph(Surv(delai_event, event) ~ haplotype + rejet_aigu, data = .x))

# Extraire les HR et les quantiles pour chaque variable
quantiles <- c(0, 5, 25, 50, 75, 95, 100)

hr_haplotype <- map(resultats, ~ tidy(.x, exponentiate = TRUE)) %>%
  map_dfr(~ .x %>% filter(term == "haplotype") %>% select(estimate)) %>%
  reframe(across(estimate, ~ quantile(., probs = quantiles/100))) %>% 
  mutate(quantiles = c(0, 5, 25, 50, 75, 95, 100), name = "haplotype")


hr_rejet_aigu <- map(resultats, ~ tidy(.x, exponentiate = TRUE)) %>%
  map_dfr(~ .x %>% filter(term == "rejet_aigu") %>% select(estimate)) %>%
  reframe(across(estimate, ~ quantile(., probs = quantiles/100))) %>% 
  mutate(quantiles = c(0, 5, 25, 50, 75, 95, 100), name = "rejet_aigu")

# Afficher et combiner les résultats
hr_results_tvae <- bind_rows(hr_haplotype, hr_rejet_aigu)
# Print summary statistics
knitr::kable(hr_results_tvae, "simple")


```


## Survtvae Augmented data

### load the data

```{r}
survae_augmented <- read_csv("sfpt24_survae_data_large_v240111.dat") %>% 
   select(haplotype:delai_event)
survae_augmented_factor <- survae_augmented %>% 
  mutate(haplotype = as.factor(haplotype),
         cyp3A5D = as.factor(cyp3A5D),
         sexe_r = as.factor(sexe_r),
         sexe_d = as.factor(sexe_d),
         # CYP3A4_1B = as.factor(CYP3A4_1B),
         # MDR1_C1236T = as.factor(MDR1_C1236T),
         # MDR1_G2677T = as.factor(MDR1_G2677T),
         # MDR1_C3435T = as.factor(MDR1_C3435T),
         rejet_aigu = as.factor(rejet_aigu))
```

# Comparison of the datasets

## Summary of the 2 datasets

```{r}
summary(original)
summary(survae_augmented)

# Combine original and synthetic data for visualization
combined_data <- rbind(
  original1 %>% mutate(DataType = 'Original'),
  survae_augmented_factor %>% mutate(DataType = 'Synthetic')
) %>% mutate_if(is.character, factor)
```

```{r}
## Vector of categorical variables that need transformation
catVars <- c("haplotype", "cyp3A5D",  "sexe_r",  "sexe_d", 
"rejet_aigu", "event")
## Create a variable list.
vars <- c( "haplotype", "cyp3A5D", "age_r", "sexe_r", "age_d", "sexe_d", 
"rejet_aigu", "TIF", "event", "delai_event", "DataType")
tableOne <- CreateTableOne(vars = vars, strata = "DataType",factorVars = catVars, data = combined_data)
tableOne2<-print(tableOne, nonnormal = c( "age_r", "age_d", "TIF", "delai_event"), printToggle=F, minMax=T)
```

```{r , echo=F}
kableone(tableOne2)
```

### individual data explorer

```{r}

# boxplots
plot_boxplot(combined_data , by ="DataType") 

# histograms

# Function to create histogram for each continuous variable
plot_histograms <- function(data, var_name, group_var) {
  ggplot(data, aes(x = !!sym(var_name), fill = !!sym(group_var))) +
    geom_histogram(alpha = 0.5,show.legend = FALSE) +
    labs(x = var_name, y = "Count") +
    theme_minimal() +
    ggtitle(paste(var_name))
}

# Using select_if to identify continuous variables and map to apply the function
plots <- combined_data %>%
  select( -sexe_r,-sexe_d) %>% 
  select_if(is.numeric) %>%
  names() %>%
  map(~plot_histograms(combined_data, ., "DataType"))

# Optionally, print or arrange plots (e.g., using gridExtra or patchwork packages)

wrap_plots(plots)
```

### plot correlation
```{r}

##Correlation Analysis
  cor_real <- cor(original1, use = "complete.obs")
  cor_synthetic <- cor(survae_augmented, use = "complete.obs")
  
# plots
ggcorrplot(cor_real, hc.order = TRUE, type = "lower",
           lab = TRUE,  pch.cex = 5,
  tl.cex = 6, lab_size = 2)
# plots
ggcorrplot(cor_synthetic, hc.order = TRUE, type = "lower",
           lab = TRUE,  pch.cex = 5,
  tl.cex = 6, lab_size = 2)
```

## Modele de Cox

```{r}
# original
fit_original <- coxph(Surv(delai_event, event) ~ haplotype + cyp3A5D +  age_r + sexe_r + age_d +  sexe_d + rejet_aigu + TIF , data = original1)
summary(fit_original)
ggforest(fit_original, data = original)

# synthetique
fit_synthetique <- coxph(Surv(delai_event, event) ~ haplotype + cyp3A5D +  age_r + sexe_r + age_d +  sexe_d + rejet_aigu + TIF , data = survae_augmented)
summary(fit_synthetique)
ggforest(fit_synthetique)
```

BootstepAIC synhtetic augmented survae

```{r}

boot.stepAIC(fit_synthetique, survae_augmented, B = 100, k=log(nrow(survae_augmented)))

```


Final model original

```{r}
fit_original <- coxph(Surv(delai_event, event) ~ haplotype + cyp3A5D , data = original1)
summary(fit_original)
```

Final model synthetic

```{r}
fit_synthetique <- coxph(Surv(delai_event, event) ~ haplotype + cyp3A5D  , data = survae_augmented)
summary(fit_synthetique)
```



### Bootstrap of the coefficient for haplotype

Allow to define the variability range of HR for a given dataset (intra dataset variability)

```{r bootstrqp tvae augmented intra dataset}
# Define the Cox model
cox_model <- function(data, indices) {
  d <- data[indices,] # allows bootstrapping to sample the data
  fit <- coxph(Surv(delai_event, event) ~ haplotype + cyp3A5D , data=d)
  return(fit$coefficients)
}
# Set the seed for reproducibility
set.seed(12)

# Bootstrap the Cox model
boot_results <- boot(data=survae_augmented, statistic=cox_model, R=100)

# Calculate summary statistics

# Convert bootstrap results to a data frame for ggplot2
boot_hrs <- exp(boot_results$t) # Convert log(HR) to HR
hr_data_haplo <- data.frame(HR=boot_hrs[,1])
hr_data_cyp3A5D <- data.frame(HR=boot_hrs[,2])

# Calculate summary statistics
summary_stats <- quantile(hr_data_haplo$HR, probs = c(0, 0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975, 1)) %>%  
  bind_rows(quantile(hr_data_cyp3A5D$HR, probs = c(0, 0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975, 1)) )
names(summary_stats) <- c("Min","2.5th", "5th", "25th", "Median", "75th", "95th","97.5th","Max")

# Create the histogram
ggplot(hr_data_haplo, aes(x=HR)) +
  geom_histogram(bins=30, fill="#007a86", color="black") +
  # # geom_vline(aes(xintercept=summary_stats["Min"]), color="red", linetype="dashed") +
  # geom_vline(aes(xintercept=summary_stats["25th"][[1]][[1]]), color="gray", linetype="dashed", linewidth=2) +
  # geom_vline(aes(xintercept=summary_stats["Median"][[1]][[1]]), color="blue", linetype="dashed", linewidth=2) +
  # geom_vline(aes(xintercept=summary_stats["75th"])[[1]][[1]], color="gray", linetype="dashed", linewidth=2) +
  # geom_vline(aes(xintercept=summary_stats["Max"]), color="purple", linetype="dashed") +
  labs(title="Bootstrap Distribution of Hazard Ratios", x="Hazard Ratio (HR)", y="Frequency") +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5))

# Print summary statistics
knitr::kable(summary_stats, "simple")

```

# Modele final & KM

```{r}
km_original <- survfit(Surv(delai_event, event) ~ haplotype, data = original)
ggsurvplot(
  km_original,
  data = original,
  size = 1,                 # change line size
  conf.int = TRUE,          # Add confidence interval
  pval = TRUE,              # Add p-value
  risk.table = TRUE,        # Add risk table
  risk.table.col = "strata",# Risk table color by groups
  risk.table.height = 0.25, # Useful to change when you have multiple groups
  ggtheme = theme_bw()      # Change ggplot2 theme
)
```

```{r KM augmented survae}
km_synthetique <- survfit(Surv(delai_event, event) ~ haplotype, data = survae_augmented_factor)
km_synthetique_survae_augmented <- ggsurvplot(
  km_synthetique,
  data = survae_augmented_factor,
  size = 1,                 # change line size
  conf.int = TRUE,          # Add confidence interval
  pval = TRUE,              # Add p-value
  risk.table = TRUE,        # Add risk table
  risk.table.col = "strata",# Risk table color by groups
  risk.table.height = 0.25, # Useful to change when you have multiple groups
  ggtheme = theme_bw()      # Change ggplot2 theme
)
km_synthetique_survae_augmented
```


Plots original & synthetic combined

```{r}
## combine data
combined_df <- rbind(original %>% mutate(group = "original"), survae_augmented_factor %>% mutate(group = "synthetic")) %>% mutate(combined_haplotype = str_c(haplotype,"_", group ))

## fit the model
km_combined <- survfit(Surv(delai_event, event) ~ combined_haplotype, data = combined_df)

# plot
ggsurvplot(fit = km_combined, 
           data = combined_df,
           
  size = 1,                 # change line size
  conf.int = FALSE,          # Add confidence interval
  pval = TRUE,              # Add p-value
  risk.table = TRUE,        # Add risk table
  risk.table.col = "strata",# Risk table color by groups
  risk.table.height = 0.35, # Useful to change when you have multiple groups
  ggtheme = theme_bw()      # Change ggplot2 theme
)

```

Graphical exploration of distribution

```{r}
library(GGally)

pm_survae_augmented <- combined_df %>% select(haplotype:delai_event, group) %>% ggpairs(
  ggplot2::aes(colour = group,alpha = 0.5),
  upper = list(continuous = wrap("cor", size = 1.5)),
  lower=list(combo=wrap("facethist", binwidth=0.5))) + 
  theme(strip.text.x = element_text(size = 5),
           strip.text.y = element_text(size = 5),axis.text = element_text(size = 5))
pm_survae_augmented
# ggsave("comparaison_distribution_survae_augmented.pdf")

```

## generation of 500 dataset for tvae and ct-gan using python

```{python, eval=FALSE}
# génère les données synthétiques à partir des données de survie de perte de greffe
# Algorithmes utilisés issus de synthcity : survival-CTGAN, survAE
# La taille des données synthétiques est 4 fois la taille des données originales et la taille des données initiales.
# Cette opération est répétée 500 fois.

from synthcity.plugins.core.dataloader import SurvivalAnalysisDataLoader, GenericDataLoader
from synthcity.plugins import Plugins
import pandas as pd


donnees=pd.read_csv("../Donnees/original_sfpt.csv")#"../Donnees/modele_final_francoise_inflammaiton_tac.csv",sep=";").loc[:,['id','age', 'sexe', 'log_c_d', 'classe_crp', 'log_crp', 'log_tgp', 'HTE', 'log_dlpgref', 'dlpgreffe']].dropna(axis=0)#log_bilit et les liste_das supprimés; gestions des valeurs manquantes similaire à celle de JBW
n_generate=donnees.shape[0]


n_simu=500
i_simu_excluded_list=[21,38,56,134,139,175,219,260,271,289,301,314,315,317,320,325,353,394,436,438,445,449,464,470,473,488,492,514,527]
i_simu_begin=527#utilisé pour accélérer la reprise des travaux après un arrêt du programme


for i_simu in range(i_simu_begin,n_simu+len(i_simu_excluded_list)):
  if i_simu in i_simu_excluded_list:
    continue
  graft_loss_dl=SurvivalAnalysisDataLoader(donnees,
            target_column="event",
            time_to_event_column="delai_event",random_state=7*i_simu)
  syn_model = Plugins().get("survival_ctgan",random_state=1+7*i_simu)
  syn_model.fit(graft_loss_dl)
  syn_data=syn_model.generate(count=n_generate,random_state=2+7*i_simu)
  donnees_ctgan=syn_data.data
  #donnees_ctgan["age"]=donnees_ctgan.groupby("id")["age"].transform("first")
  #donnees_ctgan["sexe"]=donnees_ctgan.groupby("id")["sexe"].transform("first")
  donnees_ctgan.to_csv(path_or_buf="Graft_loss_surv_ctgan/graft_loss_surv_ctgan_data_3_multi"+str(i_simu).zfill(3)+".dat",index=False)
  syn_data=syn_model.generate(count=4*n_generate,random_state=3+7*i_simu)
  donnees_ctgan_large=syn_data.data
  #donnees_ctgan_large["age"]=donnees_ctgan_large.roupby("id")["age"].transform("first")
  #donnees_ctgan_large["sexe"]=donnees_ctgan_large.groupby("id")["sexe"].transform("first")
  donnees_ctgan_large.to_csv(path_or_buf="Graft_loss_surv_ctgan_large/graft_loss_surv_ctgan_data_large3_multi_"+str(i_simu).zfill(3)+".dat",index=False)
  syn_model = Plugins().get("survae",random_state=4+7*i_simu)
  syn_model.fit(graft_loss_dl)
  syn_data=syn_model.generate(count=n_generate,random_state=5+7*i_simu)
  donnees_tvae=syn_data.data
  #donnees_tvae["age"]=donnees_tvae.groupby("id")["age"].transform("first")
  #donnees_tvae["sexe"]=donnees_tvae.groupby("id")["sexe"].transform("first")
  donnees_tvae.to_csv(path_or_buf="Graft_loss_survae/graft_loss_survae_data3_multi_"+str(i_simu).zfill(3)+".dat",index=False)
  syn_data=syn_model.generate(count=4*n_generate,random_state=6+7*i_simu)
  donnees_tvae_large=syn_data.data
  #donnees_tvae_large["age"]=donnees_tvae_large.groupby("id")["age"].transform("first")
  #donnees_tvae_large["sexe"]=donnees_tvae_large.groupby("id")["sexe"].transform("first")
  syn_data.data.to_csv(path_or_buf="Graft_loss_survae_large/graft_loss_survae_data_large3_multi_"+str(i_simu).zfill(3)+".dat",index=False)



```



## Evaluation of variability interdataset augmented tvae

```{r variability inter seed tva augmented}

# Définir le répertoire où se trouvent les fichiers
repertoire <- "~/Documents/avatar/tvae_ctgan_variability/Gen_data_synth_for_bootstrap/Generate_graft_loss3_multi/Graft_loss_survae_large"

# Lire tous les fichiers CSV dans le répertoire
liste_donnees <- list.files(repertoire, pattern = "*.dat", full.names = TRUE) %>%
  map(read_csv)

# Appliquer le modèle de Cox à chaque jeu de données
resultats <- map(liste_donnees, ~ coxph(Surv(delai_event, event) ~ haplotype +  cyp3A5D, data = .x))

# Extraire les HR et les quantiles pour chaque variable
quantiles <- c(0, 5, 25, 50, 75, 95, 100)

hr_haplotype <- map(resultats, ~ tidy(.x, exponentiate = TRUE)) %>%
  map_dfr(~ .x %>% filter(term == "haplotype") %>% select(estimate)) %>%
  reframe(across(estimate, ~ quantile(., probs = quantiles/100))) %>% 
  mutate(quantiles = c(0, 5, 25, 50, 75, 95, 100), name = "haplotype")


hr_cyp3A5D <- map(resultats, ~ tidy(.x, exponentiate = TRUE)) %>%
  map_dfr(~ .x %>% filter(term == "cyp3A5D") %>% select(estimate)) %>%
  reframe(across(estimate, ~ quantile(., probs = quantiles/100))) %>% 
  mutate(quantiles = c(0, 5, 25, 50, 75, 95, 100), name = "cyp3A5D")


# Afficher et combiner les résultats
hr_results_augmented_tvae <-bind_rows(hr_haplotype, hr_cyp3A5D)
# Print summary statistics
knitr::kable(hr_results_augmented_tvae, "simple")


```


# Survival Tabular Generative Adversial Network (CTGAN)

These data have been generated by Clement Benoist using the Synthcity python libnrary from the Van der Schaar lab

## Non augmented data

### load the data

```{r}
survctgan <- read_csv("sfpt24_survctgan_data_v240111.dat") %>% 
   select(haplotype:delai_event) 
survctgan_factor <- survctgan %>% 
  mutate(haplotype = as.factor(haplotype),
         cyp3A5D = as.factor(cyp3A5D),
         sexe_r = as.factor(sexe_r),
         sexe_d = as.factor(sexe_d),
         # CYP3A4_1B = as.factor(CYP3A4_1B),
         # MDR1_C1236T = as.factor(MDR1_C1236T),
         # MDR1_G2677T = as.factor(MDR1_G2677T),
         # MDR1_C3435T = as.factor(MDR1_C3435T),
         rejet_aigu = as.factor(rejet_aigu))
```

# Comparison of the datasets

## Summary of the 2 datasets

```{r}
summary(original)
summary(survctgan)

# Combine original and synthetic data for visualization
combined_data <- rbind(
  original1 %>% mutate(DataType = 'Original'),
  survctgan_factor %>% mutate(DataType = 'Synthetic')
) %>% mutate_if(is.character, factor)
```

```{r}
## Vector of categorical variables that need transformation
catVars <- c("haplotype", "cyp3A5D",  "sexe_r",  "sexe_d", 
"rejet_aigu", "event")
## Create a variable list.
vars <- c( "haplotype", "cyp3A5D", "age_r", "sexe_r", "age_d", "sexe_d", 
"rejet_aigu", "TIF", "event", "delai_event", "DataType")
tableOne <- CreateTableOne(vars = vars, strata = "DataType",factorVars = catVars, data = combined_data)
tableOne2<-print(tableOne, nonnormal = c( "age_r", "age_d", "TIF", "delai_event"), printToggle=F, minMax=T)
```

```{r , echo=F}
kableone(tableOne2)
```

### individual data explorer

```{r}

# boxplots
plot_boxplot(combined_data , by ="DataType") 

# histograms

# Function to create histogram for each continuous variable
plot_histograms <- function(data, var_name, group_var) {
  ggplot(data, aes(x = !!sym(var_name), fill = !!sym(group_var))) +
    geom_histogram(alpha = 0.5,show.legend = FALSE) +
    labs(x = var_name, y = "Count") +
    theme_minimal() +
    ggtitle(paste(var_name))
}

# Using select_if to identify continuous variables and map to apply the function
plots <- combined_data %>%
  select( -sexe_r,-sexe_d) %>% 
  select_if(is.numeric) %>%
  names() %>%
  map(~plot_histograms(combined_data, ., "DataType"))

# Optionally, print or arrange plots (e.g., using gridExtra or patchwork packages)

wrap_plots(plots)
```

### plot correlation

```{r}

##Correlation Analysis
  cor_real <- cor(original1, use = "complete.obs")
  cor_synthetic <- cor(survctgan, use = "complete.obs")
  
# plots
ggcorrplot(cor_real, hc.order = TRUE, type = "lower",
           lab = TRUE,  pch.cex = 5,
  tl.cex = 6, lab_size = 2)
# plots
ggcorrplot(cor_synthetic, hc.order = TRUE, type = "lower",
           lab = TRUE,  pch.cex = 5,
  tl.cex = 6, lab_size = 2)
```

## Modele de Cox

```{r}
# original
fit_original <- coxph(Surv(delai_event, event) ~ haplotype + cyp3A5D +  age_r + sexe_r + age_d +  sexe_d + rejet_aigu + TIF , data = original1)
summary(fit_original)
ggforest(fit_original)

# synthetique
fit_synthetique <- coxph(Surv(delai_event, event) ~ haplotype + cyp3A5D +  age_r + sexe_r + age_d +  sexe_d + rejet_aigu + TIF , data = survctgan)
summary(fit_synthetique)
ggforest(fit_synthetique)
```

BootstepAIC synhtetic ctGAN

```{r}

boot.stepAIC(fit_synthetique, survctgan, B = 100, k=log(nrow(survctgan)))

```

Final model original

```{r}
fit_original <- coxph(Surv(delai_event, event) ~ haplotype +    rejet_aigu , data = original1)
summary(fit_original)
```

Final model synthetic

```{r}
fit_synthetique <- coxph(Surv(delai_event, event) ~ haplotype + age_r+ age_d , data = survctgan)
summary(fit_synthetique)
```

### Bootstrap of the coefficient for haplotype

Allow to define the variability range of HR for a given dataset (intra dataset variability)

```{r}
# Define the Cox model
cox_model <- function(data, indices) {
  d <- data[indices,] # allows bootstrapping to sample the data
  fit <- coxph(Surv(delai_event, event) ~ haplotype + age_r+ age_d  , data=d)
  return(fit$coefficients)
}
# Set the seed for reproducibility
set.seed(12)

# Bootstrap the Cox model
boot_results <- boot(data=survctgan, statistic=cox_model, R=100)


# Convert bootstrap results to a data frame for ggplot2
boot_hrs <- exp(boot_results$t) # Convert log(HR) to HR
hr_data_haplo <- data.frame(HR=boot_hrs[,1])
hr_data_age_r <- data.frame(HR=boot_hrs[,2])
hr_data_age_d <- data.frame(HR=boot_hrs[,3])

# Calculate summary statistics
summary_stats <- quantile(hr_data_haplo$HR, probs = c(0, 0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975, 1)) %>%  
  bind_rows(quantile(hr_data_age_r$HR, probs = c(0, 0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975, 1)) ) %>% 
    bind_rows(quantile(hr_data_age_d$HR, probs = c(0, 0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975, 1)) )
names(summary_stats) <- c("Min","2.5th", "5th", "25th", "Median", "75th", "95th","97.5th","Max")



# Create the histogram
ggplot(hr_data_haplo, aes(x=HR)) +
  geom_histogram(bins=30, fill="#007a86", color="black") +
  # # geom_vline(aes(xintercept=summary_stats["Min"]), color="red", linetype="dashed") +
  # geom_vline(aes(xintercept=summary_stats["25th"][[1]][[1]]), color="gray", linetype="dashed", linewidth=2) +
  # geom_vline(aes(xintercept=summary_stats["Median"][[1]][[1]]), color="blue", linetype="dashed", linewidth=2) +
  # geom_vline(aes(xintercept=summary_stats["75th"])[[1]][[1]], color="gray", linetype="dashed", linewidth=2) +
  # geom_vline(aes(xintercept=summary_stats["Max"]), color="purple", linetype="dashed") +
  labs(title="Bootstrap Distribution of Hazard Ratios", x="Hazard Ratio (HR)", y="Frequency") +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5))

# Print summary statistics
knitr::kable(summary_stats, "simple")
```

# Modele final & KM

```{r}
km_original <- survfit(Surv(delai_event, event) ~ haplotype, data = original)
ggsurvplot(
  km_original,
  data = original,
  size = 1,                 # change line size
  conf.int = TRUE,          # Add confidence interval
  pval = TRUE,              # Add p-value
  risk.table = TRUE,        # Add risk table
  risk.table.col = "strata",# Risk table color by groups
  risk.table.height = 0.25, # Useful to change when you have multiple groups
  ggtheme = theme_bw()      # Change ggplot2 theme
)
```

```{r KM ctgan}
km_synthetique <- survfit(Surv(delai_event, event) ~ haplotype, data = survctgan_factor)
km_synthetique_ctgan <- ggsurvplot(
  km_synthetique,
  data = survctgan_factor,
  size = 1,                 # change line size
  conf.int = TRUE,          # Add confidence interval
  pval = TRUE,              # Add p-value
  risk.table = TRUE,        # Add risk table
  risk.table.col = "strata",# Risk table color by groups
  risk.table.height = 0.25, # Useful to change when you have multiple groups
  ggtheme = theme_bw()      # Change ggplot2 theme
)
km_synthetique_ctgan
```


Plots original & synthetic combined

```{r}
## combine data
combined_df <- rbind(original %>% mutate(group = "original"), survctgan_factor %>% mutate(group = "synthetic")) %>% mutate(combined_haplotype = str_c(haplotype,"_", group ))

## fit the model
km_combined <- survfit(Surv(delai_event, event) ~ combined_haplotype, data = combined_df)

# plot
ggsurvplot(fit = km_combined, 
           data = combined_df,
           
  size = 1,                 # change line size
  conf.int = FALSE,          # Add confidence interval
  pval = TRUE,              # Add p-value
  risk.table = TRUE,        # Add risk table
  risk.table.col = "strata",# Risk table color by groups
  risk.table.height = 0.35, # Useful to change when you have multiple groups
  ggtheme = theme_bw()      # Change ggplot2 theme
)

```

Graphical exploraiotn of distribution

```{r}
library(GGally)

pm_ctgan <- combined_df %>% select(haplotype:delai_event, group) %>% ggpairs(
  ggplot2::aes(colour = group,alpha = 0.5),
  upper = list(continuous = wrap("cor", size = 1.5)),
  lower=list(combo=wrap("facethist", binwidth=0.5))) + 
  theme(strip.text.x = element_text(size = 5),
           strip.text.y = element_text(size = 5),axis.text = element_text(size = 5))
pm_ctgan
# ggsave("comparaison_distribution_survctgan.pdf")

```

## Evaluation of variability interdataset ctgan

```{r variability inter seed ctgan }

# Définir le répertoire où se trouvent les fichiers
repertoire <- "~/Documents/avatar/tvae_ctgan_variability/Gen_data_synth_for_bootstrap/Generate_graft_loss3_multi/Graft_loss_surv_ctgan"

# Lire tous les fichiers CSV dans le répertoire
liste_donnees <- list.files(repertoire, pattern = "*.dat", full.names = TRUE) %>%
  map(read_csv)

# Appliquer le modèle de Cox à chaque jeu de données
resultats <- map(liste_donnees, ~ coxph(Surv(delai_event, event) ~ haplotype + age_r+ age_d , data = .x))

# Extraire les HR et les quantiles pour chaque variable
quantiles <- c(0, 5, 25, 50, 75, 95, 100)

hr_haplotype <- map(resultats, ~ tidy(.x, exponentiate = TRUE)) %>%
  map_dfr(~ .x %>% filter(term == "haplotype") %>% select(estimate)) %>%
  reframe(across(estimate, ~ quantile(., probs = quantiles/100))) %>% 
  mutate(quantiles = c(0, 5, 25, 50, 75, 95, 100), name = "haplotype")


hr_age_r <- map(resultats, ~ tidy(.x, exponentiate = TRUE)) %>%
  map_dfr(~ .x %>% filter(term == "age_r") %>% select(estimate)) %>%
  reframe(across(estimate, ~ quantile(., probs = quantiles/100))) %>% 
  mutate(quantiles = c(0, 5, 25, 50, 75, 95, 100), name = "age_r")

hr_age_d <- map(resultats, ~ tidy(.x, exponentiate = TRUE)) %>%
  map_dfr(~ .x %>% filter(term == "age_d") %>% select(estimate)) %>%
  reframe(across(estimate, ~ quantile(., probs = quantiles/100))) %>% 
  mutate(quantiles = c(0, 5, 25, 50, 75, 95, 100), name = "age_d")

# Afficher et combiner les résultats
hr_results_ctgan <- bind_rows(hr_haplotype, hr_age_r,hr_age_d)
# Print summary statistics
knitr::kable(hr_results_ctgan, "simple")


```

## Augmented data

### load the data

```{r}
survctgan_augmented <- read_csv("sfpt24_survctgan_data_large_v240111.dat") %>% 
   select(haplotype:delai_event)
survctgan_augmented_factor <- survctgan_augmented %>% 
  mutate(haplotype = as.factor(haplotype),
         cyp3A5D = as.factor(cyp3A5D),
         sexe_r = as.factor(sexe_r),
         sexe_d = as.factor(sexe_d),
         # CYP3A4_1B = as.factor(CYP3A4_1B),
         # MDR1_C1236T = as.factor(MDR1_C1236T),
         # MDR1_G2677T = as.factor(MDR1_G2677T),
         # MDR1_C3435T = as.factor(MDR1_C3435T),
         rejet_aigu = as.factor(rejet_aigu))
```

# Comparison of the datasets

## Summary of the 2 datasets

```{r}
summary(original)
summary(survctgan_augmented)

# Combine original and synthetic data for visualization
combined_data <- rbind(
  original1 %>% mutate(DataType = 'Original'),
  survctgan_augmented_factor %>% mutate(DataType = 'Synthetic')
) %>% mutate_if(is.character, factor)
```

```{r}
## Vector of categorical variables that need transformation
catVars <- c("haplotype", "cyp3A5D",  "sexe_r",  "sexe_d", 
"rejet_aigu", "event")
## Create a variable list.
vars <- c( "haplotype", "cyp3A5D", "age_r", "sexe_r", "age_d", "sexe_d", 
"rejet_aigu", "TIF", "event", "delai_event", "DataType")
tableOne <- CreateTableOne(vars = vars, strata = "DataType",factorVars = catVars, data = combined_data)
tableOne2<-print(tableOne, nonnormal = c( "age_r", "age_d", "TIF", "delai_event"), printToggle=F, minMax=T)
```

```{r , echo=F}
kableone(tableOne2)
```

### individual data explorer

```{r}

# boxplots
plot_boxplot(combined_data , by ="DataType") 

# histograms

# Function to create histogram for each continuous variable
plot_histograms <- function(data, var_name, group_var) {
  ggplot(data, aes(x = !!sym(var_name), fill = !!sym(group_var))) +
    geom_histogram(alpha = 0.5,show.legend = FALSE) +
    labs(x = var_name, y = "Count") +
    theme_minimal() +
    ggtitle(paste(var_name))
}

# Using select_if to identify continuous variables and map to apply the function
plots <- combined_data %>%
  select( -sexe_r,-sexe_d) %>% 
  select_if(is.numeric) %>%
  names() %>%
  map(~plot_histograms(combined_data, ., "DataType"))

# Optionally, print or arrange plots (e.g., using gridExtra or patchwork packages)

wrap_plots(plots)
```

### plot correlation
```{r}

##Correlation Analysis
  cor_real <- cor(original1, use = "complete.obs")
  cor_synthetic <- cor(survctgan_augmented, use = "complete.obs")
  
# plots
ggcorrplot(cor_real, hc.order = TRUE, type = "lower",
           lab = TRUE,  pch.cex = 5,
  tl.cex = 6, lab_size = 2)
# plots
ggcorrplot(cor_synthetic, hc.order = TRUE, type = "lower",
           lab = TRUE,  pch.cex = 5,
  tl.cex = 6, lab_size = 2)
```

## Modele de Cox

```{r}
# original
fit_original <- coxph(Surv(delai_event, event) ~ haplotype + cyp3A5D +  age_r + sexe_r + age_d +  sexe_d + rejet_aigu + TIF , data = original1)
summary(fit_original)
ggforest(fit_original)

# synthetique
fit_synthetique <- coxph(Surv(delai_event, event) ~ haplotype + cyp3A5D +  age_r + sexe_r + age_d +  sexe_d + rejet_aigu + TIF , data = survctgan_augmented)
summary(fit_synthetique)
ggforest(fit_synthetique)
```

BootstepAIC synhtetic augmented ctGAN

```{r}

boot.stepAIC(fit_synthetique, survctgan_augmented, B = 100, k=log(nrow(survctgan_augmented)))

```


Final model original

```{r}
fit_original <- coxph(Surv(delai_event, event) ~ haplotype +    rejet_aigu , data = original1)
summary(fit_original)
```

Final model synthetic

```{r}
fit_synthetique <- coxph(Surv(delai_event, event) ~ haplotype +  cyp3A5D+ age_d  , data = survctgan_augmented)
summary(fit_synthetique)
```

### Bootstrap of the coefficient for haplotype

Allow to define the variability range of HR for a given dataset (intra dataset variability)

```{r}
# Define the Cox model
cox_model <- function(data, indices) {
  d <- data[indices,] # allows bootstrapping to sample the data
  fit <- coxph(Surv(delai_event, event) ~ haplotype + cyp3A5D+ age_d  , data=d)
  return(fit$coefficients)
}

# Set the seed for reproducibility
set.seed(12)

# Bootstrap the Cox model
boot_results <- boot(data=survctgan_augmented, statistic=cox_model, R=100)

# Convert bootstrap results to a data frame for ggplot2
boot_hrs <- exp(boot_results$t) # Convert log(HR) to HR
hr_data_haplo <- data.frame(HR=boot_hrs[,1])
hr_data_cyp3A5D <- data.frame(HR=boot_hrs[,2])
hr_data_age_d <- data.frame(HR=boot_hrs[,3])

# Calculate summary statistics
summary_stats <- quantile(hr_data_haplo$HR, probs = c(0, 0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975, 1)) %>%  
  bind_rows(quantile(hr_data_cyp3A5D$HR, probs = c(0, 0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975, 1)) ) %>% 
    bind_rows(quantile(hr_data_age_d$HR, probs = c(0, 0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975, 1)) )
names(summary_stats) <- c("Min","2.5th", "5th", "25th", "Median", "75th", "95th","97.5th","Max")


# Create the histogram
ggplot(hr_data_haplo, aes(x=HR)) +
  geom_histogram(bins=30, fill="#007a86", color="black") +
  # # geom_vline(aes(xintercept=summary_stats["Min"]), color="red", linetype="dashed") +
  # geom_vline(aes(xintercept=summary_stats["25th"][[1]][[1]]), color="gray", linetype="dashed", linewidth=2) +
  # geom_vline(aes(xintercept=summary_stats["Median"][[1]][[1]]), color="blue", linetype="dashed", linewidth=2) +
  # geom_vline(aes(xintercept=summary_stats["75th"])[[1]][[1]], color="gray", linetype="dashed", linewidth=2) +
  # geom_vline(aes(xintercept=summary_stats["Max"]), color="purple", linetype="dashed") +
  labs(title="Bootstrap Distribution of Hazard Ratios", x="Hazard Ratio (HR)", y="Frequency") +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5))


# Print summary statistics
knitr::kable(summary_stats, "simple")

```


# Modele final & KM

```{r}
km_original <- survfit(Surv(delai_event, event) ~ haplotype, data = original)
ggsurvplot(
  km_original,
  data = original,
  size = 1,                 # change line size
  conf.int = TRUE,          # Add confidence interval
  pval = TRUE,              # Add p-value
  risk.table = TRUE,        # Add risk table
  risk.table.col = "strata",# Risk table color by groups
  risk.table.height = 0.25, # Useful to change when you have multiple groups
  ggtheme = theme_bw()      # Change ggplot2 theme
)
```

```{r KM augmented ctGAN}
km_synthetique <- survfit(Surv(delai_event, event) ~ haplotype, data = survctgan_augmented_factor)
km_synthetique_ctgan_augmented <- ggsurvplot(
  km_synthetique,
  data = survctgan_augmented_factor,
  size = 1,                 # change line size
  conf.int = TRUE,          # Add confidence interval
  pval = TRUE,              # Add p-value
  risk.table = TRUE,        # Add risk table
  risk.table.col = "strata",# Risk table color by groups
  risk.table.height = 0.25, # Useful to change when you have multiple groups
  ggtheme = theme_bw()      # Change ggplot2 theme
)
km_synthetique_ctgan_augmented
```


Plots original & synthetic combined

```{r}
## combine data
combined_df <- rbind(original %>% mutate(group = "original"), survctgan_augmented_factor %>% mutate(group = "synthetic")) %>% mutate(combined_haplotype = str_c(haplotype,"_", group ))

## fit the model
km_combined <- survfit(Surv(delai_event, event) ~ combined_haplotype, data = combined_df)

# plot
ggsurvplot(fit = km_combined, 
           data = combined_df,
           
  size = 1,                 # change line size
  conf.int = FALSE,          # Add confidence interval
  pval = TRUE,              # Add p-value
  risk.table = TRUE,        # Add risk table
  risk.table.col = "strata",# Risk table color by groups
  risk.table.height = 0.35, # Useful to change when you have multiple groups
  ggtheme = theme_bw()      # Change ggplot2 theme
)

```

Graphical exploraiotn of distribution

```{r}
library(GGally)

pm_ctgan_augmented <- combined_df %>% select(haplotype:delai_event, group) %>% ggpairs(
  ggplot2::aes(colour = group,alpha = 0.5),
  upper = list(continuous = wrap("cor", size = 1.5)),
  lower=list(combo=wrap("facethist", binwidth=0.5))) + 
  theme(strip.text.x = element_text(size = 5),
           strip.text.y = element_text(size = 5),axis.text = element_text(size = 5))
pm_ctgan_augmented
# ggsave("comparaison_distribution_survctgan_augmented.pdf")

```


## Evaluation of variability interdataset augmented ctgan

```{r variability inter seed ctgan augmented}

# Définir le répertoire où se trouvent les fichiers
repertoire <- "~/Documents/avatar/tvae_ctgan_variability/Gen_data_synth_for_bootstrap/Generate_graft_loss3_multi/Graft_loss_surv_ctgan_large"

# Lire tous les fichiers CSV dans le répertoire
liste_donnees <- list.files(repertoire, pattern = "*.dat", full.names = TRUE) %>%
  map(read_csv)

# Appliquer le modèle de Cox à chaque jeu de données
resultats <- map(liste_donnees, ~ coxph(Surv(delai_event, event) ~ haplotype + cyp3A5D+ age_d , data = .x))

# Extraire les HR et les quantiles pour chaque variable
quantiles <- c(0, 5, 25, 50, 75, 95, 100)

hr_haplotype <- map(resultats, ~ tidy(.x, exponentiate = TRUE)) %>%
  map_dfr(~ .x %>% filter(term == "haplotype") %>% select(estimate)) %>%
  reframe(across(estimate, ~ quantile(., probs = quantiles/100))) %>% 
  mutate(quantiles = c(0, 5, 25, 50, 75, 95, 100), name = "haplotype")


hr_cyp3A5D <- map(resultats, ~ tidy(.x, exponentiate = TRUE)) %>%
  map_dfr(~ .x %>% filter(term == "cyp3A5D") %>% select(estimate)) %>%
  reframe(across(estimate, ~ quantile(., probs = quantiles/100))) %>% 
  mutate(quantiles = c(0, 5, 25, 50, 75, 95, 100), name = "cyp3A5D")

hr_age_d <- map(resultats, ~ tidy(.x, exponentiate = TRUE)) %>%
  map_dfr(~ .x %>% filter(term == "age_d") %>% select(estimate)) %>%
  reframe(across(estimate, ~ quantile(., probs = quantiles/100))) %>% 
  mutate(quantiles = c(0, 5, 25, 50, 75, 95, 100), name = "age_d")

# Afficher et combiner les résultats
hr_results_augmented_ctgan <- bind_rows(hr_haplotype, hr_cyp3A5D,hr_age_d)
# Print summary statistics
knitr::kable(hr_results_augmented_ctgan, "simple")


```

# Tableone summary all data

```{r, tableone summary all data}
combined_data <- bind_rows(original1 %>% mutate(DataType = 'Original', haplotype = as.factor(haplotype), cyp3A5D = as.factor(cyp3A5D), sexe_r = as.factor(sexe_r), sexe_d = as.factor(sexe_d),rejet_aigu = as.factor(rejet_aigu),event = as.factor(event)),
                           avatars_tibble_knn5 %>% mutate(DataType = 'knn5', haplotype = as.factor(haplotype), cyp3A5D = as.factor(cyp3A5D), sexe_r = as.factor(sexe_r), sexe_d = as.factor(sexe_d),rejet_aigu = as.factor(rejet_aigu),event = as.factor(event)),
                           avatars_tibble_knn20 %>% mutate(DataType = 'knn20', haplotype = as.factor(haplotype), cyp3A5D = as.factor(cyp3A5D), sexe_r = as.factor(sexe_r), sexe_d = as.factor(sexe_d),rejet_aigu = as.factor(rejet_aigu),event = as.factor(event)),
                           avatars_tibble_knn10 %>% mutate(DataType = 'knn10', haplotype = as.factor(haplotype), cyp3A5D = as.factor(cyp3A5D), sexe_r = as.factor(sexe_r), sexe_d = as.factor(sexe_d),rejet_aigu = as.factor(rejet_aigu),event = as.factor(event)),
                           augmented_data_5 %>% mutate(DataType = 'augmented_knn5', haplotype = as.factor(haplotype), cyp3A5D = as.factor(cyp3A5D), sexe_r = as.factor(sexe_r), sexe_d = as.factor(sexe_d),rejet_aigu = as.factor(rejet_aigu),event = as.factor(event)),
                           augmented_data_20 %>% mutate(DataType = 'augmented_knn20', haplotype = as.factor(haplotype), cyp3A5D = as.factor(cyp3A5D), sexe_r = as.factor(sexe_r), sexe_d = as.factor(sexe_d),rejet_aigu = as.factor(rejet_aigu),event = as.factor(event)),
                           augmented_data_10 %>% mutate(DataType = 'augmented_knn10', haplotype = as.factor(haplotype), cyp3A5D = as.factor(cyp3A5D), sexe_r = as.factor(sexe_r), sexe_d = as.factor(sexe_d),rejet_aigu = as.factor(rejet_aigu),event = as.factor(event)),
                           survae_factor %>% mutate(DataType = 'survae', haplotype = as.factor(haplotype), cyp3A5D = as.factor(cyp3A5D), sexe_r = as.factor(sexe_r), sexe_d = as.factor(sexe_d),rejet_aigu = as.factor(rejet_aigu),event = as.factor(event)),
                           survae_augmented_factor %>% mutate(DataType = 'augmented_survae', haplotype = as.factor(haplotype), cyp3A5D = as.factor(cyp3A5D), sexe_r = as.factor(sexe_r), sexe_d = as.factor(sexe_d),rejet_aigu = as.factor(rejet_aigu),event = as.factor(event)),
                           survctgan_factor %>% mutate(DataType = 'ctgan', haplotype = as.factor(haplotype), cyp3A5D = as.factor(cyp3A5D), sexe_r = as.factor(sexe_r), sexe_d = as.factor(sexe_d),rejet_aigu = as.factor(rejet_aigu),event = as.factor(event)),
                           survctgan_augmented_factor %>% mutate(DataType = 'augmented_ctgan', haplotype = as.factor(haplotype), cyp3A5D = as.factor(cyp3A5D), sexe_r = as.factor(sexe_r), sexe_d = as.factor(sexe_d),rejet_aigu = as.factor(rejet_aigu),event = as.factor(event))
                           )



## Vector of categorical variables that need transformation
catVars <- c("haplotype", "cyp3A5D",  "sexe_r",  "sexe_d", 
             "rejet_aigu", "event")
## Create a variable list.
vars <- c( "haplotype", "cyp3A5D", "age_r", "sexe_r", "age_d", "sexe_d", 
           "rejet_aigu", "TIF", "event", "delai_event", "DataType")
tableOne <- CreateTableOne(vars = vars, strata = "DataType",factorVars = catVars, data = combined_data)
tableOne2<-print(tableOne, nonnormal = c( "age_r", "age_d", "TIF", "delai_event"), printToggle=F, minMax=T)
kableone(tableOne2)
```

# plot all covariation ggpair

```{r figure ggpair combined}
library(patchwork)
pm_knn5
ggsave("Figure1.pdf")


```

# plot all KM curve haplotype

```{r figure km combined}
# List of ggsurvplots
require("survminer")
splots <- list()
splots[[1]] <- km_original_plot
splots[[2]] <- km_synthetique_avatar_5 
splots[[3]] <- km_synthetique_avatar_5_augmented 
splots[[4]] <- km_synthetique_survae 
splots[[5]] <- km_synthetique_survae_augmented 
splots[[6]] <- km_synthetique_ctgan 
splots[[7]] <- km_synthetique_ctgan_augmented 

# Arrange multiple ggsurvplots and print the output
arrange_ggsurvplots(splots, print = TRUE,
  ncol = 1, nrow = 7)#, risk.table.height = 0.4)


if (FALSE) {
# Arrange and save into pdf file
res <- arrange_ggsurvplots(splots, print = FALSE)
ggsave("Figure2.pdf", res)
}

```

# python scirpt for clauclation of privacy metrics

ALl the privacy metrics have been calculated by clement benoist using the following script

```{python, eval=FALSE}
#Sur le jeu de données de perte du greffon, métriques de fidélité et de privacy
#Algorithme de syrnthèse de données : Avatar knn=3,5,10,15,20, Surv-CTGAN, surVAE

import synthcity
import pandas as pd
import synthcity.metrics

import synthcity.metrics.eval_privacy
import sklearn 
import sklearn.model_selection

import sksurv
import sksurv.ensemble

import numpy as np

#loading of the data

columns_drop_list=["CYP3A4_1B","MDR1_C1236T","MDR1_G2677T","MDR1_C3435T","pente_creat"]

donnees_origin=pd.read_csv("../../Anonym_sfpt24/Donnees/original_sfpt.csv").drop(columns=columns_drop_list)
donnees_avatar_knn3=pd.read_csv("../../Anonym_sfpt24/Donnees/avatar_sfpt_knn3.csv").drop(columns=columns_drop_list)
donnees_avatar_knn5=pd.read_csv("../../Anonym_sfpt24/Donnees/avatar_sfpt_knn5.csv").drop(columns=columns_drop_list)
donnees_avatar_knn10=pd.read_csv("../../Anonym_sfpt24/Donnees/avatar_sfpt_knn10.csv").drop(columns=columns_drop_list)
donnees_avatar_knn15=pd.read_csv("../../Anonym_sfpt24/Donnees/avatar_sfpt_knn15.csv").drop(columns=columns_drop_list)
donnees_avatar_knn20=pd.read_csv("../../Anonym_sfpt24/Donnees/avatar_sfpt_knn20.csv").drop(columns=columns_drop_list)

donnees_surv_ctgan=pd.read_csv("../Generate_anonym_data/CTGAN/sfpt24_survctgan_data.dat").drop(columns=columns_drop_list)
donnees_survae=pd.read_csv("../Generate_anonym_data/CTGAN/sfpt24_survae_data.dat").drop(columns=columns_drop_list)

# calculation of delta presence
dp_object=synthcity.metrics.eval_privacy.DeltaPresence()
dp_object.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn3))
dp_object.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn5))
dp_object.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn10))
dp_object.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn15))
dp_object.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn20))
dp_object.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_surv_ctgan))
dp_object.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_survae))

# detection by a xgboost of true vs synthetic data
detection_xgb=synthcity.metrics.eval_detection.SyntheticDetectionXGB()
detection_xgb.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn3))
detection_xgb.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn5))
detection_xgb.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn10))
detection_xgb.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn15))
detection_xgb.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn20))
detection_xgb.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_surv_ctgan))
detection_xgb.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_survae))

# detection by a mlp of true vs synthetic data
detection_mlp=synthcity.metrics.eval_detection.SyntheticDetectionMLP()
detection_mlp.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn3))
detection_mlp.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn5))
detection_mlp.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn10))
detection_mlp.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn15))
detection_mlp.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn20))
detection_mlp.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_surv_ctgan))
detection_mlp.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_survae))

synthcity.metrics.eval_performance.XGBSurvivalAnalysis()

# calculation of k anonymity
kanonym=synthcity.metrics.eval_privacy.kAnonymization()
kanonym.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn3))
kanonym.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn5))
kanonym.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn10))
kanonym.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn15))
kanonym.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn20))
kanonym.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_surv_ctgan))
kanonym.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_survae))

# calculation of kulback leiber distance
kl_dist=synthcity.metrics.eval_statistical.InverseKLDivergence()
kl_dist.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn3))
kl_dist.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn5))
kl_dist.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn10))
kl_dist.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn15))
kl_dist.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn20))
kl_dist.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_surv_ctgan))
kl_dist.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_survae))

# calculation of kolmogorf and smirnof test
ks_test=synthcity.metrics.eval_statistical.KolmogorovSmirnovTest()
ks_test.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn3))
ks_test.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn5))
ks_test.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn10))
ks_test.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn15))
ks_test.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn20))
ks_test.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_surv_ctgan))
ks_test.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_survae))

# calculation of max discrepancy
max_discrepancy=synthcity.metrics.eval_statistical.MaximumMeanDiscrepancy()
max_discrepancy.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn3))
max_discrepancy.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn5))
max_discrepancy.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn10))
max_discrepancy.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn15))
max_discrepancy.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_avatar_knn20))
max_discrepancy.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_surv_ctgan))
max_discrepancy.evaluate(X_gt=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_origin),X_syn=synthcity.plugins.core.dataloader.GenericDataLoader(donnees_survae))

# calcuation of dcr and nndr
data_by_algo_pkpop={"avatar_knn5":donnees_avatar_knn5,"avatar_knn5_large":donnees_avatar_knn5_large,"avatar_knn15":donnees_avatar_knn15,"avatar_knn15_large":donnees_avatar_knn15_large,"ctgan":donnees_infl_ctgan,"ctgan_large":donnees_infl_ctgan_large}
synth_dcr_dict=dict()
synth_nndr_dict=dict()
for algo in data_by_algo_pkpop.keys():
  train_coord, model = saiph.fit_transform(data_by_algo_pkpop[algo])
  synth_coord = saiph.transform(data_by_algo_pkpop[algo],model)
  synth_dcr_dict[algo]=np.mean(np.array(get_dcr(data_by_algo_pkpop[algo],synth_coord)))
  synth_nndr_dict[algo]=np.mean(np.array(get_nndr(data_by_algo_pkpop[algo],synth_coord)))

```


## calculation metrics privacy DCR, NNDR

```{r}
#knn5
metric_avatar_knn5_graft_loss <- read_csv("Metrics_avatar2/metric_avatar_knn5_graft_loss.csv") 

metric_knn5 <- metric_avatar_knn5_graft_loss %>% 
  summarise(
    across(
      everything(),
      list(
        min = ~ quantile(., probs = 0),
        p5 = ~ quantile(., probs = 0.05),
        p25 = ~ quantile(., probs = 0.25),
        p50 = ~ quantile(., probs = 0.5),
        p75 = ~ quantile(., probs = 0.75),
        p95 = ~ quantile(., probs = 0.95),
        max = ~ quantile(., probs = 1)
      )
    )
  )
knitr::kable(metric_knn5, "simple")

#knn5 augmented
metric_avatar_knn5_large_graft_loss <- read_csv("Metrics_avatar2/metric_avatar_knn5_large_graft_loss.csv")
metric_knn5_augmented <- metric_avatar_knn5_large_graft_loss %>% summarise(
    across(
      everything(),
      list(
        min = ~ quantile(., probs = 0),
        p5 = ~ quantile(., probs = 0.05),
        p25 = ~ quantile(., probs = 0.25),
        p50 = ~ quantile(., probs = 0.5),
        p75 = ~ quantile(., probs = 0.75),
        p95 = ~ quantile(., probs = 0.95),
        max = ~ quantile(., probs = 1)
      )
    )
  )
knitr::kable(metric_knn5_augmented, "simple")

#survae
metric_survae_graft_loss <- read_csv("Metrics_avatar2/metric_survae_graft_loss.csv") 

metric_survae <- metric_survae_graft_loss %>% 
  summarise(
    across(
      everything(),
      list(
        min = ~ quantile(., probs = 0),
        p5 = ~ quantile(., probs = 0.05),
        p25 = ~ quantile(., probs = 0.25),
        p50 = ~ quantile(., probs = 0.5),
        p75 = ~ quantile(., probs = 0.75),
        p95 = ~ quantile(., probs = 0.95),
        max = ~ quantile(., probs = 1)
      )
    )
  )
knitr::kable(metric_survae, "simple")

#survae augmented
metric_survae_large_graft_loss <- read_csv("Metrics_avatar2/metric_survae_large_graft_loss.csv") 

metric_survae_augmented <- metric_survae_large_graft_loss %>% 
   summarise(
    across(
      everything(),
      list(
        min = ~ quantile(., probs = 0),
        p5 = ~ quantile(., probs = 0.05),
        p25 = ~ quantile(., probs = 0.25),
        p50 = ~ quantile(., probs = 0.5),
        p75 = ~ quantile(., probs = 0.75),
        p95 = ~ quantile(., probs = 0.95),
        max = ~ quantile(., probs = 1)
      )
    )
  )
knitr::kable(metric_survae_augmented, "simple")

#ctgan
metric_survctgan_graft_loss <- read_csv("Metrics_avatar2/metric_survctgan_graft_loss.csv") 

metric_ctgan <- metric_survctgan_graft_loss %>% 
     summarise(
    across(
      everything(),
      list(
        min = ~ quantile(., probs = 0),
        p5 = ~ quantile(., probs = 0.05),
        p25 = ~ quantile(., probs = 0.25),
        p50 = ~ quantile(., probs = 0.5),
        p75 = ~ quantile(., probs = 0.75),
        p95 = ~ quantile(., probs = 0.95),
        max = ~ quantile(., probs = 1)
      )
    )
  )
knitr::kable(metric_ctgan, "simple")

#ctgan augmented
metric_avatar_survctgan_large_graft_loss <- read_csv("Metrics_avatar2/metric_survctgan_large_graft_loss.csv") 

metric_ctgan_augmented <- metric_avatar_survctgan_large_graft_loss %>% 
       summarise(
    across(
      everything(),
      list(
        min = ~ quantile(., probs = 0),
        p5 = ~ quantile(., probs = 0.05),
        p25 = ~ quantile(., probs = 0.25),
        p50 = ~ quantile(., probs = 0.5),
        p75 = ~ quantile(., probs = 0.75),
        p95 = ~ quantile(., probs = 0.95),
        max = ~ quantile(., probs = 1)
      )
    )
  )
knitr::kable(metric_ctgan_augmented, "simple")
```

plot of the metrics distribution

```{r}
metrics_plot <- metric_avatar_knn5_graft_loss %>% mutate(type = "knn5") %>% 
  bind_rows(metric_avatar_knn5_large_graft_loss %>% mutate(type = "knn5_augmented")) %>% 
  bind_rows(metric_survae_graft_loss %>% mutate(type = "survae")) %>% 
  bind_rows(metric_survae_large_graft_loss %>% mutate(type = "survae_augmented")) %>% 
  bind_rows(metric_survctgan_graft_loss %>% mutate(type = "survctgan")) %>% 
  bind_rows(metric_avatar_survctgan_large_graft_loss %>% mutate(type = "survctgan_augmented")) 

# dcr
ggplot(metrics_plot, aes(x = dcr, fill = type, color = type, alpha = 0.5)) +
  geom_density(adjust = 1.5) +
  scale_alpha_identity() +
  labs(title = "DCR density Distribution by Group",
       x = "DCR",
       y = "Density",
       fill = "Group",
       color = "Group") +
  theme_minimal() +
  theme(legend.position = "right")

# nndr
ggplot(metrics_plot, aes(x = nndr, fill = type, color = type, alpha = 0.5)) +
  geom_density(adjust = 1.5) +
  scale_alpha_identity() +
  labs(title = "NNDR density distribution per group",
       x = "NNDR",
       y = "Density",
       fill = "Group",
       color = "Group") +
  theme_minimal() +
  theme(legend.position = "right")



```

# Reviewer answer 020824

Calculation of differential privacy cf jupyter notebook diffpriv020824.html for an example with knn5


# Development of GLM logistic model to differentiate synhtetic from original data

## with knn5

```{r}

# Combine original and synthetic data
combined_data <- original1 %>%
  mutate(cat = "original") %>%
  bind_rows(avatars_tibble_knn5 %>% mutate(cat = "synthetic"))

# Convert the target variable to a factor
combined_data <- combined_data %>%
  mutate(cat = factor(cat, levels = c("original", "synthetic")))

# Split the data into training and testing sets
set.seed(123)
data_split <- initial_split(combined_data, strata = cat)
train_data <- training(data_split)
test_data <- testing(data_split)

# Specify the logistic regression model
log_reg_model <- logistic_reg() %>%
  set_engine("glm")

# Create a recipe for preprocessing
data_recipe <- recipe(cat ~ ., data = train_data) %>%
  step_normalize(all_numeric_predictors())

# Create a workflow
log_reg_workflow <- workflow() %>%
  add_model(log_reg_model) %>%
  add_recipe(data_recipe)

# Train the model
log_reg_fit <- log_reg_workflow %>%
  fit(data = train_data)

# Make predictions on the test set
test_predictions <- log_reg_fit %>%
  augment(new_data = test_data)

# conf mat
test_predictions %>%
  conf_mat(cat, .pred_class) %>%
  autoplot(type = "heatmap")

# Calculate the ROC curve and AUC
roc_curve_data <- roc_curve(test_predictions, truth = cat, .pred_synthetic)
roc_auc_value <- roc_auc(test_predictions, truth = cat, .pred_synthetic)

# Print the AUC value
print(roc_auc_value)

# Plot the ROC curve
roc_curve_data %>%
  autoplot()


```
## with knn10

```{r}

# Combine original and synthetic data
combined_data <- original1 %>%
  mutate(cat = "original") %>%
  bind_rows(avatars_tibble_knn10 %>% mutate(cat = "synthetic"))

# Convert the target variable to a factor
combined_data <- combined_data %>%
  mutate(cat = factor(cat, levels = c("original", "synthetic")))

# Split the data into training and testing sets
set.seed(123)
data_split <- initial_split(combined_data, strata = cat)
train_data <- training(data_split)
test_data <- testing(data_split)

# Specify the logistic regression model
log_reg_model <- logistic_reg() %>%
  set_engine("glm")

# Create a recipe for preprocessing
data_recipe <- recipe(cat ~ ., data = train_data) %>%
  step_normalize(all_numeric_predictors())

# Create a workflow
log_reg_workflow <- workflow() %>%
  add_model(log_reg_model) %>%
  add_recipe(data_recipe)

# Train the model
log_reg_fit <- log_reg_workflow %>%
  fit(data = train_data)

# Make predictions on the test set
test_predictions <- log_reg_fit %>%
  augment(new_data = test_data)

# conf mat
test_predictions %>%
  conf_mat(cat, .pred_class) %>%
  autoplot(type = "heatmap")

# Calculate the ROC curve and AUC
roc_curve_data <- roc_curve(test_predictions, truth = cat, .pred_synthetic)
roc_auc_value <- roc_auc(test_predictions, truth = cat, .pred_synthetic)

# Print the AUC value
print(roc_auc_value)

# Plot the ROC curve
roc_curve_data %>%
  autoplot()

```

## with knn20

```{r}

# Combine original and synthetic data
combined_data <- original1 %>%
  mutate(cat = "original") %>%
  bind_rows(avatars_tibble_knn20 %>% mutate(cat = "synthetic"))

# Convert the target variable to a factor
combined_data <- combined_data %>%
  mutate(cat = factor(cat, levels = c("original", "synthetic")))

# Split the data into training and testing sets
set.seed(123)
data_split <- initial_split(combined_data, strata = cat)
train_data <- training(data_split)
test_data <- testing(data_split)

# Specify the logistic regression model
log_reg_model <- logistic_reg() %>%
  set_engine("glm")

# Create a recipe for preprocessing
data_recipe <- recipe(cat ~ ., data = train_data) %>%
  step_normalize(all_numeric_predictors())

# Create a workflow
log_reg_workflow <- workflow() %>%
  add_model(log_reg_model) %>%
  add_recipe(data_recipe)

# Train the model
log_reg_fit <- log_reg_workflow %>%
  fit(data = train_data)

# Make predictions on the test set
test_predictions <- log_reg_fit %>%
  augment(new_data = test_data)

# conf mat
test_predictions %>%
  conf_mat(cat, .pred_class) %>%
  autoplot(type = "heatmap")

# Calculate the ROC curve and AUC
roc_curve_data <- roc_curve(test_predictions, truth = cat, .pred_synthetic)
roc_auc_value <- roc_auc(test_predictions, truth = cat, .pred_synthetic)

# Print the AUC value
print(roc_auc_value)

# Plot the ROC curve
roc_curve_data %>%
  autoplot()

```

## with knn5 augmented

```{r}

# Combine original and synthetic data
combined_data <- original1 %>%
  mutate(cat = "original") %>%
  bind_rows(augmented_data_5 %>% mutate(cat = "synthetic"))

# Convert the target variable to a factor
combined_data <- combined_data %>%
  mutate(cat = factor(cat, levels = c("original", "synthetic")))

# Split the data into training and testing sets
set.seed(123)
data_split <- initial_split(combined_data, strata = cat)
train_data <- training(data_split)
test_data <- testing(data_split)

# Specify the logistic regression model
log_reg_model <- logistic_reg() %>%
  set_engine("glm")

# Create a recipe for preprocessing
data_recipe <- recipe(cat ~ ., data = train_data) %>%
  step_normalize(all_numeric_predictors())

# Create a workflow
log_reg_workflow <- workflow() %>%
  add_model(log_reg_model) %>%
  add_recipe(data_recipe)

# Train the model
log_reg_fit <- log_reg_workflow %>%
  fit(data = train_data)

# Make predictions on the test set
test_predictions <- log_reg_fit %>%
  augment(new_data = test_data)

# conf mat
test_predictions %>%
  conf_mat(cat, .pred_class) %>%
  autoplot(type = "heatmap")

# Calculate the ROC curve and AUC
roc_curve_data <- roc_curve(test_predictions, truth = cat, .pred_synthetic)
roc_auc_value <- roc_auc(test_predictions, truth = cat, .pred_synthetic)

# Print the AUC value
print(roc_auc_value)

# Plot the ROC curve
roc_curve_data %>%
  autoplot()


```

## with knn10 augmented

```{r}

# Combine original and synthetic data
combined_data <- original1 %>%
  mutate(cat = "original") %>%
  bind_rows(augmented_data_10 %>% mutate(cat = "synthetic"))

# Convert the target variable to a factor
combined_data <- combined_data %>%
  mutate(cat = factor(cat, levels = c("original", "synthetic")))

# Split the data into training and testing sets
set.seed(123)
data_split <- initial_split(combined_data, strata = cat)
train_data <- training(data_split)
test_data <- testing(data_split)

# Specify the logistic regression model
log_reg_model <- logistic_reg() %>%
  set_engine("glm")

# Create a recipe for preprocessing
data_recipe <- recipe(cat ~ ., data = train_data) %>%
  step_normalize(all_numeric_predictors())

# Create a workflow
log_reg_workflow <- workflow() %>%
  add_model(log_reg_model) %>%
  add_recipe(data_recipe)

# Train the model
log_reg_fit <- log_reg_workflow %>%
  fit(data = train_data)

# Make predictions on the test set
test_predictions <- log_reg_fit %>%
  augment(new_data = test_data)

# conf mat
test_predictions %>%
  conf_mat(cat, .pred_class) %>%
  autoplot(type = "heatmap")

# Calculate the ROC curve and AUC
roc_curve_data <- roc_curve(test_predictions, truth = cat, .pred_synthetic)
roc_auc_value <- roc_auc(test_predictions, truth = cat, .pred_synthetic)

# Print the AUC value
print(roc_auc_value)

# Plot the ROC curve
roc_curve_data %>%
  autoplot()


```

## with knn20 augmented

```{r}

# Combine original and synthetic data
combined_data <- original1 %>%
  mutate(cat = "original") %>%
  bind_rows(augmented_data_20 %>% mutate(cat = "synthetic"))

# Convert the target variable to a factor
combined_data <- combined_data %>%
  mutate(cat = factor(cat, levels = c("original", "synthetic")))

# Split the data into training and testing sets
set.seed(123)
data_split <- initial_split(combined_data, strata = cat)
train_data <- training(data_split)
test_data <- testing(data_split)

# Specify the logistic regression model
log_reg_model <- logistic_reg() %>%
  set_engine("glm")

# Create a recipe for preprocessing
data_recipe <- recipe(cat ~ ., data = train_data) %>%
  step_normalize(all_numeric_predictors())

# Create a workflow
log_reg_workflow <- workflow() %>%
  add_model(log_reg_model) %>%
  add_recipe(data_recipe)

# Train the model
log_reg_fit <- log_reg_workflow %>%
  fit(data = train_data)

# Make predictions on the test set
test_predictions <- log_reg_fit %>%
  augment(new_data = test_data)

# conf mat
test_predictions %>%
  conf_mat(cat, .pred_class) %>%
  autoplot(type = "heatmap")

# Calculate the ROC curve and AUC
roc_curve_data <- roc_curve(test_predictions, truth = cat, .pred_synthetic)
roc_auc_value <- roc_auc(test_predictions, truth = cat, .pred_synthetic)

# Print the AUC value
print(roc_auc_value)

# Plot the ROC curve
roc_curve_data %>%
  autoplot()


```

## with survae

```{r}

# Combine original and synthetic data
combined_data <- original1 %>%
  mutate(cat = "original") %>%
  bind_rows(survae %>% mutate(cat = "synthetic"))

# Convert the target variable to a factor
combined_data <- combined_data %>%
  mutate(cat = factor(cat, levels = c("original", "synthetic")))

# Split the data into training and testing sets
set.seed(123)
data_split <- initial_split(combined_data, strata = cat)
train_data <- training(data_split)
test_data <- testing(data_split)

# Specify the logistic regression model
log_reg_model <- logistic_reg() %>%
  set_engine("glm")

# Create a recipe for preprocessing
data_recipe <- recipe(cat ~ ., data = train_data) %>%
  step_normalize(all_numeric_predictors())

# Create a workflow
log_reg_workflow <- workflow() %>%
  add_model(log_reg_model) %>%
  add_recipe(data_recipe)

# Train the model
log_reg_fit <- log_reg_workflow %>%
  fit(data = train_data)

# Make predictions on the test set
test_predictions <- log_reg_fit %>%
  augment(new_data = test_data)

# conf mat
test_predictions %>%
  conf_mat(cat, .pred_class) %>%
  autoplot(type = "heatmap")

# Calculate the ROC curve and AUC
roc_curve_data <- roc_curve(test_predictions, truth = cat, .pred_synthetic)
roc_auc_value <- roc_auc(test_predictions, truth = cat, .pred_synthetic)

# Print the AUC value
print(roc_auc_value)

# Plot the ROC curve
roc_curve_data %>%
  autoplot()


```

## with survae_augmented

```{r}

# Combine original and synthetic data
combined_data <- original1 %>%
  mutate(cat = "original") %>%
  bind_rows(survae_augmented %>% mutate(cat = "synthetic"))

# Convert the target variable to a factor
combined_data <- combined_data %>%
  mutate(cat = factor(cat, levels = c("original", "synthetic")))

# Split the data into training and testing sets
set.seed(123)
data_split <- initial_split(combined_data, strata = cat)
train_data <- training(data_split)
test_data <- testing(data_split)

# Specify the logistic regression model
log_reg_model <- logistic_reg() %>%
  set_engine("glm")

# Create a recipe for preprocessing
data_recipe <- recipe(cat ~ ., data = train_data) %>%
  step_normalize(all_numeric_predictors())

# Create a workflow
log_reg_workflow <- workflow() %>%
  add_model(log_reg_model) %>%
  add_recipe(data_recipe)

# Train the model
log_reg_fit <- log_reg_workflow %>%
  fit(data = train_data)

# Make predictions on the test set
test_predictions <- log_reg_fit %>%
  augment(new_data = test_data)

# conf mat
test_predictions %>%
  conf_mat(cat, .pred_class) %>%
  autoplot(type = "heatmap")

# Calculate the ROC curve and AUC
roc_curve_data <- roc_curve(test_predictions, truth = cat, .pred_synthetic)
roc_auc_value <- roc_auc(test_predictions, truth = cat, .pred_synthetic)

# Print the AUC value
print(roc_auc_value)

# Plot the ROC curve
roc_curve_data %>%
  autoplot()


```

## with surctgan

```{r}

# Combine original and synthetic data
combined_data <- original1 %>%
  mutate(cat = "original") %>%
  bind_rows(survctgan %>% mutate(cat = "synthetic"))

# Convert the target variable to a factor
combined_data <- combined_data %>%
  mutate(cat = factor(cat, levels = c("original", "synthetic")))

# Split the data into training and testing sets
set.seed(123)
data_split <- initial_split(combined_data, strata = cat)
train_data <- training(data_split)
test_data <- testing(data_split)

# Specify the logistic regression model
log_reg_model <- logistic_reg() %>%
  set_engine("glm")

# Create a recipe for preprocessing
data_recipe <- recipe(cat ~ ., data = train_data) %>%
  step_normalize(all_numeric_predictors())

# Create a workflow
log_reg_workflow <- workflow() %>%
  add_model(log_reg_model) %>%
  add_recipe(data_recipe)

# Train the model
log_reg_fit <- log_reg_workflow %>%
  fit(data = train_data)

# Make predictions on the test set
test_predictions <- log_reg_fit %>%
  augment(new_data = test_data)

# conf mat
test_predictions %>%
  conf_mat(cat, .pred_class) %>%
  autoplot(type = "heatmap")

# Calculate the ROC curve and AUC
roc_curve_data <- roc_curve(test_predictions, truth = cat, .pred_synthetic)
roc_auc_value <- roc_auc(test_predictions, truth = cat, .pred_synthetic)

# Print the AUC value
print(roc_auc_value)

# Plot the ROC curve
roc_curve_data %>%
  autoplot()


```

## with survctgan_augmented

```{r}

# Combine original and synthetic data
combined_data <- original1 %>%
  mutate(cat = "original") %>%
  bind_rows(survctgan_augmented %>% mutate(cat = "synthetic"))

# Convert the target variable to a factor
combined_data <- combined_data %>%
  mutate(cat = factor(cat, levels = c("original", "synthetic")))

# Split the data into training and testing sets
set.seed(123)
data_split <- initial_split(combined_data, strata = cat)
train_data <- training(data_split)
test_data <- testing(data_split)

# Specify the logistic regression model
log_reg_model <- logistic_reg() %>%
  set_engine("glm")

# Create a recipe for preprocessing
data_recipe <- recipe(cat ~ ., data = train_data) %>%
  step_normalize(all_numeric_predictors())

# Create a workflow
log_reg_workflow <- workflow() %>%
  add_model(log_reg_model) %>%
  add_recipe(data_recipe)

# Train the model
log_reg_fit <- log_reg_workflow %>%
  fit(data = train_data)

# Make predictions on the test set
test_predictions <- log_reg_fit %>%
  augment(new_data = test_data)

# conf mat
test_predictions %>%
  conf_mat(cat, .pred_class) %>%
  autoplot(type = "heatmap")

# Calculate the ROC curve and AUC
roc_curve_data <- roc_curve(test_predictions, truth = cat, .pred_synthetic)
roc_auc_value <- roc_auc(test_predictions, truth = cat, .pred_synthetic)

# Print the AUC value
print(roc_auc_value)

# Plot the ROC curve
roc_curve_data %>%
  autoplot()


```